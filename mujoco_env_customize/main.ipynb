{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apt install xvfb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xvfb-run: error: Xvfb failed to start\n"
     ]
    }
   ],
   "source": [
    "!export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libGLEW.so:/usr/lib/x86_64-linux-gnu/libGL.so\n",
    "!xvfb-run -s \"-screen 0 1400x900x24\" /bin/bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['LD_PRELOAD'] = \"/usr/lib/x86_64-linux-gnu/libGLEW.so:/usr/lib/x86_64-linux-gnu/libGL.so\"\n",
    "os.environ['DISPLAY'] = ':0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":0\n"
     ]
    }
   ],
   "source": [
    "!echo $DISPLAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/research/agent-playground/mujoco_env_customize/gym_robotics/envs/fetch/reach.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/research/agent-playground/mujoco_env_customize/gym_robotics/envs/robot_env.py:309: UserWarning: \u001b[33mWARN: This version of the mujoco environments depends on the mujoco-py bindings, which are no longer maintained and may stop working. Please upgrade to the v4 versions of the environments (which depend on the mujoco python bindings instead), unless you are trying to precisely replicate previous works).\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reach environment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reach environment\n",
    "Dict(\n",
    "    \n",
    "    'achieved_goal': Box(-inf, inf, (3,), float64), \n",
    "    \n",
    "    'desired_goal': Box(-inf, inf, (3,), float64), \n",
    "    \n",
    "    'observation': Box(-inf, inf, (10,), float64)) : \n",
    "    [ grip_pos, object_pos.ravel(), object_rel_pos.ravel(), gripper_state, object_rot.ravel(), object_velp.ravel(), object_velr.ravel(), grip_velp, gripper_vel,]\n",
    "    [3, 0, 0, 2, 0, 0, 0, 3, 2]\n",
    "\n",
    "    so, [grip_pos 3, gripper_state 2, grip_velp 3, gripper_vel 2]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jesnk_utils.rgb_to_video import RGB2VIDEO\n",
    "# reset environment\n",
    "env.reset()\n",
    "env.render_mode = 'rgb_array'\n",
    "img = env.render()\n",
    "\n",
    "rgb_to_video = RGB2VIDEO()\n",
    "\n",
    "# get frames for video, random actions, 30 frames\n",
    "frames = []\n",
    "for i in range(30):\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, terminated, info = env.step(action)\n",
    "    # obs, reward, terminated, truncated, info\n",
    "    # obs : ['observation', 'achieved_goal', 'desired_goal']\n",
    "    img = env.render()\n",
    "    frames.append(img)\n",
    "    \n",
    "# save video\n",
    "rgb_to_video.set_frames(frames)\n",
    "rgb_to_video.save(mode='gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/research/agent-playground/mujoco_env_customize/gym_robotics/envs/robot_env.py:309: UserWarning: \u001b[33mWARN: This version of the mujoco environments depends on the mujoco-py bindings, which are no longer maintained and may stop working. Please upgrade to the v4 versions of the environments (which depend on the mujoco python bindings instead), unless you are trying to precisely replicate previous works).\u001b[0m\n",
      "  logger.warn(\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/research/agent-playground/mujoco_env_customize/gym_robotics/envs/fetch/reach.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjesnk\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/research/agent-playground/mujoco_env_customize/wandb/run-20230613_135821-y0hqfxfq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jesnk/sb3/runs/y0hqfxfq' target=\"_blank\">radiant-wood-74</a></strong> to <a href='https://wandb.ai/jesnk/sb3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jesnk/sb3' target=\"_blank\">https://wandb.ai/jesnk/sb3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jesnk/sb3/runs/y0hqfxfq' target=\"_blank\">https://wandb.ai/jesnk/sb3/runs/y0hqfxfq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/research/agent-playground/mujoco_env_customize/gym_robotics/envs/robot_env.py:309: UserWarning: \u001b[33mWARN: This version of the mujoco environments depends on the mujoco-py bindings, which are no longer maintained and may stop working. Please upgrade to the v4 versions of the environments (which depend on the mujoco python bindings instead), unless you are trying to precisely replicate previous works).\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Eval num_timesteps=10, episode_reward=-141.12 +/- 25.03\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -141     |\n",
      "| time/              |          |\n",
      "|    total timesteps | 10       |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20, episode_reward=-138.55 +/- 29.05\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -139     |\n",
      "| time/              |          |\n",
      "|    total timesteps | 20       |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30, episode_reward=-141.05 +/- 33.67\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -141     |\n",
      "| time/              |          |\n",
      "|    total timesteps | 30       |\n",
      "---------------------------------\n",
      "Eval num_timesteps=40, episode_reward=-154.10 +/- 26.00\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -154     |\n",
      "| time/              |          |\n",
      "|    total timesteps | 40       |\n",
      "---------------------------------\n",
      "Eval num_timesteps=50, episode_reward=-111.64 +/- 23.82\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -112     |\n",
      "| time/              |          |\n",
      "|    total timesteps | 50       |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60, episode_reward=-157.68 +/- 33.01\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -158     |\n",
      "| time/              |          |\n",
      "|    total timesteps | 60       |\n",
      "---------------------------------\n",
      "Eval num_timesteps=70, episode_reward=-133.68 +/- 26.04\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -134     |\n",
      "| time/              |          |\n",
      "|    total timesteps | 70       |\n",
      "---------------------------------\n",
      "Eval num_timesteps=80, episode_reward=-113.53 +/- 26.93\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -114     |\n",
      "| time/              |          |\n",
      "|    total timesteps | 80       |\n",
      "---------------------------------\n",
      "Eval num_timesteps=90, episode_reward=-136.47 +/- 26.27\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -136     |\n",
      "| time/              |          |\n",
      "|    total timesteps | 90       |\n",
      "---------------------------------\n",
      "Eval num_timesteps=100, episode_reward=-118.61 +/- 30.86\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -119     |\n",
      "| time/              |          |\n",
      "|    total timesteps | 100      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=110, episode_reward=-171.34 +/- 53.05\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -171     |\n",
      "| time/              |          |\n",
      "|    total timesteps | 110      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.22    |\n",
      "|    critic_loss     | 3.85     |\n",
      "|    ent_coef        | 0.998    |\n",
      "|    ent_coef_loss   | -0.0162  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 9        |\n",
      "---------------------------------\n",
      "Eval num_timesteps=120, episode_reward=-141.10 +/- 35.32\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -141     |\n",
      "| time/              |          |\n",
      "|    total timesteps | 120      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.85    |\n",
      "|    critic_loss     | 1.76     |\n",
      "|    ent_coef        | 0.995    |\n",
      "|    ent_coef_loss   | -0.0364  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 19       |\n",
      "---------------------------------\n",
      "Eval num_timesteps=130, episode_reward=-139.13 +/- 41.59\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -139     |\n",
      "| time/              |          |\n",
      "|    total timesteps | 130      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -4.63    |\n",
      "|    critic_loss     | 0.456    |\n",
      "|    ent_coef        | 0.992    |\n",
      "|    ent_coef_loss   | -0.0566  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 29       |\n",
      "---------------------------------\n",
      "Eval num_timesteps=140, episode_reward=-109.41 +/- 17.63\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -109     |\n",
      "| time/              |          |\n",
      "|    total timesteps | 140      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -5.35    |\n",
      "|    critic_loss     | 0.0827   |\n",
      "|    ent_coef        | 0.989    |\n",
      "|    ent_coef_loss   | -0.0768  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 39       |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=150, episode_reward=-159.05 +/- 33.56\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -159     |\n",
      "| time/              |          |\n",
      "|    total timesteps | 150      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -5.5     |\n",
      "|    critic_loss     | 0.132    |\n",
      "|    ent_coef        | 0.986    |\n",
      "|    ent_coef_loss   | -0.0968  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 49       |\n",
      "---------------------------------\n",
      "Eval num_timesteps=160, episode_reward=-139.22 +/- 42.65\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -139     |\n",
      "| time/              |          |\n",
      "|    total timesteps | 160      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -5.32    |\n",
      "|    critic_loss     | 0.0541   |\n",
      "|    ent_coef        | 0.983    |\n",
      "|    ent_coef_loss   | -0.117   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 59       |\n",
      "---------------------------------\n",
      "Eval num_timesteps=170, episode_reward=-78.65 +/- 45.86\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -78.7    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 170      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -5.36    |\n",
      "|    critic_loss     | 0.0951   |\n",
      "|    ent_coef        | 0.98     |\n",
      "|    ent_coef_loss   | -0.138   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 69       |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=180, episode_reward=-125.45 +/- 47.73\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -125     |\n",
      "| time/              |          |\n",
      "|    total timesteps | 180      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -5.52    |\n",
      "|    critic_loss     | 0.0853   |\n",
      "|    ent_coef        | 0.977    |\n",
      "|    ent_coef_loss   | -0.158   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 79       |\n",
      "---------------------------------\n",
      "Eval num_timesteps=190, episode_reward=-124.21 +/- 70.33\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -124     |\n",
      "| time/              |          |\n",
      "|    total timesteps | 190      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -5.64    |\n",
      "|    critic_loss     | 0.0649   |\n",
      "|    ent_coef        | 0.974    |\n",
      "|    ent_coef_loss   | -0.178   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 89       |\n",
      "---------------------------------\n",
      "Eval num_timesteps=200, episode_reward=-82.55 +/- 12.26\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -82.6    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 200      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -5.66    |\n",
      "|    critic_loss     | 0.0654   |\n",
      "|    ent_coef        | 0.971    |\n",
      "|    ent_coef_loss   | -0.197   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 99       |\n",
      "---------------------------------\n",
      "Eval num_timesteps=210, episode_reward=-121.72 +/- 28.92\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -122     |\n",
      "| time/              |          |\n",
      "|    total timesteps | 210      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -5.76    |\n",
      "|    critic_loss     | 0.0626   |\n",
      "|    ent_coef        | 0.968    |\n",
      "|    ent_coef_loss   | -0.218   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 109      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=220, episode_reward=-202.74 +/- 55.06\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -203     |\n",
      "| time/              |          |\n",
      "|    total timesteps | 220      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -5.85    |\n",
      "|    critic_loss     | 0.108    |\n",
      "|    ent_coef        | 0.965    |\n",
      "|    ent_coef_loss   | -0.239   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 119      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=230, episode_reward=-142.73 +/- 39.15\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -143     |\n",
      "| time/              |          |\n",
      "|    total timesteps | 230      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -5.94    |\n",
      "|    critic_loss     | 0.077    |\n",
      "|    ent_coef        | 0.962    |\n",
      "|    ent_coef_loss   | -0.26    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 129      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=240, episode_reward=-151.68 +/- 48.41\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -152     |\n",
      "| time/              |          |\n",
      "|    total timesteps | 240      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -5.99    |\n",
      "|    critic_loss     | 0.0484   |\n",
      "|    ent_coef        | 0.959    |\n",
      "|    ent_coef_loss   | -0.279   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 139      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=250, episode_reward=-133.37 +/- 60.90\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -133     |\n",
      "| time/              |          |\n",
      "|    total timesteps | 250      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -6.02    |\n",
      "|    critic_loss     | 0.0856   |\n",
      "|    ent_coef        | 0.957    |\n",
      "|    ent_coef_loss   | -0.299   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 149      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=260, episode_reward=-71.16 +/- 22.96\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -71.2    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 260      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -6.12    |\n",
      "|    critic_loss     | 0.0682   |\n",
      "|    ent_coef        | 0.954    |\n",
      "|    ent_coef_loss   | -0.319   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 159      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=270, episode_reward=-144.51 +/- 40.50\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -145     |\n",
      "| time/              |          |\n",
      "|    total timesteps | 270      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -6.18    |\n",
      "|    critic_loss     | 0.0763   |\n",
      "|    ent_coef        | 0.951    |\n",
      "|    ent_coef_loss   | -0.339   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 169      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=280, episode_reward=-89.95 +/- 30.51\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -90      |\n",
      "| time/              |          |\n",
      "|    total timesteps | 280      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -6.24    |\n",
      "|    critic_loss     | 0.0634   |\n",
      "|    ent_coef        | 0.948    |\n",
      "|    ent_coef_loss   | -0.359   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 179      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=290, episode_reward=-86.12 +/- 24.95\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -86.1    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 290      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -6.31    |\n",
      "|    critic_loss     | 0.06     |\n",
      "|    ent_coef        | 0.945    |\n",
      "|    ent_coef_loss   | -0.379   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 189      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=300, episode_reward=-41.23 +/- 3.30\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -41.2    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 300      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -6.41    |\n",
      "|    critic_loss     | 0.0423   |\n",
      "|    ent_coef        | 0.942    |\n",
      "|    ent_coef_loss   | -0.401   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 199      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=310, episode_reward=-111.65 +/- 33.04\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -112     |\n",
      "| time/              |          |\n",
      "|    total timesteps | 310      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -6.42    |\n",
      "|    critic_loss     | 0.0698   |\n",
      "|    ent_coef        | 0.939    |\n",
      "|    ent_coef_loss   | -0.419   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 209      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=320, episode_reward=-108.02 +/- 19.72\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -108     |\n",
      "| time/              |          |\n",
      "|    total timesteps | 320      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -6.51    |\n",
      "|    critic_loss     | 0.0654   |\n",
      "|    ent_coef        | 0.937    |\n",
      "|    ent_coef_loss   | -0.442   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 219      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=330, episode_reward=-167.88 +/- 62.46\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -168     |\n",
      "| time/              |          |\n",
      "|    total timesteps | 330      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -6.58    |\n",
      "|    critic_loss     | 0.0457   |\n",
      "|    ent_coef        | 0.934    |\n",
      "|    ent_coef_loss   | -0.462   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 229      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=340, episode_reward=-38.37 +/- 3.57\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -38.4    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 340      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -6.63    |\n",
      "|    critic_loss     | 0.0692   |\n",
      "|    ent_coef        | 0.931    |\n",
      "|    ent_coef_loss   | -0.478   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 239      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=350, episode_reward=-85.57 +/- 19.22\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -85.6    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 350      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -6.69    |\n",
      "|    critic_loss     | 0.0543   |\n",
      "|    ent_coef        | 0.928    |\n",
      "|    ent_coef_loss   | -0.501   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 249      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=360, episode_reward=-36.43 +/- 2.40\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -36.4    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 360      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -6.77    |\n",
      "|    critic_loss     | 0.0442   |\n",
      "|    ent_coef        | 0.926    |\n",
      "|    ent_coef_loss   | -0.521   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 259      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=370, episode_reward=-28.76 +/- 7.96\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -28.8    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 370      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -6.81    |\n",
      "|    critic_loss     | 0.0483   |\n",
      "|    ent_coef        | 0.923    |\n",
      "|    ent_coef_loss   | -0.541   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 269      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=380, episode_reward=-103.58 +/- 18.11\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -104     |\n",
      "| time/              |          |\n",
      "|    total timesteps | 380      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -6.9     |\n",
      "|    critic_loss     | 0.0435   |\n",
      "|    ent_coef        | 0.92     |\n",
      "|    ent_coef_loss   | -0.562   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 279      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=390, episode_reward=-186.73 +/- 70.97\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -187     |\n",
      "| time/              |          |\n",
      "|    total timesteps | 390      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -6.96    |\n",
      "|    critic_loss     | 0.0367   |\n",
      "|    ent_coef        | 0.917    |\n",
      "|    ent_coef_loss   | -0.582   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 289      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=400, episode_reward=-144.12 +/- 53.25\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -144     |\n",
      "| time/              |          |\n",
      "|    total timesteps | 400      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -6.99    |\n",
      "|    critic_loss     | 0.0426   |\n",
      "|    ent_coef        | 0.914    |\n",
      "|    ent_coef_loss   | -0.599   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 299      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=410, episode_reward=-121.28 +/- 43.51\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -121     |\n",
      "| time/              |          |\n",
      "|    total timesteps | 410      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.06    |\n",
      "|    critic_loss     | 0.0328   |\n",
      "|    ent_coef        | 0.912    |\n",
      "|    ent_coef_loss   | -0.619   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 309      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=420, episode_reward=-123.33 +/- 25.85\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -123     |\n",
      "| time/              |          |\n",
      "|    total timesteps | 420      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.16    |\n",
      "|    critic_loss     | 0.0521   |\n",
      "|    ent_coef        | 0.909    |\n",
      "|    ent_coef_loss   | -0.643   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 319      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=430, episode_reward=-76.69 +/- 12.78\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -76.7    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 430      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.21    |\n",
      "|    critic_loss     | 0.0586   |\n",
      "|    ent_coef        | 0.906    |\n",
      "|    ent_coef_loss   | -0.662   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 329      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=440, episode_reward=-161.34 +/- 72.51\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -161     |\n",
      "| time/              |          |\n",
      "|    total timesteps | 440      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.26    |\n",
      "|    critic_loss     | 0.0654   |\n",
      "|    ent_coef        | 0.904    |\n",
      "|    ent_coef_loss   | -0.68    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 339      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=450, episode_reward=-66.78 +/- 14.04\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -66.8    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 450      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.37    |\n",
      "|    critic_loss     | 0.0471   |\n",
      "|    ent_coef        | 0.901    |\n",
      "|    ent_coef_loss   | -0.704   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 349      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=460, episode_reward=-56.60 +/- 3.96\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -56.6    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 460      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.36    |\n",
      "|    critic_loss     | 0.0458   |\n",
      "|    ent_coef        | 0.898    |\n",
      "|    ent_coef_loss   | -0.721   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 359      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=470, episode_reward=-65.37 +/- 6.58\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -65.4    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 470      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.45    |\n",
      "|    critic_loss     | 0.035    |\n",
      "|    ent_coef        | 0.895    |\n",
      "|    ent_coef_loss   | -0.743   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 369      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=480, episode_reward=-91.79 +/- 19.31\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -91.8    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 480      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.54    |\n",
      "|    critic_loss     | 0.0442   |\n",
      "|    ent_coef        | 0.893    |\n",
      "|    ent_coef_loss   | -0.762   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 379      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=490, episode_reward=-119.34 +/- 44.24\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -119     |\n",
      "| time/              |          |\n",
      "|    total timesteps | 490      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.6     |\n",
      "|    critic_loss     | 0.0344   |\n",
      "|    ent_coef        | 0.89     |\n",
      "|    ent_coef_loss   | -0.786   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 389      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=500, episode_reward=-123.56 +/- 36.58\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -124     |\n",
      "| time/              |          |\n",
      "|    total timesteps | 500      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.65    |\n",
      "|    critic_loss     | 0.0418   |\n",
      "|    ent_coef        | 0.887    |\n",
      "|    ent_coef_loss   | -0.803   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 399      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=510, episode_reward=-85.09 +/- 29.73\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -85.1    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 510      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.74    |\n",
      "|    critic_loss     | 0.0526   |\n",
      "|    ent_coef        | 0.885    |\n",
      "|    ent_coef_loss   | -0.825   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 409      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=520, episode_reward=-121.68 +/- 34.37\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -122     |\n",
      "| time/              |          |\n",
      "|    total timesteps | 520      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.8     |\n",
      "|    critic_loss     | 0.0346   |\n",
      "|    ent_coef        | 0.882    |\n",
      "|    ent_coef_loss   | -0.844   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 419      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=530, episode_reward=-105.47 +/- 28.02\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -105     |\n",
      "| time/              |          |\n",
      "|    total timesteps | 530      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.93    |\n",
      "|    critic_loss     | 0.0387   |\n",
      "|    ent_coef        | 0.879    |\n",
      "|    ent_coef_loss   | -0.866   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 429      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=540, episode_reward=-147.46 +/- 55.69\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -147     |\n",
      "| time/              |          |\n",
      "|    total timesteps | 540      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.92    |\n",
      "|    critic_loss     | 0.0481   |\n",
      "|    ent_coef        | 0.877    |\n",
      "|    ent_coef_loss   | -0.885   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 439      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=550, episode_reward=-72.45 +/- 11.47\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -72.4    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 550      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.99    |\n",
      "|    critic_loss     | 0.0393   |\n",
      "|    ent_coef        | 0.874    |\n",
      "|    ent_coef_loss   | -0.905   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 449      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=560, episode_reward=-69.31 +/- 12.27\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -69.3    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 560      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -8.1     |\n",
      "|    critic_loss     | 0.0388   |\n",
      "|    ent_coef        | 0.872    |\n",
      "|    ent_coef_loss   | -0.93    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 459      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=570, episode_reward=-61.08 +/- 3.82\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -61.1    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 570      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -8.12    |\n",
      "|    critic_loss     | 0.0466   |\n",
      "|    ent_coef        | 0.869    |\n",
      "|    ent_coef_loss   | -0.942   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 469      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=580, episode_reward=-22.91 +/- 1.09\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -22.9    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 580      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -8.17    |\n",
      "|    critic_loss     | 0.0424   |\n",
      "|    ent_coef        | 0.866    |\n",
      "|    ent_coef_loss   | -0.965   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 479      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=590, episode_reward=-74.00 +/- 35.48\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -74      |\n",
      "| time/              |          |\n",
      "|    total timesteps | 590      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -8.22    |\n",
      "|    critic_loss     | 0.0429   |\n",
      "|    ent_coef        | 0.864    |\n",
      "|    ent_coef_loss   | -0.985   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 489      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=600, episode_reward=-77.29 +/- 19.31\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -77.3    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 600      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -8.29    |\n",
      "|    critic_loss     | 0.0352   |\n",
      "|    ent_coef        | 0.861    |\n",
      "|    ent_coef_loss   | -1.01    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 499      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=610, episode_reward=-54.78 +/- 1.04\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -54.8    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 610      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -8.37    |\n",
      "|    critic_loss     | 0.0446   |\n",
      "|    ent_coef        | 0.859    |\n",
      "|    ent_coef_loss   | -1.02    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 509      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=620, episode_reward=-58.97 +/- 2.06\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -59      |\n",
      "| time/              |          |\n",
      "|    total timesteps | 620      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -8.42    |\n",
      "|    critic_loss     | 0.0456   |\n",
      "|    ent_coef        | 0.856    |\n",
      "|    ent_coef_loss   | -1.04    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 519      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=630, episode_reward=-64.67 +/- 13.15\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -64.7    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 630      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -8.46    |\n",
      "|    critic_loss     | 0.0269   |\n",
      "|    ent_coef        | 0.854    |\n",
      "|    ent_coef_loss   | -1.06    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 529      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=640, episode_reward=-72.38 +/- 20.02\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -72.4    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 640      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -8.51    |\n",
      "|    critic_loss     | 0.0509   |\n",
      "|    ent_coef        | 0.851    |\n",
      "|    ent_coef_loss   | -1.08    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 539      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=650, episode_reward=-87.38 +/- 10.79\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -87.4    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 650      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -8.59    |\n",
      "|    critic_loss     | 0.0408   |\n",
      "|    ent_coef        | 0.848    |\n",
      "|    ent_coef_loss   | -1.1     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 549      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=660, episode_reward=-102.94 +/- 31.31\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -103     |\n",
      "| time/              |          |\n",
      "|    total timesteps | 660      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -8.65    |\n",
      "|    critic_loss     | 0.0237   |\n",
      "|    ent_coef        | 0.846    |\n",
      "|    ent_coef_loss   | -1.12    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 559      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=670, episode_reward=-58.70 +/- 4.94\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -58.7    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 670      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -8.74    |\n",
      "|    critic_loss     | 0.0358   |\n",
      "|    ent_coef        | 0.843    |\n",
      "|    ent_coef_loss   | -1.15    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 569      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=680, episode_reward=-73.63 +/- 4.78\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -73.6    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 680      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -8.77    |\n",
      "|    critic_loss     | 0.0441   |\n",
      "|    ent_coef        | 0.841    |\n",
      "|    ent_coef_loss   | -1.17    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 579      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=690, episode_reward=-90.73 +/- 12.36\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -90.7    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 690      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -8.86    |\n",
      "|    critic_loss     | 0.0379   |\n",
      "|    ent_coef        | 0.838    |\n",
      "|    ent_coef_loss   | -1.19    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 589      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=700, episode_reward=-68.88 +/- 4.03\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -68.9    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 700      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -8.97    |\n",
      "|    critic_loss     | 0.0449   |\n",
      "|    ent_coef        | 0.836    |\n",
      "|    ent_coef_loss   | -1.21    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 599      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=710, episode_reward=-49.24 +/- 6.71\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -49.2    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 710      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -8.95    |\n",
      "|    critic_loss     | 0.0588   |\n",
      "|    ent_coef        | 0.833    |\n",
      "|    ent_coef_loss   | -1.23    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 609      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=720, episode_reward=-66.06 +/- 3.18\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -66.1    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 720      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -9.01    |\n",
      "|    critic_loss     | 0.026    |\n",
      "|    ent_coef        | 0.831    |\n",
      "|    ent_coef_loss   | -1.25    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 619      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=730, episode_reward=-109.83 +/- 32.51\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -110     |\n",
      "| time/              |          |\n",
      "|    total timesteps | 730      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -9.11    |\n",
      "|    critic_loss     | 0.0487   |\n",
      "|    ent_coef        | 0.828    |\n",
      "|    ent_coef_loss   | -1.27    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 629      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=740, episode_reward=-125.49 +/- 42.19\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -125     |\n",
      "| time/              |          |\n",
      "|    total timesteps | 740      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -9.11    |\n",
      "|    critic_loss     | 0.0322   |\n",
      "|    ent_coef        | 0.826    |\n",
      "|    ent_coef_loss   | -1.28    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 639      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=750, episode_reward=-94.82 +/- 11.02\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -94.8    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 750      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -9.22    |\n",
      "|    critic_loss     | 0.0471   |\n",
      "|    ent_coef        | 0.823    |\n",
      "|    ent_coef_loss   | -1.31    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 649      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=760, episode_reward=-66.17 +/- 3.51\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -66.2    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 760      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -9.27    |\n",
      "|    critic_loss     | 0.0323   |\n",
      "|    ent_coef        | 0.821    |\n",
      "|    ent_coef_loss   | -1.33    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 659      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=770, episode_reward=-63.04 +/- 5.33\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -63      |\n",
      "| time/              |          |\n",
      "|    total timesteps | 770      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -9.39    |\n",
      "|    critic_loss     | 0.0431   |\n",
      "|    ent_coef        | 0.818    |\n",
      "|    ent_coef_loss   | -1.35    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 669      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=780, episode_reward=-58.62 +/- 14.21\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -58.6    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 780      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -9.46    |\n",
      "|    critic_loss     | 0.0334   |\n",
      "|    ent_coef        | 0.816    |\n",
      "|    ent_coef_loss   | -1.37    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 679      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=790, episode_reward=-83.12 +/- 13.65\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -83.1    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 790      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -9.45    |\n",
      "|    critic_loss     | 0.0288   |\n",
      "|    ent_coef        | 0.814    |\n",
      "|    ent_coef_loss   | -1.38    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 689      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=800, episode_reward=-53.38 +/- 7.03\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -53.4    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 800      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -9.5     |\n",
      "|    critic_loss     | 0.0248   |\n",
      "|    ent_coef        | 0.811    |\n",
      "|    ent_coef_loss   | -1.41    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 699      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=810, episode_reward=-143.91 +/- 46.20\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -144     |\n",
      "| time/              |          |\n",
      "|    total timesteps | 810      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -9.67    |\n",
      "|    critic_loss     | 0.0318   |\n",
      "|    ent_coef        | 0.809    |\n",
      "|    ent_coef_loss   | -1.43    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 709      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=820, episode_reward=-51.01 +/- 4.86\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -51      |\n",
      "| time/              |          |\n",
      "|    total timesteps | 820      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -9.69    |\n",
      "|    critic_loss     | 0.0537   |\n",
      "|    ent_coef        | 0.806    |\n",
      "|    ent_coef_loss   | -1.45    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 719      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=830, episode_reward=-77.57 +/- 18.52\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -77.6    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 830      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -9.78    |\n",
      "|    critic_loss     | 0.0303   |\n",
      "|    ent_coef        | 0.804    |\n",
      "|    ent_coef_loss   | -1.48    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 729      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=840, episode_reward=-78.24 +/- 12.92\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -78.2    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 840      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -9.82    |\n",
      "|    critic_loss     | 0.0276   |\n",
      "|    ent_coef        | 0.801    |\n",
      "|    ent_coef_loss   | -1.49    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 739      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=850, episode_reward=-61.22 +/- 2.04\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -61.2    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 850      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -9.91    |\n",
      "|    critic_loss     | 0.0493   |\n",
      "|    ent_coef        | 0.799    |\n",
      "|    ent_coef_loss   | -1.51    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 749      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=860, episode_reward=-61.26 +/- 2.09\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -61.3    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 860      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -9.93    |\n",
      "|    critic_loss     | 0.0461   |\n",
      "|    ent_coef        | 0.797    |\n",
      "|    ent_coef_loss   | -1.53    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 759      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=870, episode_reward=-47.58 +/- 0.45\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -47.6    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 870      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -10      |\n",
      "|    critic_loss     | 0.0534   |\n",
      "|    ent_coef        | 0.794    |\n",
      "|    ent_coef_loss   | -1.55    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 769      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=880, episode_reward=-34.57 +/- 7.63\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -34.6    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 880      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -10      |\n",
      "|    critic_loss     | 0.0323   |\n",
      "|    ent_coef        | 0.792    |\n",
      "|    ent_coef_loss   | -1.57    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 779      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=890, episode_reward=-92.90 +/- 5.61\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -92.9    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 890      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -10.2    |\n",
      "|    critic_loss     | 0.0377   |\n",
      "|    ent_coef        | 0.789    |\n",
      "|    ent_coef_loss   | -1.59    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 789      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=900, episode_reward=-54.07 +/- 7.97\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -54.1    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 900      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -10.2    |\n",
      "|    critic_loss     | 0.0363   |\n",
      "|    ent_coef        | 0.787    |\n",
      "|    ent_coef_loss   | -1.61    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 799      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=910, episode_reward=-73.01 +/- 5.79\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -73      |\n",
      "| time/              |          |\n",
      "|    total timesteps | 910      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -10.3    |\n",
      "|    critic_loss     | 0.0687   |\n",
      "|    ent_coef        | 0.785    |\n",
      "|    ent_coef_loss   | -1.63    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 809      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=920, episode_reward=-89.87 +/- 18.05\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -89.9    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 920      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -10.4    |\n",
      "|    critic_loss     | 0.0425   |\n",
      "|    ent_coef        | 0.782    |\n",
      "|    ent_coef_loss   | -1.65    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 819      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=930, episode_reward=-48.37 +/- 1.24\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -48.4    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 930      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -10.5    |\n",
      "|    critic_loss     | 0.0336   |\n",
      "|    ent_coef        | 0.78     |\n",
      "|    ent_coef_loss   | -1.67    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 829      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=940, episode_reward=-85.02 +/- 12.36\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -85      |\n",
      "| time/              |          |\n",
      "|    total timesteps | 940      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -10.5    |\n",
      "|    critic_loss     | 0.0233   |\n",
      "|    ent_coef        | 0.778    |\n",
      "|    ent_coef_loss   | -1.69    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 839      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=950, episode_reward=-110.48 +/- 20.89\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -110     |\n",
      "| time/              |          |\n",
      "|    total timesteps | 950      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -10.5    |\n",
      "|    critic_loss     | 0.0441   |\n",
      "|    ent_coef        | 0.775    |\n",
      "|    ent_coef_loss   | -1.71    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 849      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=960, episode_reward=-34.86 +/- 7.51\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -34.9    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 960      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -10.6    |\n",
      "|    critic_loss     | 0.0343   |\n",
      "|    ent_coef        | 0.773    |\n",
      "|    ent_coef_loss   | -1.73    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 859      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=970, episode_reward=-93.60 +/- 17.53\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -93.6    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 970      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -10.7    |\n",
      "|    critic_loss     | 0.0639   |\n",
      "|    ent_coef        | 0.771    |\n",
      "|    ent_coef_loss   | -1.76    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 869      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=980, episode_reward=-74.20 +/- 9.82\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -74.2    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 980      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -10.8    |\n",
      "|    critic_loss     | 0.0371   |\n",
      "|    ent_coef        | 0.768    |\n",
      "|    ent_coef_loss   | -1.77    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 879      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=990, episode_reward=-60.54 +/- 9.28\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -60.5    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 990      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -10.8    |\n",
      "|    critic_loss     | 0.0226   |\n",
      "|    ent_coef        | 0.766    |\n",
      "|    ent_coef_loss   | -1.79    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 889      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1000, episode_reward=-65.72 +/- 5.78\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -65.7    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -11      |\n",
      "|    critic_loss     | 0.0467   |\n",
      "|    ent_coef        | 0.764    |\n",
      "|    ent_coef_loss   | -1.81    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 899      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1010, episode_reward=-83.10 +/- 9.01\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -83.1    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1010     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -11      |\n",
      "|    critic_loss     | 0.0356   |\n",
      "|    ent_coef        | 0.762    |\n",
      "|    ent_coef_loss   | -1.84    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 909      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1020, episode_reward=-81.02 +/- 7.19\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -81      |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1020     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -11      |\n",
      "|    critic_loss     | 0.0369   |\n",
      "|    ent_coef        | 0.759    |\n",
      "|    ent_coef_loss   | -1.86    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 919      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1030, episode_reward=-93.36 +/- 15.13\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -93.4    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1030     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -11.2    |\n",
      "|    critic_loss     | 0.0355   |\n",
      "|    ent_coef        | 0.757    |\n",
      "|    ent_coef_loss   | -1.88    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 929      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1040, episode_reward=-79.85 +/- 10.96\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -79.9    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1040     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -11.1    |\n",
      "|    critic_loss     | 0.0284   |\n",
      "|    ent_coef        | 0.755    |\n",
      "|    ent_coef_loss   | -1.89    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 939      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1050, episode_reward=-46.89 +/- 8.95\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -46.9    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1050     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -11.2    |\n",
      "|    critic_loss     | 0.0396   |\n",
      "|    ent_coef        | 0.752    |\n",
      "|    ent_coef_loss   | -1.91    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 949      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1060, episode_reward=-70.65 +/- 8.49\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -70.7    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1060     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -11.3    |\n",
      "|    critic_loss     | 0.0318   |\n",
      "|    ent_coef        | 0.75     |\n",
      "|    ent_coef_loss   | -1.93    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 959      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1070, episode_reward=-73.91 +/- 10.39\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -73.9    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1070     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -11.3    |\n",
      "|    critic_loss     | 0.0349   |\n",
      "|    ent_coef        | 0.748    |\n",
      "|    ent_coef_loss   | -1.95    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 969      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1080, episode_reward=-23.88 +/- 1.63\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -23.9    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1080     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -11.3    |\n",
      "|    critic_loss     | 0.0371   |\n",
      "|    ent_coef        | 0.746    |\n",
      "|    ent_coef_loss   | -1.97    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 979      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1090, episode_reward=-38.47 +/- 1.20\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -38.5    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1090     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -11.6    |\n",
      "|    critic_loss     | 0.0414   |\n",
      "|    ent_coef        | 0.743    |\n",
      "|    ent_coef_loss   | -1.99    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 989      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1100, episode_reward=-72.19 +/- 2.07\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -72.2    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1100     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -11.6    |\n",
      "|    critic_loss     | 0.0277   |\n",
      "|    ent_coef        | 0.741    |\n",
      "|    ent_coef_loss   | -2.02    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 999      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1110, episode_reward=-65.85 +/- 13.83\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -65.9    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1110     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -11.6    |\n",
      "|    critic_loss     | 0.0334   |\n",
      "|    ent_coef        | 0.739    |\n",
      "|    ent_coef_loss   | -2.03    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1009     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1120, episode_reward=-33.18 +/- 1.47\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -33.2    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1120     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -11.7    |\n",
      "|    critic_loss     | 0.0351   |\n",
      "|    ent_coef        | 0.737    |\n",
      "|    ent_coef_loss   | -2.05    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1019     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1130, episode_reward=-84.80 +/- 12.01\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -84.8    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1130     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -11.8    |\n",
      "|    critic_loss     | 0.0348   |\n",
      "|    ent_coef        | 0.735    |\n",
      "|    ent_coef_loss   | -2.08    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1029     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1140, episode_reward=-52.63 +/- 6.29\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -52.6    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1140     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -11.8    |\n",
      "|    critic_loss     | 0.035    |\n",
      "|    ent_coef        | 0.732    |\n",
      "|    ent_coef_loss   | -2.09    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1039     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1150, episode_reward=-72.08 +/- 6.13\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -72.1    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1150     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -11.9    |\n",
      "|    critic_loss     | 0.0308   |\n",
      "|    ent_coef        | 0.73     |\n",
      "|    ent_coef_loss   | -2.11    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1049     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1160, episode_reward=-34.53 +/- 3.46\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -34.5    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1160     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -11.9    |\n",
      "|    critic_loss     | 0.0308   |\n",
      "|    ent_coef        | 0.728    |\n",
      "|    ent_coef_loss   | -2.13    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1059     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1170, episode_reward=-41.18 +/- 5.06\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -41.2    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1170     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -12      |\n",
      "|    critic_loss     | 0.032    |\n",
      "|    ent_coef        | 0.726    |\n",
      "|    ent_coef_loss   | -2.16    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1069     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1180, episode_reward=-57.68 +/- 3.52\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -57.7    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1180     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -12.1    |\n",
      "|    critic_loss     | 0.0337   |\n",
      "|    ent_coef        | 0.724    |\n",
      "|    ent_coef_loss   | -2.17    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1079     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1190, episode_reward=-47.61 +/- 1.62\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -47.6    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1190     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -12.1    |\n",
      "|    critic_loss     | 0.0377   |\n",
      "|    ent_coef        | 0.722    |\n",
      "|    ent_coef_loss   | -2.2     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1089     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1200, episode_reward=-43.21 +/- 4.37\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -43.2    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1200     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -12.2    |\n",
      "|    critic_loss     | 0.0354   |\n",
      "|    ent_coef        | 0.719    |\n",
      "|    ent_coef_loss   | -2.21    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1099     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1210, episode_reward=-76.69 +/- 4.65\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -76.7    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1210     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -12.2    |\n",
      "|    critic_loss     | 0.0261   |\n",
      "|    ent_coef        | 0.717    |\n",
      "|    ent_coef_loss   | -2.23    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1109     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1220, episode_reward=-59.16 +/- 8.78\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -59.2    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1220     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -12.3    |\n",
      "|    critic_loss     | 0.0636   |\n",
      "|    ent_coef        | 0.715    |\n",
      "|    ent_coef_loss   | -2.26    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1119     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1230, episode_reward=-60.67 +/- 2.53\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -60.7    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1230     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -12.3    |\n",
      "|    critic_loss     | 0.0267   |\n",
      "|    ent_coef        | 0.713    |\n",
      "|    ent_coef_loss   | -2.28    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1129     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1240, episode_reward=-39.41 +/- 2.38\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -39.4    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1240     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -12.4    |\n",
      "|    critic_loss     | 0.0271   |\n",
      "|    ent_coef        | 0.711    |\n",
      "|    ent_coef_loss   | -2.29    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1139     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1250, episode_reward=-52.43 +/- 1.38\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -52.4    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1250     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -12.4    |\n",
      "|    critic_loss     | 0.0357   |\n",
      "|    ent_coef        | 0.709    |\n",
      "|    ent_coef_loss   | -2.31    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1149     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1260, episode_reward=-76.92 +/- 4.45\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -76.9    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1260     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -12.5    |\n",
      "|    critic_loss     | 0.0263   |\n",
      "|    ent_coef        | 0.707    |\n",
      "|    ent_coef_loss   | -2.34    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1159     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1270, episode_reward=-57.81 +/- 1.85\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -57.8    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1270     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -12.6    |\n",
      "|    critic_loss     | 0.0213   |\n",
      "|    ent_coef        | 0.704    |\n",
      "|    ent_coef_loss   | -2.38    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1169     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1280, episode_reward=-29.47 +/- 0.91\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -29.5    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1280     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -12.6    |\n",
      "|    critic_loss     | 0.0317   |\n",
      "|    ent_coef        | 0.702    |\n",
      "|    ent_coef_loss   | -2.38    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1179     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1290, episode_reward=-34.08 +/- 2.08\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -34.1    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1290     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -12.7    |\n",
      "|    critic_loss     | 0.0229   |\n",
      "|    ent_coef        | 0.7      |\n",
      "|    ent_coef_loss   | -2.4     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1189     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1300, episode_reward=-65.57 +/- 1.99\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -65.6    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1300     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -12.8    |\n",
      "|    critic_loss     | 0.0331   |\n",
      "|    ent_coef        | 0.698    |\n",
      "|    ent_coef_loss   | -2.41    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1199     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1310, episode_reward=-38.58 +/- 4.79\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -38.6    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1310     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -12.8    |\n",
      "|    critic_loss     | 0.0264   |\n",
      "|    ent_coef        | 0.696    |\n",
      "|    ent_coef_loss   | -2.44    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1209     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1320, episode_reward=-73.13 +/- 14.48\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -73.1    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1320     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -12.9    |\n",
      "|    critic_loss     | 0.0285   |\n",
      "|    ent_coef        | 0.694    |\n",
      "|    ent_coef_loss   | -2.45    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1219     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1330, episode_reward=-27.61 +/- 4.54\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -27.6    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1330     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -13      |\n",
      "|    critic_loss     | 0.0268   |\n",
      "|    ent_coef        | 0.692    |\n",
      "|    ent_coef_loss   | -2.48    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1229     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1340, episode_reward=-24.02 +/- 1.43\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -24      |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1340     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -12.9    |\n",
      "|    critic_loss     | 0.0273   |\n",
      "|    ent_coef        | 0.69     |\n",
      "|    ent_coef_loss   | -2.5     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1239     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1350, episode_reward=-30.26 +/- 1.15\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -30.3    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1350     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -13.1    |\n",
      "|    critic_loss     | 0.0235   |\n",
      "|    ent_coef        | 0.688    |\n",
      "|    ent_coef_loss   | -2.52    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1249     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1360, episode_reward=-59.04 +/- 1.39\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -59      |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1360     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -13.2    |\n",
      "|    critic_loss     | 0.0216   |\n",
      "|    ent_coef        | 0.686    |\n",
      "|    ent_coef_loss   | -2.54    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1259     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1370, episode_reward=-62.56 +/- 2.41\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -62.6    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1370     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -13.2    |\n",
      "|    critic_loss     | 0.0237   |\n",
      "|    ent_coef        | 0.684    |\n",
      "|    ent_coef_loss   | -2.55    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1269     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1380, episode_reward=-64.21 +/- 5.24\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -64.2    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1380     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -13.3    |\n",
      "|    critic_loss     | 0.0332   |\n",
      "|    ent_coef        | 0.682    |\n",
      "|    ent_coef_loss   | -2.57    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1279     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1390, episode_reward=-73.81 +/- 11.06\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -73.8    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1390     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -13.3    |\n",
      "|    critic_loss     | 0.0252   |\n",
      "|    ent_coef        | 0.68     |\n",
      "|    ent_coef_loss   | -2.61    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1289     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1400, episode_reward=-73.20 +/- 10.53\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -73.2    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1400     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -13.3    |\n",
      "|    critic_loss     | 0.022    |\n",
      "|    ent_coef        | 0.677    |\n",
      "|    ent_coef_loss   | -2.62    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1299     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1410, episode_reward=-52.20 +/- 1.87\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -52.2    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1410     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -13.5    |\n",
      "|    critic_loss     | 0.0215   |\n",
      "|    ent_coef        | 0.675    |\n",
      "|    ent_coef_loss   | -2.64    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1309     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1420, episode_reward=-82.08 +/- 6.93\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -82.1    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1420     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -13.4    |\n",
      "|    critic_loss     | 0.0353   |\n",
      "|    ent_coef        | 0.673    |\n",
      "|    ent_coef_loss   | -2.65    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1319     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1430, episode_reward=-83.53 +/- 6.55\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -83.5    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1430     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -13.6    |\n",
      "|    critic_loss     | 0.016    |\n",
      "|    ent_coef        | 0.671    |\n",
      "|    ent_coef_loss   | -2.68    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1329     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1440, episode_reward=-33.51 +/- 2.32\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -33.5    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1440     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -13.6    |\n",
      "|    critic_loss     | 0.0284   |\n",
      "|    ent_coef        | 0.669    |\n",
      "|    ent_coef_loss   | -2.7     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1339     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1450, episode_reward=-32.56 +/- 0.55\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -32.6    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1450     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -13.7    |\n",
      "|    critic_loss     | 0.0403   |\n",
      "|    ent_coef        | 0.667    |\n",
      "|    ent_coef_loss   | -2.72    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1349     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1460, episode_reward=-50.61 +/- 2.21\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -50.6    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1460     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -13.7    |\n",
      "|    critic_loss     | 0.0355   |\n",
      "|    ent_coef        | 0.665    |\n",
      "|    ent_coef_loss   | -2.74    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1359     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1470, episode_reward=-47.82 +/- 3.38\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -47.8    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1470     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -13.9    |\n",
      "|    critic_loss     | 0.0317   |\n",
      "|    ent_coef        | 0.663    |\n",
      "|    ent_coef_loss   | -2.77    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1369     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1480, episode_reward=-43.19 +/- 2.64\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -43.2    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1480     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -13.9    |\n",
      "|    critic_loss     | 0.0477   |\n",
      "|    ent_coef        | 0.661    |\n",
      "|    ent_coef_loss   | -2.78    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1379     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1490, episode_reward=-32.28 +/- 3.08\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -32.3    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1490     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -13.9    |\n",
      "|    critic_loss     | 0.0314   |\n",
      "|    ent_coef        | 0.659    |\n",
      "|    ent_coef_loss   | -2.8     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1389     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-54.86 +/- 2.52\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -54.9    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1500     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -13.9    |\n",
      "|    critic_loss     | 0.0271   |\n",
      "|    ent_coef        | 0.657    |\n",
      "|    ent_coef_loss   | -2.81    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1399     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1510, episode_reward=-57.37 +/- 2.58\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -57.4    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1510     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -14      |\n",
      "|    critic_loss     | 0.0291   |\n",
      "|    ent_coef        | 0.655    |\n",
      "|    ent_coef_loss   | -2.85    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1409     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1520, episode_reward=-53.22 +/- 2.03\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -53.2    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1520     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -14.1    |\n",
      "|    critic_loss     | 0.0328   |\n",
      "|    ent_coef        | 0.654    |\n",
      "|    ent_coef_loss   | -2.86    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1419     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1530, episode_reward=-64.96 +/- 5.44\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -65      |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1530     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -14.2    |\n",
      "|    critic_loss     | 0.0302   |\n",
      "|    ent_coef        | 0.652    |\n",
      "|    ent_coef_loss   | -2.89    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1429     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1540, episode_reward=-27.17 +/- 0.86\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -27.2    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1540     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -14.1    |\n",
      "|    critic_loss     | 0.0278   |\n",
      "|    ent_coef        | 0.65     |\n",
      "|    ent_coef_loss   | -2.89    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1439     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1550, episode_reward=-40.58 +/- 1.52\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -40.6    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1550     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -14.3    |\n",
      "|    critic_loss     | 0.0259   |\n",
      "|    ent_coef        | 0.648    |\n",
      "|    ent_coef_loss   | -2.92    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1449     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1560, episode_reward=-59.42 +/- 3.44\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -59.4    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1560     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -14.3    |\n",
      "|    critic_loss     | 0.0342   |\n",
      "|    ent_coef        | 0.646    |\n",
      "|    ent_coef_loss   | -2.93    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1459     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1570, episode_reward=-41.75 +/- 2.23\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -41.7    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1570     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -14.2    |\n",
      "|    critic_loss     | 0.0295   |\n",
      "|    ent_coef        | 0.644    |\n",
      "|    ent_coef_loss   | -2.96    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1469     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1580, episode_reward=-76.50 +/- 3.84\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -76.5    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1580     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -14.5    |\n",
      "|    critic_loss     | 0.027    |\n",
      "|    ent_coef        | 0.642    |\n",
      "|    ent_coef_loss   | -2.99    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1479     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1590, episode_reward=-27.99 +/- 2.11\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -28      |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1590     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -14.5    |\n",
      "|    critic_loss     | 0.0212   |\n",
      "|    ent_coef        | 0.64     |\n",
      "|    ent_coef_loss   | -3.01    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1489     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1600, episode_reward=-58.08 +/- 2.38\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -58.1    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1600     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -14.5    |\n",
      "|    critic_loss     | 0.0298   |\n",
      "|    ent_coef        | 0.638    |\n",
      "|    ent_coef_loss   | -3.02    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1499     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1610, episode_reward=-68.68 +/- 7.88\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -68.7    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1610     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -14.6    |\n",
      "|    critic_loss     | 0.0271   |\n",
      "|    ent_coef        | 0.636    |\n",
      "|    ent_coef_loss   | -3.05    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1509     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1620, episode_reward=-25.85 +/- 1.67\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -25.8    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1620     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -14.7    |\n",
      "|    critic_loss     | 0.0251   |\n",
      "|    ent_coef        | 0.634    |\n",
      "|    ent_coef_loss   | -3.07    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1519     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1630, episode_reward=-13.83 +/- 0.76\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -13.8    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1630     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -14.7    |\n",
      "|    critic_loss     | 0.0197   |\n",
      "|    ent_coef        | 0.632    |\n",
      "|    ent_coef_loss   | -3.09    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1529     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1640, episode_reward=-53.31 +/- 3.44\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -53.3    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1640     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -14.7    |\n",
      "|    critic_loss     | 0.0265   |\n",
      "|    ent_coef        | 0.63     |\n",
      "|    ent_coef_loss   | -3.11    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1539     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1650, episode_reward=-41.06 +/- 6.45\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -41.1    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1650     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -14.8    |\n",
      "|    critic_loss     | 0.0263   |\n",
      "|    ent_coef        | 0.629    |\n",
      "|    ent_coef_loss   | -3.13    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1549     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1660, episode_reward=-31.47 +/- 2.04\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -31.5    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1660     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -14.7    |\n",
      "|    critic_loss     | 0.0273   |\n",
      "|    ent_coef        | 0.627    |\n",
      "|    ent_coef_loss   | -3.14    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1559     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1670, episode_reward=-68.15 +/- 1.64\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -68.2    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1670     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -14.9    |\n",
      "|    critic_loss     | 0.0236   |\n",
      "|    ent_coef        | 0.625    |\n",
      "|    ent_coef_loss   | -3.17    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1569     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1680, episode_reward=-23.54 +/- 1.07\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -23.5    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1680     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -15      |\n",
      "|    critic_loss     | 0.0274   |\n",
      "|    ent_coef        | 0.623    |\n",
      "|    ent_coef_loss   | -3.18    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1579     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1690, episode_reward=-59.83 +/- 0.99\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -59.8    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1690     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -14.9    |\n",
      "|    critic_loss     | 0.0303   |\n",
      "|    ent_coef        | 0.621    |\n",
      "|    ent_coef_loss   | -3.19    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1589     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1700, episode_reward=-47.43 +/- 6.64\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -47.4    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1700     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -15.1    |\n",
      "|    critic_loss     | 0.022    |\n",
      "|    ent_coef        | 0.619    |\n",
      "|    ent_coef_loss   | -3.23    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1599     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1710, episode_reward=-88.58 +/- 11.23\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -88.6    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1710     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -15.1    |\n",
      "|    critic_loss     | 0.0205   |\n",
      "|    ent_coef        | 0.617    |\n",
      "|    ent_coef_loss   | -3.24    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1609     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1720, episode_reward=-51.82 +/- 3.75\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -51.8    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1720     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -15.2    |\n",
      "|    critic_loss     | 0.0257   |\n",
      "|    ent_coef        | 0.615    |\n",
      "|    ent_coef_loss   | -3.27    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1619     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1730, episode_reward=-61.90 +/- 5.66\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -61.9    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1730     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -15.2    |\n",
      "|    critic_loss     | 0.0265   |\n",
      "|    ent_coef        | 0.614    |\n",
      "|    ent_coef_loss   | -3.3     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1629     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1740, episode_reward=-66.20 +/- 2.60\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -66.2    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1740     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -15.3    |\n",
      "|    critic_loss     | 0.0321   |\n",
      "|    ent_coef        | 0.612    |\n",
      "|    ent_coef_loss   | -3.3     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1639     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1750, episode_reward=-59.78 +/- 1.79\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -59.8    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1750     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -15.3    |\n",
      "|    critic_loss     | 0.0285   |\n",
      "|    ent_coef        | 0.61     |\n",
      "|    ent_coef_loss   | -3.32    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1649     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1760, episode_reward=-65.07 +/- 3.07\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -65.1    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1760     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -15.5    |\n",
      "|    critic_loss     | 0.022    |\n",
      "|    ent_coef        | 0.608    |\n",
      "|    ent_coef_loss   | -3.35    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1659     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1770, episode_reward=-19.83 +/- 1.28\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -19.8    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1770     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -15.4    |\n",
      "|    critic_loss     | 0.0218   |\n",
      "|    ent_coef        | 0.606    |\n",
      "|    ent_coef_loss   | -3.38    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1669     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1780, episode_reward=-33.34 +/- 0.88\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -33.3    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1780     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -15.5    |\n",
      "|    critic_loss     | 0.0237   |\n",
      "|    ent_coef        | 0.604    |\n",
      "|    ent_coef_loss   | -3.39    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1679     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1790, episode_reward=-35.89 +/- 3.53\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -35.9    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1790     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -15.6    |\n",
      "|    critic_loss     | 0.0311   |\n",
      "|    ent_coef        | 0.603    |\n",
      "|    ent_coef_loss   | -3.41    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1689     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1800, episode_reward=-69.57 +/- 5.07\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -69.6    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1800     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -15.6    |\n",
      "|    critic_loss     | 0.0212   |\n",
      "|    ent_coef        | 0.601    |\n",
      "|    ent_coef_loss   | -3.44    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1699     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1810, episode_reward=-73.73 +/- 7.76\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -73.7    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1810     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -15.7    |\n",
      "|    critic_loss     | 0.0223   |\n",
      "|    ent_coef        | 0.599    |\n",
      "|    ent_coef_loss   | -3.44    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1709     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1820, episode_reward=-77.30 +/- 2.69\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -77.3    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1820     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -15.7    |\n",
      "|    critic_loss     | 0.0236   |\n",
      "|    ent_coef        | 0.597    |\n",
      "|    ent_coef_loss   | -3.46    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1719     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1830, episode_reward=-26.72 +/- 3.65\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -26.7    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1830     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -15.7    |\n",
      "|    critic_loss     | 0.0228   |\n",
      "|    ent_coef        | 0.595    |\n",
      "|    ent_coef_loss   | -3.5     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1729     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1840, episode_reward=-58.05 +/- 2.74\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -58      |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1840     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -15.6    |\n",
      "|    critic_loss     | 0.0183   |\n",
      "|    ent_coef        | 0.594    |\n",
      "|    ent_coef_loss   | -3.52    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1739     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1850, episode_reward=-59.48 +/- 4.71\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -59.5    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1850     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -15.7    |\n",
      "|    critic_loss     | 0.0288   |\n",
      "|    ent_coef        | 0.592    |\n",
      "|    ent_coef_loss   | -3.52    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1749     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1860, episode_reward=-58.24 +/- 3.82\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -58.2    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1860     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -15.8    |\n",
      "|    critic_loss     | 0.02     |\n",
      "|    ent_coef        | 0.59     |\n",
      "|    ent_coef_loss   | -3.54    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1759     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1870, episode_reward=-75.83 +/- 8.77\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -75.8    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1870     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -16      |\n",
      "|    critic_loss     | 0.0234   |\n",
      "|    ent_coef        | 0.588    |\n",
      "|    ent_coef_loss   | -3.58    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1769     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1880, episode_reward=-58.59 +/- 4.60\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -58.6    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1880     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -16      |\n",
      "|    critic_loss     | 0.0223   |\n",
      "|    ent_coef        | 0.587    |\n",
      "|    ent_coef_loss   | -3.59    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1779     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1890, episode_reward=-36.65 +/- 0.50\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -36.7    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1890     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -16      |\n",
      "|    critic_loss     | 0.0569   |\n",
      "|    ent_coef        | 0.585    |\n",
      "|    ent_coef_loss   | -3.6     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1789     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1900, episode_reward=-60.92 +/- 3.02\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -60.9    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1900     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -16      |\n",
      "|    critic_loss     | 0.0192   |\n",
      "|    ent_coef        | 0.583    |\n",
      "|    ent_coef_loss   | -3.64    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1799     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1910, episode_reward=-69.62 +/- 7.04\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -69.6    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1910     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -16      |\n",
      "|    critic_loss     | 0.0164   |\n",
      "|    ent_coef        | 0.581    |\n",
      "|    ent_coef_loss   | -3.66    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1809     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1920, episode_reward=-89.21 +/- 6.69\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -89.2    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1920     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -16.2    |\n",
      "|    critic_loss     | 0.0173   |\n",
      "|    ent_coef        | 0.58     |\n",
      "|    ent_coef_loss   | -3.68    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1819     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1930, episode_reward=-77.41 +/- 6.44\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -77.4    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1930     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -16.1    |\n",
      "|    critic_loss     | 0.0179   |\n",
      "|    ent_coef        | 0.578    |\n",
      "|    ent_coef_loss   | -3.71    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1829     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1940, episode_reward=-74.29 +/- 5.10\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -74.3    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1940     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -16.2    |\n",
      "|    critic_loss     | 0.0195   |\n",
      "|    ent_coef        | 0.576    |\n",
      "|    ent_coef_loss   | -3.71    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1839     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1950, episode_reward=-30.13 +/- 3.29\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -30.1    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1950     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -16.2    |\n",
      "|    critic_loss     | 0.0157   |\n",
      "|    ent_coef        | 0.574    |\n",
      "|    ent_coef_loss   | -3.75    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1849     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1960, episode_reward=-76.01 +/- 5.01\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -76      |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1960     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -16.4    |\n",
      "|    critic_loss     | 0.0138   |\n",
      "|    ent_coef        | 0.573    |\n",
      "|    ent_coef_loss   | -3.76    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1859     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1970, episode_reward=-32.74 +/- 1.25\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -32.7    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1970     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -16.4    |\n",
      "|    critic_loss     | 0.0209   |\n",
      "|    ent_coef        | 0.571    |\n",
      "|    ent_coef_loss   | -3.77    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1869     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1980, episode_reward=-45.00 +/- 4.96\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -45      |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1980     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -16.4    |\n",
      "|    critic_loss     | 0.0179   |\n",
      "|    ent_coef        | 0.569    |\n",
      "|    ent_coef_loss   | -3.77    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1879     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1990, episode_reward=-76.72 +/- 9.74\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -76.7    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 1990     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -16.4    |\n",
      "|    critic_loss     | 0.0269   |\n",
      "|    ent_coef        | 0.568    |\n",
      "|    ent_coef_loss   | -3.82    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1889     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-23.58 +/- 1.53\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -23.6    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -16.5    |\n",
      "|    critic_loss     | 0.0189   |\n",
      "|    ent_coef        | 0.566    |\n",
      "|    ent_coef_loss   | -3.84    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1899     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2010, episode_reward=-33.66 +/- 2.09\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -33.7    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2010     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -16.7    |\n",
      "|    critic_loss     | 0.0244   |\n",
      "|    ent_coef        | 0.564    |\n",
      "|    ent_coef_loss   | -3.85    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1909     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2020, episode_reward=-54.43 +/- 5.53\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -54.4    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2020     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -16.5    |\n",
      "|    critic_loss     | 0.0178   |\n",
      "|    ent_coef        | 0.563    |\n",
      "|    ent_coef_loss   | -3.86    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1919     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2030, episode_reward=-71.29 +/- 5.50\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -71.3    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2030     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -16.6    |\n",
      "|    critic_loss     | 0.0154   |\n",
      "|    ent_coef        | 0.561    |\n",
      "|    ent_coef_loss   | -3.88    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1929     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2040, episode_reward=-56.04 +/- 2.88\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -56      |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2040     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -16.7    |\n",
      "|    critic_loss     | 0.0192   |\n",
      "|    ent_coef        | 0.559    |\n",
      "|    ent_coef_loss   | -3.92    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1939     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2050, episode_reward=-57.59 +/- 5.00\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -57.6    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2050     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -16.8    |\n",
      "|    critic_loss     | 0.0176   |\n",
      "|    ent_coef        | 0.557    |\n",
      "|    ent_coef_loss   | -3.92    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1949     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2060, episode_reward=-51.19 +/- 5.13\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -51.2    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2060     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -16.9    |\n",
      "|    critic_loss     | 0.0229   |\n",
      "|    ent_coef        | 0.556    |\n",
      "|    ent_coef_loss   | -3.94    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1959     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2070, episode_reward=-96.79 +/- 6.45\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -96.8    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2070     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -16.8    |\n",
      "|    critic_loss     | 0.021    |\n",
      "|    ent_coef        | 0.554    |\n",
      "|    ent_coef_loss   | -3.97    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1969     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2080, episode_reward=-38.17 +/- 3.46\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -38.2    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2080     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -16.8    |\n",
      "|    critic_loss     | 0.0166   |\n",
      "|    ent_coef        | 0.552    |\n",
      "|    ent_coef_loss   | -3.99    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1979     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2090, episode_reward=-51.31 +/- 5.20\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -51.3    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2090     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -16.8    |\n",
      "|    critic_loss     | 0.0202   |\n",
      "|    ent_coef        | 0.551    |\n",
      "|    ent_coef_loss   | -4.02    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1989     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2100, episode_reward=-53.11 +/- 3.16\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -53.1    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2100     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -16.9    |\n",
      "|    critic_loss     | 0.0152   |\n",
      "|    ent_coef        | 0.549    |\n",
      "|    ent_coef_loss   | -4.04    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1999     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2110, episode_reward=-71.14 +/- 7.90\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -71.1    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2110     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -17.2    |\n",
      "|    critic_loss     | 0.0157   |\n",
      "|    ent_coef        | 0.548    |\n",
      "|    ent_coef_loss   | -4.06    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2009     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2120, episode_reward=-55.53 +/- 3.49\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -55.5    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2120     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -17.1    |\n",
      "|    critic_loss     | 0.0195   |\n",
      "|    ent_coef        | 0.546    |\n",
      "|    ent_coef_loss   | -4.07    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2019     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2130, episode_reward=-57.44 +/- 4.98\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -57.4    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2130     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -17.2    |\n",
      "|    critic_loss     | 0.0217   |\n",
      "|    ent_coef        | 0.544    |\n",
      "|    ent_coef_loss   | -4.09    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2029     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2140, episode_reward=-40.43 +/- 6.08\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -40.4    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2140     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -17.1    |\n",
      "|    critic_loss     | 0.0209   |\n",
      "|    ent_coef        | 0.543    |\n",
      "|    ent_coef_loss   | -4.1     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2039     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2150, episode_reward=-51.62 +/- 5.57\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -51.6    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2150     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -17.2    |\n",
      "|    critic_loss     | 0.0152   |\n",
      "|    ent_coef        | 0.541    |\n",
      "|    ent_coef_loss   | -4.12    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2049     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2160, episode_reward=-55.58 +/- 7.13\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -55.6    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2160     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -17.3    |\n",
      "|    critic_loss     | 0.0245   |\n",
      "|    ent_coef        | 0.539    |\n",
      "|    ent_coef_loss   | -4.15    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2059     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2170, episode_reward=-90.71 +/- 8.95\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -90.7    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2170     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -17.3    |\n",
      "|    critic_loss     | 0.0262   |\n",
      "|    ent_coef        | 0.538    |\n",
      "|    ent_coef_loss   | -4.19    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2069     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2180, episode_reward=-68.54 +/- 5.47\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -68.5    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2180     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -17.2    |\n",
      "|    critic_loss     | 0.0166   |\n",
      "|    ent_coef        | 0.536    |\n",
      "|    ent_coef_loss   | -4.18    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2079     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2190, episode_reward=-79.41 +/- 6.06\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -79.4    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2190     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -17.3    |\n",
      "|    critic_loss     | 0.0178   |\n",
      "|    ent_coef        | 0.535    |\n",
      "|    ent_coef_loss   | -4.22    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2089     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2200, episode_reward=-62.46 +/- 4.81\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -62.5    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2200     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -17.2    |\n",
      "|    critic_loss     | 0.0199   |\n",
      "|    ent_coef        | 0.533    |\n",
      "|    ent_coef_loss   | -4.22    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2099     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2210, episode_reward=-59.45 +/- 4.96\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -59.5    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2210     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -17.3    |\n",
      "|    critic_loss     | 0.0204   |\n",
      "|    ent_coef        | 0.531    |\n",
      "|    ent_coef_loss   | -4.23    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2109     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2220, episode_reward=-48.30 +/- 4.98\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -48.3    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2220     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -17.5    |\n",
      "|    critic_loss     | 0.0154   |\n",
      "|    ent_coef        | 0.53     |\n",
      "|    ent_coef_loss   | -4.28    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2119     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2230, episode_reward=-55.80 +/- 7.04\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -55.8    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2230     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -17.5    |\n",
      "|    critic_loss     | 0.0178   |\n",
      "|    ent_coef        | 0.528    |\n",
      "|    ent_coef_loss   | -4.29    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2129     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2240, episode_reward=-52.84 +/- 4.00\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -52.8    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2240     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -17.4    |\n",
      "|    critic_loss     | 0.0182   |\n",
      "|    ent_coef        | 0.527    |\n",
      "|    ent_coef_loss   | -4.29    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2139     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2250, episode_reward=-60.96 +/- 3.70\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -61      |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2250     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -17.6    |\n",
      "|    critic_loss     | 0.0184   |\n",
      "|    ent_coef        | 0.525    |\n",
      "|    ent_coef_loss   | -4.33    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2149     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2260, episode_reward=-71.79 +/- 3.44\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -71.8    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2260     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -17.6    |\n",
      "|    critic_loss     | 0.0168   |\n",
      "|    ent_coef        | 0.523    |\n",
      "|    ent_coef_loss   | -4.35    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2159     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2270, episode_reward=-79.30 +/- 3.49\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -79.3    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2270     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -17.6    |\n",
      "|    critic_loss     | 0.0139   |\n",
      "|    ent_coef        | 0.522    |\n",
      "|    ent_coef_loss   | -4.37    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2169     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2280, episode_reward=-62.93 +/- 1.43\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -62.9    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2280     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -17.7    |\n",
      "|    critic_loss     | 0.0207   |\n",
      "|    ent_coef        | 0.52     |\n",
      "|    ent_coef_loss   | -4.4     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2179     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2290, episode_reward=-64.86 +/- 3.70\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -64.9    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2290     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -17.8    |\n",
      "|    critic_loss     | 0.0143   |\n",
      "|    ent_coef        | 0.519    |\n",
      "|    ent_coef_loss   | -4.41    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2189     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2300, episode_reward=-58.00 +/- 3.33\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -58      |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2300     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -17.6    |\n",
      "|    critic_loss     | 0.0234   |\n",
      "|    ent_coef        | 0.517    |\n",
      "|    ent_coef_loss   | -4.42    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2199     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2310, episode_reward=-34.80 +/- 2.55\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -34.8    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2310     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -17.7    |\n",
      "|    critic_loss     | 0.0176   |\n",
      "|    ent_coef        | 0.516    |\n",
      "|    ent_coef_loss   | -4.44    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2209     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2320, episode_reward=-47.88 +/- 3.76\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -47.9    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2320     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -17.7    |\n",
      "|    critic_loss     | 0.0177   |\n",
      "|    ent_coef        | 0.514    |\n",
      "|    ent_coef_loss   | -4.46    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2219     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2330, episode_reward=-52.86 +/- 2.09\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -52.9    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2330     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -17.7    |\n",
      "|    critic_loss     | 0.0154   |\n",
      "|    ent_coef        | 0.513    |\n",
      "|    ent_coef_loss   | -4.48    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2229     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2340, episode_reward=-82.69 +/- 3.95\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -82.7    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2340     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -17.9    |\n",
      "|    critic_loss     | 0.0182   |\n",
      "|    ent_coef        | 0.511    |\n",
      "|    ent_coef_loss   | -4.51    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2239     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2350, episode_reward=-54.52 +/- 1.07\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -54.5    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2350     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -18      |\n",
      "|    critic_loss     | 0.0228   |\n",
      "|    ent_coef        | 0.51     |\n",
      "|    ent_coef_loss   | -4.53    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2249     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2360, episode_reward=-36.19 +/- 4.71\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -36.2    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2360     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -18.1    |\n",
      "|    critic_loss     | 0.026    |\n",
      "|    ent_coef        | 0.508    |\n",
      "|    ent_coef_loss   | -4.54    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2259     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2370, episode_reward=-73.51 +/- 5.43\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -73.5    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2370     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -18      |\n",
      "|    critic_loss     | 0.0189   |\n",
      "|    ent_coef        | 0.507    |\n",
      "|    ent_coef_loss   | -4.58    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2269     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2380, episode_reward=-81.77 +/- 7.46\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -81.8    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2380     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -18.1    |\n",
      "|    critic_loss     | 0.0188   |\n",
      "|    ent_coef        | 0.505    |\n",
      "|    ent_coef_loss   | -4.61    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2279     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2390, episode_reward=-48.99 +/- 4.32\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -49      |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2390     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -18.1    |\n",
      "|    critic_loss     | 0.0166   |\n",
      "|    ent_coef        | 0.503    |\n",
      "|    ent_coef_loss   | -4.61    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2289     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2400, episode_reward=-52.28 +/- 0.44\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -52.3    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2400     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -18.1    |\n",
      "|    critic_loss     | 0.02     |\n",
      "|    ent_coef        | 0.502    |\n",
      "|    ent_coef_loss   | -4.62    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2299     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2410, episode_reward=-68.94 +/- 3.70\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -68.9    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2410     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -18.2    |\n",
      "|    critic_loss     | 0.0219   |\n",
      "|    ent_coef        | 0.5      |\n",
      "|    ent_coef_loss   | -4.62    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2309     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2420, episode_reward=-60.22 +/- 4.01\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -60.2    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2420     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -18.2    |\n",
      "|    critic_loss     | 0.0232   |\n",
      "|    ent_coef        | 0.499    |\n",
      "|    ent_coef_loss   | -4.67    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2319     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2430, episode_reward=-83.38 +/- 3.16\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -83.4    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2430     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -18.2    |\n",
      "|    critic_loss     | 0.0146   |\n",
      "|    ent_coef        | 0.497    |\n",
      "|    ent_coef_loss   | -4.69    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2329     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2440, episode_reward=-78.17 +/- 4.20\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -78.2    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2440     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -18.3    |\n",
      "|    critic_loss     | 0.0213   |\n",
      "|    ent_coef        | 0.496    |\n",
      "|    ent_coef_loss   | -4.69    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2339     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2450, episode_reward=-52.15 +/- 5.10\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -52.2    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2450     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -18.3    |\n",
      "|    critic_loss     | 0.0239   |\n",
      "|    ent_coef        | 0.495    |\n",
      "|    ent_coef_loss   | -4.75    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2349     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2460, episode_reward=-33.86 +/- 3.37\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -33.9    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2460     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -18.4    |\n",
      "|    critic_loss     | 0.0176   |\n",
      "|    ent_coef        | 0.493    |\n",
      "|    ent_coef_loss   | -4.75    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2359     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2470, episode_reward=-73.78 +/- 3.71\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -73.8    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2470     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -18.5    |\n",
      "|    critic_loss     | 0.0204   |\n",
      "|    ent_coef        | 0.492    |\n",
      "|    ent_coef_loss   | -4.77    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2369     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2480, episode_reward=-47.84 +/- 1.69\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -47.8    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2480     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -18.4    |\n",
      "|    critic_loss     | 0.0114   |\n",
      "|    ent_coef        | 0.49     |\n",
      "|    ent_coef_loss   | -4.81    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2379     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2490, episode_reward=-52.27 +/- 1.21\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -52.3    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2490     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -18.4    |\n",
      "|    critic_loss     | 0.0231   |\n",
      "|    ent_coef        | 0.489    |\n",
      "|    ent_coef_loss   | -4.82    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2389     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-56.61 +/- 2.71\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -56.6    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2500     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -18.5    |\n",
      "|    critic_loss     | 0.0255   |\n",
      "|    ent_coef        | 0.487    |\n",
      "|    ent_coef_loss   | -4.83    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2399     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2510, episode_reward=-53.23 +/- 2.93\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -53.2    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2510     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -18.6    |\n",
      "|    critic_loss     | 0.0149   |\n",
      "|    ent_coef        | 0.486    |\n",
      "|    ent_coef_loss   | -4.84    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2409     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2520, episode_reward=-74.14 +/- 3.60\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -74.1    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2520     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -18.6    |\n",
      "|    critic_loss     | 0.0165   |\n",
      "|    ent_coef        | 0.484    |\n",
      "|    ent_coef_loss   | -4.87    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2419     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2530, episode_reward=-62.75 +/- 4.58\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -62.8    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2530     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -18.6    |\n",
      "|    critic_loss     | 0.0129   |\n",
      "|    ent_coef        | 0.483    |\n",
      "|    ent_coef_loss   | -4.88    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2429     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2540, episode_reward=-36.07 +/- 1.41\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -36.1    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2540     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -18.8    |\n",
      "|    critic_loss     | 0.0185   |\n",
      "|    ent_coef        | 0.481    |\n",
      "|    ent_coef_loss   | -4.92    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2439     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2550, episode_reward=-48.77 +/- 0.54\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -48.8    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2550     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -18.8    |\n",
      "|    critic_loss     | 0.0155   |\n",
      "|    ent_coef        | 0.48     |\n",
      "|    ent_coef_loss   | -4.91    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2449     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2560, episode_reward=-66.46 +/- 4.35\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -66.5    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2560     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -18.7    |\n",
      "|    critic_loss     | 0.0199   |\n",
      "|    ent_coef        | 0.478    |\n",
      "|    ent_coef_loss   | -4.93    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2459     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2570, episode_reward=-60.70 +/- 2.22\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -60.7    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2570     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -18.8    |\n",
      "|    critic_loss     | 0.0216   |\n",
      "|    ent_coef        | 0.477    |\n",
      "|    ent_coef_loss   | -4.95    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2469     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2580, episode_reward=-63.02 +/- 4.19\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -63      |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2580     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -18.9    |\n",
      "|    critic_loss     | 0.0176   |\n",
      "|    ent_coef        | 0.476    |\n",
      "|    ent_coef_loss   | -4.99    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2479     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2590, episode_reward=-89.71 +/- 4.26\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -89.7    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2590     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -18.9    |\n",
      "|    critic_loss     | 0.0151   |\n",
      "|    ent_coef        | 0.474    |\n",
      "|    ent_coef_loss   | -5.01    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2489     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2600, episode_reward=-91.50 +/- 4.55\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -91.5    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2600     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -18.9    |\n",
      "|    critic_loss     | 0.0154   |\n",
      "|    ent_coef        | 0.473    |\n",
      "|    ent_coef_loss   | -5.04    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2499     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2610, episode_reward=-48.11 +/- 2.06\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -48.1    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2610     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -18.9    |\n",
      "|    critic_loss     | 0.0188   |\n",
      "|    ent_coef        | 0.471    |\n",
      "|    ent_coef_loss   | -5.03    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2509     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2620, episode_reward=-45.36 +/- 2.04\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -45.4    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2620     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -18.9    |\n",
      "|    critic_loss     | 0.0189   |\n",
      "|    ent_coef        | 0.47     |\n",
      "|    ent_coef_loss   | -5.06    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2519     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2630, episode_reward=-42.48 +/- 2.95\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -42.5    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2630     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -19.1    |\n",
      "|    critic_loss     | 0.0164   |\n",
      "|    ent_coef        | 0.469    |\n",
      "|    ent_coef_loss   | -5.09    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2529     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2640, episode_reward=-56.45 +/- 1.22\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -56.4    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2640     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -19      |\n",
      "|    critic_loss     | 0.0213   |\n",
      "|    ent_coef        | 0.467    |\n",
      "|    ent_coef_loss   | -5.11    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2539     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2650, episode_reward=-73.79 +/- 3.46\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -73.8    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2650     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -19.1    |\n",
      "|    critic_loss     | 0.0219   |\n",
      "|    ent_coef        | 0.466    |\n",
      "|    ent_coef_loss   | -5.1     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2549     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2660, episode_reward=-58.02 +/- 2.36\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -58      |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2660     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -19      |\n",
      "|    critic_loss     | 0.0122   |\n",
      "|    ent_coef        | 0.464    |\n",
      "|    ent_coef_loss   | -5.17    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2559     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2670, episode_reward=-32.17 +/- 2.44\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -32.2    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2670     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -19.1    |\n",
      "|    critic_loss     | 0.0183   |\n",
      "|    ent_coef        | 0.463    |\n",
      "|    ent_coef_loss   | -5.16    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2569     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2680, episode_reward=-69.06 +/- 3.46\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -69.1    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2680     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -19.1    |\n",
      "|    critic_loss     | 0.012    |\n",
      "|    ent_coef        | 0.462    |\n",
      "|    ent_coef_loss   | -5.18    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2579     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2690, episode_reward=-80.76 +/- 4.03\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -80.8    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2690     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -19.2    |\n",
      "|    critic_loss     | 0.0129   |\n",
      "|    ent_coef        | 0.46     |\n",
      "|    ent_coef_loss   | -5.21    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2589     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2700, episode_reward=-60.25 +/- 1.59\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -60.3    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2700     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -19.3    |\n",
      "|    critic_loss     | 0.0252   |\n",
      "|    ent_coef        | 0.459    |\n",
      "|    ent_coef_loss   | -5.23    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2599     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2710, episode_reward=-55.86 +/- 1.39\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -55.9    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2710     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -19.2    |\n",
      "|    critic_loss     | 0.0164   |\n",
      "|    ent_coef        | 0.457    |\n",
      "|    ent_coef_loss   | -5.25    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2609     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2720, episode_reward=-45.76 +/- 2.05\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -45.8    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2720     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -19.2    |\n",
      "|    critic_loss     | 0.0107   |\n",
      "|    ent_coef        | 0.456    |\n",
      "|    ent_coef_loss   | -5.24    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2619     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2730, episode_reward=-43.47 +/- 2.08\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -43.5    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2730     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -19.3    |\n",
      "|    critic_loss     | 0.016    |\n",
      "|    ent_coef        | 0.455    |\n",
      "|    ent_coef_loss   | -5.3     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2629     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2740, episode_reward=-43.87 +/- 1.91\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -43.9    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2740     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -19.4    |\n",
      "|    critic_loss     | 0.0144   |\n",
      "|    ent_coef        | 0.453    |\n",
      "|    ent_coef_loss   | -5.32    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2639     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2750, episode_reward=-72.51 +/- 3.38\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -72.5    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2750     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -19.4    |\n",
      "|    critic_loss     | 0.0151   |\n",
      "|    ent_coef        | 0.452    |\n",
      "|    ent_coef_loss   | -5.32    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2649     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2760, episode_reward=-28.83 +/- 1.42\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -28.8    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2760     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -19.5    |\n",
      "|    critic_loss     | 0.0168   |\n",
      "|    ent_coef        | 0.451    |\n",
      "|    ent_coef_loss   | -5.33    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2659     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2770, episode_reward=-47.66 +/- 1.47\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -47.7    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2770     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -19.4    |\n",
      "|    critic_loss     | 0.0166   |\n",
      "|    ent_coef        | 0.449    |\n",
      "|    ent_coef_loss   | -5.36    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2669     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2780, episode_reward=-61.98 +/- 1.62\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -62      |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2780     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -19.5    |\n",
      "|    critic_loss     | 0.0115   |\n",
      "|    ent_coef        | 0.448    |\n",
      "|    ent_coef_loss   | -5.4     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2679     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2790, episode_reward=-75.45 +/- 3.25\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -75.4    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2790     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -19.5    |\n",
      "|    critic_loss     | 0.00971  |\n",
      "|    ent_coef        | 0.447    |\n",
      "|    ent_coef_loss   | -5.43    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2689     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2800, episode_reward=-68.53 +/- 3.57\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -68.5    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2800     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -19.4    |\n",
      "|    critic_loss     | 0.0171   |\n",
      "|    ent_coef        | 0.445    |\n",
      "|    ent_coef_loss   | -5.42    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2699     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2810, episode_reward=-49.67 +/- 1.76\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -49.7    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2810     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -19.6    |\n",
      "|    critic_loss     | 0.0153   |\n",
      "|    ent_coef        | 0.444    |\n",
      "|    ent_coef_loss   | -5.44    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2709     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2820, episode_reward=-54.47 +/- 1.19\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -54.5    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2820     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -19.5    |\n",
      "|    critic_loss     | 0.0123   |\n",
      "|    ent_coef        | 0.443    |\n",
      "|    ent_coef_loss   | -5.48    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2719     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2830, episode_reward=-53.25 +/- 1.12\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -53.2    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2830     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -19.4    |\n",
      "|    critic_loss     | 0.0109   |\n",
      "|    ent_coef        | 0.441    |\n",
      "|    ent_coef_loss   | -5.5     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2729     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2840, episode_reward=-62.10 +/- 0.92\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -62.1    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2840     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -19.7    |\n",
      "|    critic_loss     | 0.0126   |\n",
      "|    ent_coef        | 0.44     |\n",
      "|    ent_coef_loss   | -5.5     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2739     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2850, episode_reward=-34.83 +/- 0.66\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -34.8    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2850     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -19.7    |\n",
      "|    critic_loss     | 0.0133   |\n",
      "|    ent_coef        | 0.439    |\n",
      "|    ent_coef_loss   | -5.54    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2749     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2860, episode_reward=-68.37 +/- 1.70\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -68.4    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2860     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -19.8    |\n",
      "|    critic_loss     | 0.0198   |\n",
      "|    ent_coef        | 0.437    |\n",
      "|    ent_coef_loss   | -5.55    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2759     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2870, episode_reward=-52.40 +/- 0.89\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -52.4    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2870     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -19.7    |\n",
      "|    critic_loss     | 0.0186   |\n",
      "|    ent_coef        | 0.436    |\n",
      "|    ent_coef_loss   | -5.53    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2769     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2880, episode_reward=-44.31 +/- 0.77\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -44.3    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2880     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -19.6    |\n",
      "|    critic_loss     | 0.014    |\n",
      "|    ent_coef        | 0.435    |\n",
      "|    ent_coef_loss   | -5.58    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2779     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2890, episode_reward=-61.11 +/- 1.61\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -61.1    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2890     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -19.8    |\n",
      "|    critic_loss     | 0.0189   |\n",
      "|    ent_coef        | 0.433    |\n",
      "|    ent_coef_loss   | -5.6     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2789     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2900, episode_reward=-36.53 +/- 1.28\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -36.5    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2900     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -19.9    |\n",
      "|    critic_loss     | 0.0167   |\n",
      "|    ent_coef        | 0.432    |\n",
      "|    ent_coef_loss   | -5.6     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2799     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2910, episode_reward=-59.56 +/- 1.05\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -59.6    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2910     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -19.8    |\n",
      "|    critic_loss     | 0.00986  |\n",
      "|    ent_coef        | 0.431    |\n",
      "|    ent_coef_loss   | -5.66    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2809     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2920, episode_reward=-43.79 +/- 1.21\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -43.8    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2920     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -19.9    |\n",
      "|    critic_loss     | 0.0175   |\n",
      "|    ent_coef        | 0.43     |\n",
      "|    ent_coef_loss   | -5.66    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2819     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2930, episode_reward=-52.08 +/- 1.66\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -52.1    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2930     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -20.1    |\n",
      "|    critic_loss     | 0.0167   |\n",
      "|    ent_coef        | 0.428    |\n",
      "|    ent_coef_loss   | -5.68    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2829     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2940, episode_reward=-18.50 +/- 0.46\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -18.5    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2940     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -19.9    |\n",
      "|    critic_loss     | 0.0136   |\n",
      "|    ent_coef        | 0.427    |\n",
      "|    ent_coef_loss   | -5.68    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2839     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2950, episode_reward=-41.20 +/- 0.81\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -41.2    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2950     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -20.1    |\n",
      "|    critic_loss     | 0.0229   |\n",
      "|    ent_coef        | 0.426    |\n",
      "|    ent_coef_loss   | -5.71    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2849     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2960, episode_reward=-29.61 +/- 0.73\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -29.6    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2960     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -20.1    |\n",
      "|    critic_loss     | 0.0161   |\n",
      "|    ent_coef        | 0.424    |\n",
      "|    ent_coef_loss   | -5.74    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2859     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2970, episode_reward=-62.72 +/- 1.45\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -62.7    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2970     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -20      |\n",
      "|    critic_loss     | 0.0169   |\n",
      "|    ent_coef        | 0.423    |\n",
      "|    ent_coef_loss   | -5.76    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2869     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2980, episode_reward=-69.58 +/- 2.05\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -69.6    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2980     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -20.2    |\n",
      "|    critic_loss     | 0.0194   |\n",
      "|    ent_coef        | 0.422    |\n",
      "|    ent_coef_loss   | -5.81    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2879     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2990, episode_reward=-36.92 +/- 1.78\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -36.9    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 2990     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -20.1    |\n",
      "|    critic_loss     | 0.0153   |\n",
      "|    ent_coef        | 0.421    |\n",
      "|    ent_coef_loss   | -5.8     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2889     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-71.61 +/- 1.43\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -71.6    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -19.9    |\n",
      "|    critic_loss     | 0.014    |\n",
      "|    ent_coef        | 0.419    |\n",
      "|    ent_coef_loss   | -5.85    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2899     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3010, episode_reward=-51.24 +/- 0.64\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -51.2    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3010     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -20.2    |\n",
      "|    critic_loss     | 0.0131   |\n",
      "|    ent_coef        | 0.418    |\n",
      "|    ent_coef_loss   | -5.84    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2909     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3020, episode_reward=-50.20 +/- 0.52\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -50.2    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3020     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -20.3    |\n",
      "|    critic_loss     | 0.0139   |\n",
      "|    ent_coef        | 0.417    |\n",
      "|    ent_coef_loss   | -5.85    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2919     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3030, episode_reward=-59.00 +/- 0.77\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -59      |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3030     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -20.3    |\n",
      "|    critic_loss     | 0.0117   |\n",
      "|    ent_coef        | 0.416    |\n",
      "|    ent_coef_loss   | -5.87    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2929     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3040, episode_reward=-62.47 +/- 1.67\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -62.5    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3040     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -20.3    |\n",
      "|    critic_loss     | 0.0137   |\n",
      "|    ent_coef        | 0.414    |\n",
      "|    ent_coef_loss   | -5.89    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2939     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3050, episode_reward=-64.07 +/- 0.87\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -64.1    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3050     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -20.4    |\n",
      "|    critic_loss     | 0.00981  |\n",
      "|    ent_coef        | 0.413    |\n",
      "|    ent_coef_loss   | -5.95    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2949     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3060, episode_reward=-45.08 +/- 0.58\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -45.1    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3060     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -20.3    |\n",
      "|    critic_loss     | 0.0121   |\n",
      "|    ent_coef        | 0.412    |\n",
      "|    ent_coef_loss   | -5.94    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2959     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3070, episode_reward=-44.43 +/- 0.77\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -44.4    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3070     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -20.4    |\n",
      "|    critic_loss     | 0.0131   |\n",
      "|    ent_coef        | 0.411    |\n",
      "|    ent_coef_loss   | -5.96    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2969     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3080, episode_reward=-43.94 +/- 1.41\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -43.9    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3080     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -20.6    |\n",
      "|    critic_loss     | 0.0174   |\n",
      "|    ent_coef        | 0.41     |\n",
      "|    ent_coef_loss   | -6       |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2979     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3090, episode_reward=-59.77 +/- 1.11\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -59.8    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3090     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -20.4    |\n",
      "|    critic_loss     | 0.0138   |\n",
      "|    ent_coef        | 0.408    |\n",
      "|    ent_coef_loss   | -5.99    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2989     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3100, episode_reward=-65.02 +/- 1.64\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -65      |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3100     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -20.4    |\n",
      "|    critic_loss     | 0.0138   |\n",
      "|    ent_coef        | 0.407    |\n",
      "|    ent_coef_loss   | -6.01    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2999     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3110, episode_reward=-29.29 +/- 0.17\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -29.3    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3110     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -20.6    |\n",
      "|    critic_loss     | 0.0149   |\n",
      "|    ent_coef        | 0.406    |\n",
      "|    ent_coef_loss   | -6.04    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3009     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3120, episode_reward=-29.75 +/- 0.75\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -29.8    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3120     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -20.5    |\n",
      "|    critic_loss     | 0.0114   |\n",
      "|    ent_coef        | 0.405    |\n",
      "|    ent_coef_loss   | -6.05    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3019     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3130, episode_reward=-31.38 +/- 0.36\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -31.4    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3130     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -20.6    |\n",
      "|    critic_loss     | 0.0134   |\n",
      "|    ent_coef        | 0.403    |\n",
      "|    ent_coef_loss   | -6.05    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3029     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3140, episode_reward=-55.22 +/- 1.09\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -55.2    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3140     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -20.7    |\n",
      "|    critic_loss     | 0.0115   |\n",
      "|    ent_coef        | 0.402    |\n",
      "|    ent_coef_loss   | -6.1     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3039     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3150, episode_reward=-53.69 +/- 0.85\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -53.7    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3150     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -20.7    |\n",
      "|    critic_loss     | 0.0116   |\n",
      "|    ent_coef        | 0.401    |\n",
      "|    ent_coef_loss   | -6.08    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3049     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3160, episode_reward=-49.83 +/- 1.62\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -49.8    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3160     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -20.8    |\n",
      "|    critic_loss     | 0.013    |\n",
      "|    ent_coef        | 0.4      |\n",
      "|    ent_coef_loss   | -6.13    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3059     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3170, episode_reward=-77.19 +/- 1.68\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -77.2    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3170     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -20.4    |\n",
      "|    critic_loss     | 0.0119   |\n",
      "|    ent_coef        | 0.399    |\n",
      "|    ent_coef_loss   | -6.15    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3069     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3180, episode_reward=-53.55 +/- 0.54\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -53.6    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3180     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -20.8    |\n",
      "|    critic_loss     | 0.0167   |\n",
      "|    ent_coef        | 0.397    |\n",
      "|    ent_coef_loss   | -6.2     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3079     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3190, episode_reward=-18.10 +/- 0.55\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -18.1    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3190     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -20.8    |\n",
      "|    critic_loss     | 0.0114   |\n",
      "|    ent_coef        | 0.396    |\n",
      "|    ent_coef_loss   | -6.2     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3089     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3200, episode_reward=-65.51 +/- 1.19\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -65.5    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3200     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -20.7    |\n",
      "|    critic_loss     | 0.0128   |\n",
      "|    ent_coef        | 0.395    |\n",
      "|    ent_coef_loss   | -6.19    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3099     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3210, episode_reward=-54.78 +/- 0.64\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -54.8    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3210     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -20.6    |\n",
      "|    critic_loss     | 0.0123   |\n",
      "|    ent_coef        | 0.394    |\n",
      "|    ent_coef_loss   | -6.21    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3109     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3220, episode_reward=-37.22 +/- 0.78\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -37.2    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3220     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -20.8    |\n",
      "|    critic_loss     | 0.014    |\n",
      "|    ent_coef        | 0.393    |\n",
      "|    ent_coef_loss   | -6.24    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3119     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3230, episode_reward=-66.73 +/- 1.59\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -66.7    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3230     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -21      |\n",
      "|    critic_loss     | 0.011    |\n",
      "|    ent_coef        | 0.392    |\n",
      "|    ent_coef_loss   | -6.27    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3129     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3240, episode_reward=-57.03 +/- 1.12\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -57      |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3240     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -20.7    |\n",
      "|    critic_loss     | 0.0145   |\n",
      "|    ent_coef        | 0.39     |\n",
      "|    ent_coef_loss   | -6.31    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3139     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3250, episode_reward=-25.49 +/- 0.84\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -25.5    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3250     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -20.9    |\n",
      "|    critic_loss     | 0.00913  |\n",
      "|    ent_coef        | 0.389    |\n",
      "|    ent_coef_loss   | -6.32    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3149     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3260, episode_reward=-55.09 +/- 1.34\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -55.1    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3260     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -21      |\n",
      "|    critic_loss     | 0.015    |\n",
      "|    ent_coef        | 0.388    |\n",
      "|    ent_coef_loss   | -6.33    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3159     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3270, episode_reward=-69.41 +/- 1.76\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -69.4    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3270     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -21      |\n",
      "|    critic_loss     | 0.0111   |\n",
      "|    ent_coef        | 0.387    |\n",
      "|    ent_coef_loss   | -6.35    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3169     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3280, episode_reward=-58.61 +/- 1.30\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -58.6    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3280     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -20.8    |\n",
      "|    critic_loss     | 0.0136   |\n",
      "|    ent_coef        | 0.386    |\n",
      "|    ent_coef_loss   | -6.37    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3179     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3290, episode_reward=-39.73 +/- 1.25\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -39.7    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3290     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -21      |\n",
      "|    critic_loss     | 0.0128   |\n",
      "|    ent_coef        | 0.385    |\n",
      "|    ent_coef_loss   | -6.4     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3189     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3300, episode_reward=-68.60 +/- 1.47\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -68.6    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3300     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -20.9    |\n",
      "|    critic_loss     | 0.0108   |\n",
      "|    ent_coef        | 0.383    |\n",
      "|    ent_coef_loss   | -6.42    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3199     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3310, episode_reward=-73.13 +/- 1.64\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -73.1    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3310     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -21      |\n",
      "|    critic_loss     | 0.0116   |\n",
      "|    ent_coef        | 0.382    |\n",
      "|    ent_coef_loss   | -6.44    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3209     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3320, episode_reward=-46.45 +/- 0.53\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -46.5    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3320     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -20.9    |\n",
      "|    critic_loss     | 0.00993  |\n",
      "|    ent_coef        | 0.381    |\n",
      "|    ent_coef_loss   | -6.43    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3219     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3330, episode_reward=-23.60 +/- 0.47\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -23.6    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3330     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -21.3    |\n",
      "|    critic_loss     | 0.0124   |\n",
      "|    ent_coef        | 0.38     |\n",
      "|    ent_coef_loss   | -6.49    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3229     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3340, episode_reward=-27.31 +/- 0.32\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -27.3    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3340     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -21      |\n",
      "|    critic_loss     | 0.0192   |\n",
      "|    ent_coef        | 0.379    |\n",
      "|    ent_coef_loss   | -6.48    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3239     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3350, episode_reward=-52.17 +/- 0.93\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -52.2    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3350     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -21.2    |\n",
      "|    critic_loss     | 0.0157   |\n",
      "|    ent_coef        | 0.378    |\n",
      "|    ent_coef_loss   | -6.49    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3249     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3360, episode_reward=-33.41 +/- 0.20\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -33.4    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3360     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -21.1    |\n",
      "|    critic_loss     | 0.0134   |\n",
      "|    ent_coef        | 0.377    |\n",
      "|    ent_coef_loss   | -6.51    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3259     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3370, episode_reward=-47.22 +/- 1.19\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -47.2    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3370     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -21.1    |\n",
      "|    critic_loss     | 0.0109   |\n",
      "|    ent_coef        | 0.376    |\n",
      "|    ent_coef_loss   | -6.57    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3269     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3380, episode_reward=-19.17 +/- 0.38\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -19.2    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3380     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -21.3    |\n",
      "|    critic_loss     | 0.0154   |\n",
      "|    ent_coef        | 0.374    |\n",
      "|    ent_coef_loss   | -6.56    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3279     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3390, episode_reward=-53.45 +/- 0.68\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -53.4    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3390     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -21.1    |\n",
      "|    critic_loss     | 0.0119   |\n",
      "|    ent_coef        | 0.373    |\n",
      "|    ent_coef_loss   | -6.62    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3289     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3400, episode_reward=-38.99 +/- 0.87\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -39      |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3400     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -21.2    |\n",
      "|    critic_loss     | 0.0152   |\n",
      "|    ent_coef        | 0.372    |\n",
      "|    ent_coef_loss   | -6.6     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3299     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3410, episode_reward=-33.00 +/- 0.84\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -33      |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3410     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -21.3    |\n",
      "|    critic_loss     | 0.00976  |\n",
      "|    ent_coef        | 0.371    |\n",
      "|    ent_coef_loss   | -6.62    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3309     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3420, episode_reward=-51.90 +/- 0.98\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -51.9    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3420     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -21.2    |\n",
      "|    critic_loss     | 0.0125   |\n",
      "|    ent_coef        | 0.37     |\n",
      "|    ent_coef_loss   | -6.65    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3319     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3430, episode_reward=-37.95 +/- 0.46\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -38      |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3430     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -21.3    |\n",
      "|    critic_loss     | 0.00978  |\n",
      "|    ent_coef        | 0.369    |\n",
      "|    ent_coef_loss   | -6.63    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3329     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3440, episode_reward=-52.76 +/- 1.22\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -52.8    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3440     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -21.3    |\n",
      "|    critic_loss     | 0.0106   |\n",
      "|    ent_coef        | 0.368    |\n",
      "|    ent_coef_loss   | -6.7     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3339     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3450, episode_reward=-47.59 +/- 0.73\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -47.6    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3450     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -21.4    |\n",
      "|    critic_loss     | 0.0139   |\n",
      "|    ent_coef        | 0.367    |\n",
      "|    ent_coef_loss   | -6.67    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3349     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3460, episode_reward=-32.90 +/- 0.71\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -32.9    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3460     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -21.4    |\n",
      "|    critic_loss     | 0.00989  |\n",
      "|    ent_coef        | 0.366    |\n",
      "|    ent_coef_loss   | -6.69    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3359     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3470, episode_reward=-40.96 +/- 0.97\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -41      |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3470     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -21.6    |\n",
      "|    critic_loss     | 0.0104   |\n",
      "|    ent_coef        | 0.364    |\n",
      "|    ent_coef_loss   | -6.71    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3369     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3480, episode_reward=-75.64 +/- 1.28\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -75.6    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3480     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -21.4    |\n",
      "|    critic_loss     | 0.00865  |\n",
      "|    ent_coef        | 0.363    |\n",
      "|    ent_coef_loss   | -6.77    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3379     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3490, episode_reward=-60.74 +/- 0.76\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -60.7    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3490     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -21.5    |\n",
      "|    critic_loss     | 0.00968  |\n",
      "|    ent_coef        | 0.362    |\n",
      "|    ent_coef_loss   | -6.76    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3389     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=-60.04 +/- 0.80\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -60      |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3500     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -21.6    |\n",
      "|    critic_loss     | 0.0106   |\n",
      "|    ent_coef        | 0.361    |\n",
      "|    ent_coef_loss   | -6.75    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3399     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3510, episode_reward=-85.10 +/- 1.34\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -85.1    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3510     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -21.5    |\n",
      "|    critic_loss     | 0.0105   |\n",
      "|    ent_coef        | 0.36     |\n",
      "|    ent_coef_loss   | -6.78    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3409     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3520, episode_reward=-63.79 +/- 1.08\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -63.8    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3520     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -21.6    |\n",
      "|    critic_loss     | 0.0108   |\n",
      "|    ent_coef        | 0.359    |\n",
      "|    ent_coef_loss   | -6.81    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3419     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3530, episode_reward=-38.95 +/- 1.12\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -38.9    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3530     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -21.6    |\n",
      "|    critic_loss     | 0.011    |\n",
      "|    ent_coef        | 0.358    |\n",
      "|    ent_coef_loss   | -6.82    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3429     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3540, episode_reward=-68.17 +/- 1.27\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -68.2    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3540     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -21.8    |\n",
      "|    critic_loss     | 0.0108   |\n",
      "|    ent_coef        | 0.357    |\n",
      "|    ent_coef_loss   | -6.87    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3439     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3550, episode_reward=-59.75 +/- 0.86\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -59.8    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3550     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -21.8    |\n",
      "|    critic_loss     | 0.0088   |\n",
      "|    ent_coef        | 0.356    |\n",
      "|    ent_coef_loss   | -6.88    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3449     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3560, episode_reward=-54.70 +/- 0.91\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -54.7    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3560     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -21.8    |\n",
      "|    critic_loss     | 0.0132   |\n",
      "|    ent_coef        | 0.355    |\n",
      "|    ent_coef_loss   | -6.94    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3459     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3570, episode_reward=-20.24 +/- 0.41\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -20.2    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3570     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -21.8    |\n",
      "|    critic_loss     | 0.0115   |\n",
      "|    ent_coef        | 0.354    |\n",
      "|    ent_coef_loss   | -6.92    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3469     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3580, episode_reward=-50.08 +/- 1.06\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -50.1    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3580     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -21.6    |\n",
      "|    critic_loss     | 0.0114   |\n",
      "|    ent_coef        | 0.353    |\n",
      "|    ent_coef_loss   | -6.93    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3479     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3590, episode_reward=-49.31 +/- 0.74\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -49.3    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3590     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -21.9    |\n",
      "|    critic_loss     | 0.0118   |\n",
      "|    ent_coef        | 0.352    |\n",
      "|    ent_coef_loss   | -7.01    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3489     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3600, episode_reward=-50.33 +/- 0.96\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -50.3    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3600     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -21.7    |\n",
      "|    critic_loss     | 0.0125   |\n",
      "|    ent_coef        | 0.351    |\n",
      "|    ent_coef_loss   | -6.97    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3499     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3610, episode_reward=-59.90 +/- 0.97\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -59.9    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3610     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -21.8    |\n",
      "|    critic_loss     | 0.00919  |\n",
      "|    ent_coef        | 0.35     |\n",
      "|    ent_coef_loss   | -6.97    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3509     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3620, episode_reward=-45.78 +/- 1.28\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -45.8    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3620     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -21.8    |\n",
      "|    critic_loss     | 0.015    |\n",
      "|    ent_coef        | 0.348    |\n",
      "|    ent_coef_loss   | -7.07    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3519     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3630, episode_reward=-49.81 +/- 0.68\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -49.8    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3630     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -21.7    |\n",
      "|    critic_loss     | 0.0118   |\n",
      "|    ent_coef        | 0.347    |\n",
      "|    ent_coef_loss   | -7.02    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3529     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3640, episode_reward=-55.98 +/- 0.98\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -56      |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3640     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -21.7    |\n",
      "|    critic_loss     | 0.0175   |\n",
      "|    ent_coef        | 0.346    |\n",
      "|    ent_coef_loss   | -7.06    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3539     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3650, episode_reward=-32.07 +/- 0.88\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -32.1    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3650     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -21.7    |\n",
      "|    critic_loss     | 0.0139   |\n",
      "|    ent_coef        | 0.345    |\n",
      "|    ent_coef_loss   | -7.09    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3549     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3660, episode_reward=-79.83 +/- 1.39\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -79.8    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3660     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -21.7    |\n",
      "|    critic_loss     | 0.0126   |\n",
      "|    ent_coef        | 0.344    |\n",
      "|    ent_coef_loss   | -7.16    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3559     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3670, episode_reward=-50.20 +/- 0.26\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -50.2    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3670     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -21.9    |\n",
      "|    critic_loss     | 0.0098   |\n",
      "|    ent_coef        | 0.343    |\n",
      "|    ent_coef_loss   | -7.14    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3569     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3680, episode_reward=-34.65 +/- 0.20\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -34.6    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3680     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -21.9    |\n",
      "|    critic_loss     | 0.00846  |\n",
      "|    ent_coef        | 0.342    |\n",
      "|    ent_coef_loss   | -7.12    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3579     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3690, episode_reward=-18.37 +/- 0.29\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -18.4    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3690     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -21.9    |\n",
      "|    critic_loss     | 0.0122   |\n",
      "|    ent_coef        | 0.341    |\n",
      "|    ent_coef_loss   | -7.18    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3589     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3700, episode_reward=-58.87 +/- 1.15\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -58.9    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3700     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -22.1    |\n",
      "|    critic_loss     | 0.0102   |\n",
      "|    ent_coef        | 0.34     |\n",
      "|    ent_coef_loss   | -7.17    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3599     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3710, episode_reward=-48.40 +/- 1.12\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -48.4    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3710     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -21.6    |\n",
      "|    critic_loss     | 0.0124   |\n",
      "|    ent_coef        | 0.339    |\n",
      "|    ent_coef_loss   | -7.19    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3609     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3720, episode_reward=-41.58 +/- 0.24\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -41.6    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3720     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -21.8    |\n",
      "|    critic_loss     | 0.00951  |\n",
      "|    ent_coef        | 0.338    |\n",
      "|    ent_coef_loss   | -7.17    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3619     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3730, episode_reward=-54.54 +/- 0.87\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -54.5    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3730     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -22      |\n",
      "|    critic_loss     | 0.0103   |\n",
      "|    ent_coef        | 0.337    |\n",
      "|    ent_coef_loss   | -7.2     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3629     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3740, episode_reward=-23.36 +/- 0.28\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -23.4    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3740     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -21.9    |\n",
      "|    critic_loss     | 0.0163   |\n",
      "|    ent_coef        | 0.336    |\n",
      "|    ent_coef_loss   | -7.31    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3639     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3750, episode_reward=-55.26 +/- 1.07\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -55.3    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3750     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -22      |\n",
      "|    critic_loss     | 0.0111   |\n",
      "|    ent_coef        | 0.335    |\n",
      "|    ent_coef_loss   | -7.25    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3649     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3760, episode_reward=-44.50 +/- 0.62\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -44.5    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3760     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -22.2    |\n",
      "|    critic_loss     | 0.0123   |\n",
      "|    ent_coef        | 0.334    |\n",
      "|    ent_coef_loss   | -7.27    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3659     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3770, episode_reward=-73.85 +/- 1.32\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -73.8    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3770     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -22.1    |\n",
      "|    critic_loss     | 0.0119   |\n",
      "|    ent_coef        | 0.333    |\n",
      "|    ent_coef_loss   | -7.34    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3669     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3780, episode_reward=-37.55 +/- 0.12\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -37.5    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3780     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -22.1    |\n",
      "|    critic_loss     | 0.0106   |\n",
      "|    ent_coef        | 0.332    |\n",
      "|    ent_coef_loss   | -7.39    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3679     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3790, episode_reward=-40.85 +/- 0.15\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -40.9    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3790     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -22.4    |\n",
      "|    critic_loss     | 0.0113   |\n",
      "|    ent_coef        | 0.331    |\n",
      "|    ent_coef_loss   | -7.35    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3689     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3800, episode_reward=-59.21 +/- 1.01\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -59.2    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3800     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -22.2    |\n",
      "|    critic_loss     | 0.0116   |\n",
      "|    ent_coef        | 0.33     |\n",
      "|    ent_coef_loss   | -7.35    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3699     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3810, episode_reward=-49.45 +/- 0.51\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -49.5    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3810     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -22.2    |\n",
      "|    critic_loss     | 0.0151   |\n",
      "|    ent_coef        | 0.329    |\n",
      "|    ent_coef_loss   | -7.37    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3709     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3820, episode_reward=-73.94 +/- 0.74\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -73.9    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3820     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -22      |\n",
      "|    critic_loss     | 0.00929  |\n",
      "|    ent_coef        | 0.328    |\n",
      "|    ent_coef_loss   | -7.37    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3719     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3830, episode_reward=-33.08 +/- 0.84\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -33.1    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3830     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -22.3    |\n",
      "|    critic_loss     | 0.01     |\n",
      "|    ent_coef        | 0.327    |\n",
      "|    ent_coef_loss   | -7.44    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3729     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3840, episode_reward=-67.83 +/- 0.86\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -67.8    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3840     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -22.1    |\n",
      "|    critic_loss     | 0.00922  |\n",
      "|    ent_coef        | 0.326    |\n",
      "|    ent_coef_loss   | -7.43    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3739     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3850, episode_reward=-57.11 +/- 0.44\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -57.1    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3850     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -22.2    |\n",
      "|    critic_loss     | 0.0106   |\n",
      "|    ent_coef        | 0.325    |\n",
      "|    ent_coef_loss   | -7.41    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3749     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3860, episode_reward=-44.37 +/- 1.08\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -44.4    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3860     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -22.2    |\n",
      "|    critic_loss     | 0.0119   |\n",
      "|    ent_coef        | 0.324    |\n",
      "|    ent_coef_loss   | -7.41    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3759     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3870, episode_reward=-27.95 +/- 0.37\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -27.9    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3870     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -22.2    |\n",
      "|    critic_loss     | 0.00785  |\n",
      "|    ent_coef        | 0.323    |\n",
      "|    ent_coef_loss   | -7.44    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3769     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3880, episode_reward=-71.70 +/- 0.73\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -71.7    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3880     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -22.2    |\n",
      "|    critic_loss     | 0.00928  |\n",
      "|    ent_coef        | 0.322    |\n",
      "|    ent_coef_loss   | -7.51    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3779     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3890, episode_reward=-53.34 +/- 0.56\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -53.3    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3890     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -22.4    |\n",
      "|    critic_loss     | 0.0072   |\n",
      "|    ent_coef        | 0.322    |\n",
      "|    ent_coef_loss   | -7.54    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3789     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3900, episode_reward=-54.66 +/- 1.05\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -54.7    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3900     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -22.3    |\n",
      "|    critic_loss     | 0.0104   |\n",
      "|    ent_coef        | 0.321    |\n",
      "|    ent_coef_loss   | -7.58    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3799     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3910, episode_reward=-37.11 +/- 0.13\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -37.1    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3910     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -22.3    |\n",
      "|    critic_loss     | 0.00866  |\n",
      "|    ent_coef        | 0.32     |\n",
      "|    ent_coef_loss   | -7.55    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3809     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3920, episode_reward=-37.89 +/- 0.81\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -37.9    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3920     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -22.2    |\n",
      "|    critic_loss     | 0.00929  |\n",
      "|    ent_coef        | 0.319    |\n",
      "|    ent_coef_loss   | -7.61    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3819     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3930, episode_reward=-71.58 +/- 1.05\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -71.6    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3930     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -22.6    |\n",
      "|    critic_loss     | 0.00981  |\n",
      "|    ent_coef        | 0.318    |\n",
      "|    ent_coef_loss   | -7.63    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3829     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3940, episode_reward=-29.41 +/- 0.28\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -29.4    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3940     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -22.4    |\n",
      "|    critic_loss     | 0.00933  |\n",
      "|    ent_coef        | 0.317    |\n",
      "|    ent_coef_loss   | -7.62    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3839     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3950, episode_reward=-60.28 +/- 0.73\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -60.3    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3950     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -22.5    |\n",
      "|    critic_loss     | 0.00883  |\n",
      "|    ent_coef        | 0.316    |\n",
      "|    ent_coef_loss   | -7.73    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3849     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3960, episode_reward=-57.28 +/- 1.02\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -57.3    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3960     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -22.6    |\n",
      "|    critic_loss     | 0.0099   |\n",
      "|    ent_coef        | 0.315    |\n",
      "|    ent_coef_loss   | -7.69    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3859     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3970, episode_reward=-45.59 +/- 0.31\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -45.6    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3970     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -22.4    |\n",
      "|    critic_loss     | 0.00959  |\n",
      "|    ent_coef        | 0.314    |\n",
      "|    ent_coef_loss   | -7.66    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3869     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3980, episode_reward=-63.58 +/- 0.81\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -63.6    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3980     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -22.5    |\n",
      "|    critic_loss     | 0.00807  |\n",
      "|    ent_coef        | 0.313    |\n",
      "|    ent_coef_loss   | -7.69    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3879     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3990, episode_reward=-47.44 +/- 0.64\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -47.4    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 3990     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -22.6    |\n",
      "|    critic_loss     | 0.0107   |\n",
      "|    ent_coef        | 0.312    |\n",
      "|    ent_coef_loss   | -7.72    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3889     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-43.16 +/- 0.53\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -43.2    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -22.5    |\n",
      "|    critic_loss     | 0.00937  |\n",
      "|    ent_coef        | 0.311    |\n",
      "|    ent_coef_loss   | -7.76    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3899     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4010, episode_reward=-49.99 +/- 0.23\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -50      |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4010     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -22.7    |\n",
      "|    critic_loss     | 0.00836  |\n",
      "|    ent_coef        | 0.31     |\n",
      "|    ent_coef_loss   | -7.77    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3909     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4020, episode_reward=-70.93 +/- 1.41\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -70.9    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4020     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -22.6    |\n",
      "|    critic_loss     | 0.0106   |\n",
      "|    ent_coef        | 0.309    |\n",
      "|    ent_coef_loss   | -7.76    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3919     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4030, episode_reward=-42.35 +/- 0.67\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -42.4    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4030     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -22.8    |\n",
      "|    critic_loss     | 0.00966  |\n",
      "|    ent_coef        | 0.308    |\n",
      "|    ent_coef_loss   | -7.79    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3929     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4040, episode_reward=-38.95 +/- 0.93\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -39      |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4040     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -22.7    |\n",
      "|    critic_loss     | 0.0114   |\n",
      "|    ent_coef        | 0.307    |\n",
      "|    ent_coef_loss   | -7.86    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3939     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4050, episode_reward=-52.04 +/- 0.61\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -52      |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4050     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -22.7    |\n",
      "|    critic_loss     | 0.0103   |\n",
      "|    ent_coef        | 0.307    |\n",
      "|    ent_coef_loss   | -7.83    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3949     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4060, episode_reward=-47.94 +/- 0.94\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -47.9    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4060     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -22.7    |\n",
      "|    critic_loss     | 0.00655  |\n",
      "|    ent_coef        | 0.306    |\n",
      "|    ent_coef_loss   | -7.81    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3959     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4070, episode_reward=-68.74 +/- 0.67\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -68.7    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4070     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -22.6    |\n",
      "|    critic_loss     | 0.00774  |\n",
      "|    ent_coef        | 0.305    |\n",
      "|    ent_coef_loss   | -7.89    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3969     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4080, episode_reward=-84.79 +/- 1.22\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -84.8    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4080     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -22.5    |\n",
      "|    critic_loss     | 0.00962  |\n",
      "|    ent_coef        | 0.304    |\n",
      "|    ent_coef_loss   | -7.92    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3979     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4090, episode_reward=-61.17 +/- 0.87\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -61.2    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4090     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -22.7    |\n",
      "|    critic_loss     | 0.0108   |\n",
      "|    ent_coef        | 0.303    |\n",
      "|    ent_coef_loss   | -7.86    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3989     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4100, episode_reward=-58.83 +/- 0.53\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -58.8    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4100     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -22.6    |\n",
      "|    critic_loss     | 0.0124   |\n",
      "|    ent_coef        | 0.302    |\n",
      "|    ent_coef_loss   | -7.96    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3999     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4110, episode_reward=-64.07 +/- 1.20\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -64.1    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4110     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -22.9    |\n",
      "|    critic_loss     | 0.0115   |\n",
      "|    ent_coef        | 0.301    |\n",
      "|    ent_coef_loss   | -7.88    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4009     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4120, episode_reward=-47.65 +/- 1.05\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -47.7    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4120     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -22.4    |\n",
      "|    critic_loss     | 0.00989  |\n",
      "|    ent_coef        | 0.3      |\n",
      "|    ent_coef_loss   | -8       |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4019     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4130, episode_reward=-30.82 +/- 0.59\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -30.8    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4130     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -22.6    |\n",
      "|    critic_loss     | 0.00982  |\n",
      "|    ent_coef        | 0.299    |\n",
      "|    ent_coef_loss   | -7.99    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4029     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4140, episode_reward=-62.90 +/- 1.08\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -62.9    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4140     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -22.7    |\n",
      "|    critic_loss     | 0.0108   |\n",
      "|    ent_coef        | 0.298    |\n",
      "|    ent_coef_loss   | -8.06    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4039     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4150, episode_reward=-29.39 +/- 0.77\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -29.4    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4150     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -22.6    |\n",
      "|    critic_loss     | 0.00814  |\n",
      "|    ent_coef        | 0.298    |\n",
      "|    ent_coef_loss   | -8.03    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4049     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4160, episode_reward=-55.73 +/- 0.51\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -55.7    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4160     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -22.8    |\n",
      "|    critic_loss     | 0.00836  |\n",
      "|    ent_coef        | 0.297    |\n",
      "|    ent_coef_loss   | -8.08    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4059     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4170, episode_reward=-71.95 +/- 1.25\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -71.9    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4170     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23      |\n",
      "|    critic_loss     | 0.00755  |\n",
      "|    ent_coef        | 0.296    |\n",
      "|    ent_coef_loss   | -8.08    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4069     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4180, episode_reward=-37.38 +/- 0.55\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -37.4    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4180     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23      |\n",
      "|    critic_loss     | 0.00826  |\n",
      "|    ent_coef        | 0.295    |\n",
      "|    ent_coef_loss   | -8.04    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4079     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4190, episode_reward=-47.99 +/- 0.93\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -48      |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4190     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23      |\n",
      "|    critic_loss     | 0.00897  |\n",
      "|    ent_coef        | 0.294    |\n",
      "|    ent_coef_loss   | -8.1     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4089     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4200, episode_reward=-60.16 +/- 0.96\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -60.2    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4200     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -22.9    |\n",
      "|    critic_loss     | 0.0092   |\n",
      "|    ent_coef        | 0.293    |\n",
      "|    ent_coef_loss   | -8.16    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4099     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4210, episode_reward=-34.62 +/- 0.84\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -34.6    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4210     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.1    |\n",
      "|    critic_loss     | 0.0142   |\n",
      "|    ent_coef        | 0.292    |\n",
      "|    ent_coef_loss   | -8.16    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4109     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4220, episode_reward=-51.67 +/- 0.70\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -51.7    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4220     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23      |\n",
      "|    critic_loss     | 0.0111   |\n",
      "|    ent_coef        | 0.291    |\n",
      "|    ent_coef_loss   | -8.21    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4119     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4230, episode_reward=-25.38 +/- 0.27\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -25.4    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4230     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -22.7    |\n",
      "|    critic_loss     | 0.0089   |\n",
      "|    ent_coef        | 0.291    |\n",
      "|    ent_coef_loss   | -8.19    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4129     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4240, episode_reward=-40.38 +/- 0.89\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -40.4    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4240     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -22.7    |\n",
      "|    critic_loss     | 0.0104   |\n",
      "|    ent_coef        | 0.29     |\n",
      "|    ent_coef_loss   | -8.25    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4139     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4250, episode_reward=-65.50 +/- 1.18\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -65.5    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4250     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23      |\n",
      "|    critic_loss     | 0.00838  |\n",
      "|    ent_coef        | 0.289    |\n",
      "|    ent_coef_loss   | -8.23    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4149     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4260, episode_reward=-32.77 +/- 0.71\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -32.8    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4260     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.1    |\n",
      "|    critic_loss     | 0.00878  |\n",
      "|    ent_coef        | 0.288    |\n",
      "|    ent_coef_loss   | -8.2     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4159     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4270, episode_reward=-52.18 +/- 1.09\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -52.2    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4270     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.1    |\n",
      "|    critic_loss     | 0.0132   |\n",
      "|    ent_coef        | 0.287    |\n",
      "|    ent_coef_loss   | -8.31    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4169     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4280, episode_reward=-25.97 +/- 0.63\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -26      |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4280     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.1    |\n",
      "|    critic_loss     | 0.00776  |\n",
      "|    ent_coef        | 0.286    |\n",
      "|    ent_coef_loss   | -8.28    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4179     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4290, episode_reward=-45.12 +/- 0.18\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -45.1    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4290     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.1    |\n",
      "|    critic_loss     | 0.00862  |\n",
      "|    ent_coef        | 0.285    |\n",
      "|    ent_coef_loss   | -8.27    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4189     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4300, episode_reward=-42.19 +/- 0.59\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4300     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.2    |\n",
      "|    critic_loss     | 0.00886  |\n",
      "|    ent_coef        | 0.285    |\n",
      "|    ent_coef_loss   | -8.35    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4199     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4310, episode_reward=-56.29 +/- 0.40\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -56.3    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4310     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.1    |\n",
      "|    critic_loss     | 0.00927  |\n",
      "|    ent_coef        | 0.284    |\n",
      "|    ent_coef_loss   | -8.3     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4209     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4320, episode_reward=-48.54 +/- 0.92\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -48.5    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4320     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.1    |\n",
      "|    critic_loss     | 0.00972  |\n",
      "|    ent_coef        | 0.283    |\n",
      "|    ent_coef_loss   | -8.29    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4219     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4330, episode_reward=-17.73 +/- 0.58\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -17.7    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4330     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23      |\n",
      "|    critic_loss     | 0.00759  |\n",
      "|    ent_coef        | 0.282    |\n",
      "|    ent_coef_loss   | -8.39    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4229     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4340, episode_reward=-50.04 +/- 0.41\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -50      |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4340     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.1    |\n",
      "|    critic_loss     | 0.00889  |\n",
      "|    ent_coef        | 0.281    |\n",
      "|    ent_coef_loss   | -8.4     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4239     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4350, episode_reward=-24.77 +/- 0.28\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -24.8    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4350     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.2    |\n",
      "|    critic_loss     | 0.00928  |\n",
      "|    ent_coef        | 0.28     |\n",
      "|    ent_coef_loss   | -8.4     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4249     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4360, episode_reward=-23.50 +/- 0.20\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -23.5    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4360     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.1    |\n",
      "|    critic_loss     | 0.00825  |\n",
      "|    ent_coef        | 0.28     |\n",
      "|    ent_coef_loss   | -8.33    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4259     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4370, episode_reward=-69.29 +/- 0.48\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -69.3    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4370     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -22.9    |\n",
      "|    critic_loss     | 0.0107   |\n",
      "|    ent_coef        | 0.279    |\n",
      "|    ent_coef_loss   | -8.42    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4269     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4380, episode_reward=-59.72 +/- 1.02\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -59.7    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4380     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.3    |\n",
      "|    critic_loss     | 0.00854  |\n",
      "|    ent_coef        | 0.278    |\n",
      "|    ent_coef_loss   | -8.45    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4279     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4390, episode_reward=-51.68 +/- 0.95\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -51.7    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4390     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.2    |\n",
      "|    critic_loss     | 0.00675  |\n",
      "|    ent_coef        | 0.277    |\n",
      "|    ent_coef_loss   | -8.44    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4289     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4400, episode_reward=-61.41 +/- 0.46\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -61.4    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4400     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.1    |\n",
      "|    critic_loss     | 0.00791  |\n",
      "|    ent_coef        | 0.276    |\n",
      "|    ent_coef_loss   | -8.49    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4299     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4410, episode_reward=-48.68 +/- 0.61\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -48.7    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4410     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.1    |\n",
      "|    critic_loss     | 0.00953  |\n",
      "|    ent_coef        | 0.275    |\n",
      "|    ent_coef_loss   | -8.54    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4309     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4420, episode_reward=-27.91 +/- 0.07\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -27.9    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4420     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.3    |\n",
      "|    critic_loss     | 0.00994  |\n",
      "|    ent_coef        | 0.275    |\n",
      "|    ent_coef_loss   | -8.58    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4319     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4430, episode_reward=-45.78 +/- 0.82\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -45.8    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4430     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.2    |\n",
      "|    critic_loss     | 0.00765  |\n",
      "|    ent_coef        | 0.274    |\n",
      "|    ent_coef_loss   | -8.49    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4329     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4440, episode_reward=-22.96 +/- 0.33\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -23      |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4440     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.1    |\n",
      "|    critic_loss     | 0.00809  |\n",
      "|    ent_coef        | 0.273    |\n",
      "|    ent_coef_loss   | -8.49    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4339     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4450, episode_reward=-31.96 +/- 0.09\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -32      |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4450     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.1    |\n",
      "|    critic_loss     | 0.00929  |\n",
      "|    ent_coef        | 0.272    |\n",
      "|    ent_coef_loss   | -8.62    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4349     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4460, episode_reward=-74.81 +/- 0.85\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -74.8    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4460     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.3    |\n",
      "|    critic_loss     | 0.00836  |\n",
      "|    ent_coef        | 0.271    |\n",
      "|    ent_coef_loss   | -8.55    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4359     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4470, episode_reward=-9.44 +/- 0.22\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -9.44    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4470     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23      |\n",
      "|    critic_loss     | 0.00781  |\n",
      "|    ent_coef        | 0.271    |\n",
      "|    ent_coef_loss   | -8.58    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4369     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4480, episode_reward=-73.15 +/- 0.62\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -73.1    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4480     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.4    |\n",
      "|    critic_loss     | 0.00892  |\n",
      "|    ent_coef        | 0.27     |\n",
      "|    ent_coef_loss   | -8.66    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4379     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4490, episode_reward=-47.43 +/- 0.27\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -47.4    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4490     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.4    |\n",
      "|    critic_loss     | 0.00884  |\n",
      "|    ent_coef        | 0.269    |\n",
      "|    ent_coef_loss   | -8.65    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4389     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=-31.81 +/- 0.85\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -31.8    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4500     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.3    |\n",
      "|    critic_loss     | 0.00997  |\n",
      "|    ent_coef        | 0.268    |\n",
      "|    ent_coef_loss   | -8.62    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4399     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4510, episode_reward=-46.67 +/- 0.74\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -46.7    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4510     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.4    |\n",
      "|    critic_loss     | 0.00911  |\n",
      "|    ent_coef        | 0.267    |\n",
      "|    ent_coef_loss   | -8.71    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4409     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4520, episode_reward=-46.45 +/- 0.70\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -46.4    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4520     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23      |\n",
      "|    critic_loss     | 0.00892  |\n",
      "|    ent_coef        | 0.267    |\n",
      "|    ent_coef_loss   | -8.65    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4419     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4530, episode_reward=-14.05 +/- 0.24\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -14.1    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4530     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.4    |\n",
      "|    critic_loss     | 0.00782  |\n",
      "|    ent_coef        | 0.266    |\n",
      "|    ent_coef_loss   | -8.76    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4429     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4540, episode_reward=-60.16 +/- 0.87\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -60.2    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4540     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.2    |\n",
      "|    critic_loss     | 0.00888  |\n",
      "|    ent_coef        | 0.265    |\n",
      "|    ent_coef_loss   | -8.73    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4439     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4550, episode_reward=-46.66 +/- 0.21\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -46.7    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4550     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.4    |\n",
      "|    critic_loss     | 0.00805  |\n",
      "|    ent_coef        | 0.264    |\n",
      "|    ent_coef_loss   | -8.69    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4449     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4560, episode_reward=-65.96 +/- 0.65\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -66      |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4560     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.6    |\n",
      "|    critic_loss     | 0.00923  |\n",
      "|    ent_coef        | 0.263    |\n",
      "|    ent_coef_loss   | -8.7     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4459     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4570, episode_reward=-62.33 +/- 0.73\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -62.3    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4570     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.4    |\n",
      "|    critic_loss     | 0.00827  |\n",
      "|    ent_coef        | 0.263    |\n",
      "|    ent_coef_loss   | -8.82    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4469     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4580, episode_reward=-33.96 +/- 0.68\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -34      |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4580     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.4    |\n",
      "|    critic_loss     | 0.00892  |\n",
      "|    ent_coef        | 0.262    |\n",
      "|    ent_coef_loss   | -8.8     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4479     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4590, episode_reward=-51.89 +/- 0.56\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -51.9    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4590     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.3    |\n",
      "|    critic_loss     | 0.00771  |\n",
      "|    ent_coef        | 0.261    |\n",
      "|    ent_coef_loss   | -8.85    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4489     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4600, episode_reward=-47.73 +/- 0.56\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -47.7    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4600     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.5    |\n",
      "|    critic_loss     | 0.00899  |\n",
      "|    ent_coef        | 0.26     |\n",
      "|    ent_coef_loss   | -8.85    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4499     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4610, episode_reward=-61.97 +/- 0.43\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -62      |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4610     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.4    |\n",
      "|    critic_loss     | 0.00664  |\n",
      "|    ent_coef        | 0.26     |\n",
      "|    ent_coef_loss   | -8.88    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4509     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4620, episode_reward=-51.56 +/- 0.76\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -51.6    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4620     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.5    |\n",
      "|    critic_loss     | 0.00654  |\n",
      "|    ent_coef        | 0.259    |\n",
      "|    ent_coef_loss   | -8.75    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4519     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4630, episode_reward=-66.86 +/- 1.02\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -66.9    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4630     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.4    |\n",
      "|    critic_loss     | 0.00961  |\n",
      "|    ent_coef        | 0.258    |\n",
      "|    ent_coef_loss   | -8.87    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4529     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4640, episode_reward=-41.34 +/- 0.35\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -41.3    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4640     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.3    |\n",
      "|    critic_loss     | 0.0107   |\n",
      "|    ent_coef        | 0.257    |\n",
      "|    ent_coef_loss   | -8.9     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4539     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4650, episode_reward=-53.23 +/- 0.91\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -53.2    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4650     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.4    |\n",
      "|    critic_loss     | 0.0084   |\n",
      "|    ent_coef        | 0.257    |\n",
      "|    ent_coef_loss   | -8.9     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4549     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4660, episode_reward=-57.99 +/- 0.94\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -58      |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4660     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.4    |\n",
      "|    critic_loss     | 0.0071   |\n",
      "|    ent_coef        | 0.256    |\n",
      "|    ent_coef_loss   | -8.88    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4559     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4670, episode_reward=-51.50 +/- 0.25\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -51.5    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4670     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.6    |\n",
      "|    critic_loss     | 0.00698  |\n",
      "|    ent_coef        | 0.255    |\n",
      "|    ent_coef_loss   | -9.03    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4569     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4680, episode_reward=-61.92 +/- 0.95\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -61.9    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4680     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.6    |\n",
      "|    critic_loss     | 0.0081   |\n",
      "|    ent_coef        | 0.254    |\n",
      "|    ent_coef_loss   | -8.98    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4579     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4690, episode_reward=-49.62 +/- 0.63\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -49.6    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4690     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.4    |\n",
      "|    critic_loss     | 0.0103   |\n",
      "|    ent_coef        | 0.253    |\n",
      "|    ent_coef_loss   | -9.06    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4589     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4700, episode_reward=-21.20 +/- 0.30\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -21.2    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4700     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.7    |\n",
      "|    critic_loss     | 0.00794  |\n",
      "|    ent_coef        | 0.253    |\n",
      "|    ent_coef_loss   | -9       |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4599     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4710, episode_reward=-47.55 +/- 0.88\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -47.6    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4710     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.8    |\n",
      "|    critic_loss     | 0.0073   |\n",
      "|    ent_coef        | 0.252    |\n",
      "|    ent_coef_loss   | -9.15    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4609     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4720, episode_reward=-70.91 +/- 1.06\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -70.9    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4720     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.4    |\n",
      "|    critic_loss     | 0.0101   |\n",
      "|    ent_coef        | 0.251    |\n",
      "|    ent_coef_loss   | -9.02    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4619     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4730, episode_reward=-59.44 +/- 0.76\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -59.4    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4730     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.7    |\n",
      "|    critic_loss     | 0.0083   |\n",
      "|    ent_coef        | 0.251    |\n",
      "|    ent_coef_loss   | -9.06    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4629     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4740, episode_reward=-25.73 +/- 0.26\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -25.7    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4740     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.5    |\n",
      "|    critic_loss     | 0.00856  |\n",
      "|    ent_coef        | 0.25     |\n",
      "|    ent_coef_loss   | -9.01    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4639     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4750, episode_reward=-48.10 +/- 0.29\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -48.1    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4750     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.2    |\n",
      "|    critic_loss     | 0.0108   |\n",
      "|    ent_coef        | 0.249    |\n",
      "|    ent_coef_loss   | -9.06    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4649     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4760, episode_reward=-37.60 +/- 0.80\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -37.6    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4760     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.3    |\n",
      "|    critic_loss     | 0.00846  |\n",
      "|    ent_coef        | 0.248    |\n",
      "|    ent_coef_loss   | -9.08    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4659     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4770, episode_reward=-51.68 +/- 0.42\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -51.7    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4770     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.7    |\n",
      "|    critic_loss     | 0.00664  |\n",
      "|    ent_coef        | 0.248    |\n",
      "|    ent_coef_loss   | -9.15    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4669     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4780, episode_reward=-62.37 +/- 0.35\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -62.4    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4780     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.7    |\n",
      "|    critic_loss     | 0.00684  |\n",
      "|    ent_coef        | 0.247    |\n",
      "|    ent_coef_loss   | -9.11    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4679     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4790, episode_reward=-68.05 +/- 0.93\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -68      |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4790     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.7    |\n",
      "|    critic_loss     | 0.0084   |\n",
      "|    ent_coef        | 0.246    |\n",
      "|    ent_coef_loss   | -9.22    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4689     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4800, episode_reward=-73.06 +/- 1.00\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -73.1    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4800     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.5    |\n",
      "|    critic_loss     | 0.00693  |\n",
      "|    ent_coef        | 0.245    |\n",
      "|    ent_coef_loss   | -9.11    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4699     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4810, episode_reward=-63.19 +/- 0.98\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -63.2    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4810     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.7    |\n",
      "|    critic_loss     | 0.00658  |\n",
      "|    ent_coef        | 0.245    |\n",
      "|    ent_coef_loss   | -9.18    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4709     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4820, episode_reward=-58.56 +/- 0.92\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -58.6    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4820     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.7    |\n",
      "|    critic_loss     | 0.00834  |\n",
      "|    ent_coef        | 0.244    |\n",
      "|    ent_coef_loss   | -9.25    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4719     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4830, episode_reward=-51.61 +/- 0.44\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -51.6    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4830     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.8    |\n",
      "|    critic_loss     | 0.00843  |\n",
      "|    ent_coef        | 0.243    |\n",
      "|    ent_coef_loss   | -9.25    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4729     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4840, episode_reward=-51.13 +/- 0.79\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -51.1    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4840     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.1    |\n",
      "|    critic_loss     | 0.00801  |\n",
      "|    ent_coef        | 0.242    |\n",
      "|    ent_coef_loss   | -9.33    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4739     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4850, episode_reward=-46.58 +/- 0.06\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -46.6    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4850     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.8    |\n",
      "|    critic_loss     | 0.00734  |\n",
      "|    ent_coef        | 0.242    |\n",
      "|    ent_coef_loss   | -9.24    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4749     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4860, episode_reward=-50.50 +/- 0.82\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -50.5    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4860     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.9    |\n",
      "|    critic_loss     | 0.00792  |\n",
      "|    ent_coef        | 0.241    |\n",
      "|    ent_coef_loss   | -9.33    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4759     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4870, episode_reward=-47.15 +/- 0.18\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -47.2    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4870     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.7    |\n",
      "|    critic_loss     | 0.00737  |\n",
      "|    ent_coef        | 0.24     |\n",
      "|    ent_coef_loss   | -9.36    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4769     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4880, episode_reward=-51.15 +/- 0.78\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -51.1    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4880     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.7    |\n",
      "|    critic_loss     | 0.0063   |\n",
      "|    ent_coef        | 0.24     |\n",
      "|    ent_coef_loss   | -9.38    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4779     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4890, episode_reward=-80.26 +/- 0.53\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -80.3    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4890     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.9    |\n",
      "|    critic_loss     | 0.0066   |\n",
      "|    ent_coef        | 0.239    |\n",
      "|    ent_coef_loss   | -9.33    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4789     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4900, episode_reward=-33.47 +/- 0.53\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -33.5    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4900     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.7    |\n",
      "|    critic_loss     | 0.00754  |\n",
      "|    ent_coef        | 0.238    |\n",
      "|    ent_coef_loss   | -9.42    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4799     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4910, episode_reward=-30.60 +/- 0.52\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -30.6    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4910     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.5    |\n",
      "|    critic_loss     | 0.00807  |\n",
      "|    ent_coef        | 0.237    |\n",
      "|    ent_coef_loss   | -9.3     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4809     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4920, episode_reward=-48.01 +/- 0.78\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -48      |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4920     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24      |\n",
      "|    critic_loss     | 0.00652  |\n",
      "|    ent_coef        | 0.237    |\n",
      "|    ent_coef_loss   | -9.46    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4819     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4930, episode_reward=-35.44 +/- 0.47\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -35.4    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4930     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.6    |\n",
      "|    critic_loss     | 0.00828  |\n",
      "|    ent_coef        | 0.236    |\n",
      "|    ent_coef_loss   | -9.48    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4829     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4940, episode_reward=-64.21 +/- 0.65\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -64.2    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4940     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.8    |\n",
      "|    critic_loss     | 0.00808  |\n",
      "|    ent_coef        | 0.235    |\n",
      "|    ent_coef_loss   | -9.48    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4839     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4950, episode_reward=-53.58 +/- 0.62\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -53.6    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4950     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.8    |\n",
      "|    critic_loss     | 0.00592  |\n",
      "|    ent_coef        | 0.235    |\n",
      "|    ent_coef_loss   | -9.4     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4849     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4960, episode_reward=-64.38 +/- 0.70\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -64.4    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4960     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.8    |\n",
      "|    critic_loss     | 0.0055   |\n",
      "|    ent_coef        | 0.234    |\n",
      "|    ent_coef_loss   | -9.48    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4859     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4970, episode_reward=-19.52 +/- 0.17\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -19.5    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4970     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.6    |\n",
      "|    critic_loss     | 0.00692  |\n",
      "|    ent_coef        | 0.233    |\n",
      "|    ent_coef_loss   | -9.37    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4869     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4980, episode_reward=-19.36 +/- 0.14\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -19.4    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4980     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.7    |\n",
      "|    critic_loss     | 0.00679  |\n",
      "|    ent_coef        | 0.233    |\n",
      "|    ent_coef_loss   | -9.5     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4879     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4990, episode_reward=-71.38 +/- 0.52\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -71.4    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 4990     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.7    |\n",
      "|    critic_loss     | 0.0076   |\n",
      "|    ent_coef        | 0.232    |\n",
      "|    ent_coef_loss   | -9.51    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4889     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=-41.68 +/- 0.61\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -41.7    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.9    |\n",
      "|    critic_loss     | 0.00602  |\n",
      "|    ent_coef        | 0.231    |\n",
      "|    ent_coef_loss   | -9.48    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4899     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5010, episode_reward=-59.54 +/- 0.57\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -59.5    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5010     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.1    |\n",
      "|    critic_loss     | 0.00802  |\n",
      "|    ent_coef        | 0.231    |\n",
      "|    ent_coef_loss   | -9.68    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4909     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5020, episode_reward=-42.44 +/- 0.22\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -42.4    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5020     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.9    |\n",
      "|    critic_loss     | 0.00789  |\n",
      "|    ent_coef        | 0.23     |\n",
      "|    ent_coef_loss   | -9.51    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4919     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5030, episode_reward=-53.87 +/- 0.77\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -53.9    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5030     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24      |\n",
      "|    critic_loss     | 0.00612  |\n",
      "|    ent_coef        | 0.229    |\n",
      "|    ent_coef_loss   | -9.59    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4929     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5040, episode_reward=-43.64 +/- 0.16\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -43.6    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5040     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.8    |\n",
      "|    critic_loss     | 0.00757  |\n",
      "|    ent_coef        | 0.229    |\n",
      "|    ent_coef_loss   | -9.64    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4939     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5050, episode_reward=-77.57 +/- 0.85\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -77.6    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5050     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.7    |\n",
      "|    critic_loss     | 0.00686  |\n",
      "|    ent_coef        | 0.228    |\n",
      "|    ent_coef_loss   | -9.61    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4949     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5060, episode_reward=-24.63 +/- 0.50\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -24.6    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5060     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.8    |\n",
      "|    critic_loss     | 0.00762  |\n",
      "|    ent_coef        | 0.227    |\n",
      "|    ent_coef_loss   | -9.62    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4959     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5070, episode_reward=-27.63 +/- 0.03\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -27.6    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5070     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.8    |\n",
      "|    critic_loss     | 0.00752  |\n",
      "|    ent_coef        | 0.227    |\n",
      "|    ent_coef_loss   | -9.65    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4969     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5080, episode_reward=-58.92 +/- 0.28\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -58.9    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5080     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24      |\n",
      "|    critic_loss     | 0.00799  |\n",
      "|    ent_coef        | 0.226    |\n",
      "|    ent_coef_loss   | -9.77    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4979     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5090, episode_reward=-23.57 +/- 0.49\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -23.6    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5090     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.9    |\n",
      "|    critic_loss     | 0.0105   |\n",
      "|    ent_coef        | 0.225    |\n",
      "|    ent_coef_loss   | -9.71    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4989     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5100, episode_reward=-25.17 +/- 0.55\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -25.2    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5100     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.9    |\n",
      "|    critic_loss     | 0.00771  |\n",
      "|    ent_coef        | 0.225    |\n",
      "|    ent_coef_loss   | -9.75    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4999     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5110, episode_reward=-34.81 +/- 0.06\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -34.8    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5110     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.8    |\n",
      "|    critic_loss     | 0.00631  |\n",
      "|    ent_coef        | 0.224    |\n",
      "|    ent_coef_loss   | -9.73    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5009     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5120, episode_reward=-35.02 +/- 0.05\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -35      |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5120     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24      |\n",
      "|    critic_loss     | 0.00813  |\n",
      "|    ent_coef        | 0.223    |\n",
      "|    ent_coef_loss   | -9.66    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5019     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5130, episode_reward=-14.48 +/- 0.47\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -14.5    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5130     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.2    |\n",
      "|    critic_loss     | 0.0072   |\n",
      "|    ent_coef        | 0.223    |\n",
      "|    ent_coef_loss   | -9.81    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5029     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5140, episode_reward=-68.40 +/- 0.46\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -68.4    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5140     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.3    |\n",
      "|    critic_loss     | 0.00552  |\n",
      "|    ent_coef        | 0.222    |\n",
      "|    ent_coef_loss   | -9.86    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5039     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5150, episode_reward=-17.95 +/- 0.07\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -17.9    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5150     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.1    |\n",
      "|    critic_loss     | 0.00785  |\n",
      "|    ent_coef        | 0.221    |\n",
      "|    ent_coef_loss   | -9.88    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5049     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5160, episode_reward=-52.92 +/- 0.39\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -52.9    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5160     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.1    |\n",
      "|    critic_loss     | 0.00779  |\n",
      "|    ent_coef        | 0.221    |\n",
      "|    ent_coef_loss   | -9.85    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5059     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5170, episode_reward=-65.41 +/- 0.86\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -65.4    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5170     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.7    |\n",
      "|    critic_loss     | 0.00692  |\n",
      "|    ent_coef        | 0.22     |\n",
      "|    ent_coef_loss   | -9.76    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5069     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5180, episode_reward=-33.21 +/- 0.29\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -33.2    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5180     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.9    |\n",
      "|    critic_loss     | 0.00973  |\n",
      "|    ent_coef        | 0.219    |\n",
      "|    ent_coef_loss   | -9.86    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5079     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5190, episode_reward=-78.48 +/- 0.87\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -78.5    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5190     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.1    |\n",
      "|    critic_loss     | 0.00668  |\n",
      "|    ent_coef        | 0.219    |\n",
      "|    ent_coef_loss   | -9.9     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5089     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5200, episode_reward=-41.88 +/- 0.49\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -41.9    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5200     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.9    |\n",
      "|    critic_loss     | 0.00593  |\n",
      "|    ent_coef        | 0.218    |\n",
      "|    ent_coef_loss   | -9.79    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5099     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5210, episode_reward=-35.98 +/- 0.82\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -36      |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5210     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.2    |\n",
      "|    critic_loss     | 0.00722  |\n",
      "|    ent_coef        | 0.217    |\n",
      "|    ent_coef_loss   | -9.92    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5109     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5220, episode_reward=-15.50 +/- 0.16\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -15.5    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5220     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.3    |\n",
      "|    critic_loss     | 0.00597  |\n",
      "|    ent_coef        | 0.217    |\n",
      "|    ent_coef_loss   | -10      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5119     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5230, episode_reward=-74.50 +/- 0.87\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -74.5    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5230     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.8    |\n",
      "|    critic_loss     | 0.00653  |\n",
      "|    ent_coef        | 0.216    |\n",
      "|    ent_coef_loss   | -9.89    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5129     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5240, episode_reward=-31.03 +/- 0.24\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -31      |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5240     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.2    |\n",
      "|    critic_loss     | 0.0078   |\n",
      "|    ent_coef        | 0.215    |\n",
      "|    ent_coef_loss   | -9.98    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5139     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5250, episode_reward=-32.66 +/- 0.53\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -32.7    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5250     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.3    |\n",
      "|    critic_loss     | 0.00615  |\n",
      "|    ent_coef        | 0.215    |\n",
      "|    ent_coef_loss   | -9.94    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5149     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5260, episode_reward=-44.21 +/- 0.69\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -44.2    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5260     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.3    |\n",
      "|    critic_loss     | 0.00767  |\n",
      "|    ent_coef        | 0.214    |\n",
      "|    ent_coef_loss   | -10      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5159     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5270, episode_reward=-65.89 +/- 0.91\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -65.9    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5270     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.1    |\n",
      "|    critic_loss     | 0.00801  |\n",
      "|    ent_coef        | 0.214    |\n",
      "|    ent_coef_loss   | -9.95    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5169     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5280, episode_reward=-41.60 +/- 0.47\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -41.6    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5280     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.2    |\n",
      "|    critic_loss     | 0.00529  |\n",
      "|    ent_coef        | 0.213    |\n",
      "|    ent_coef_loss   | -10.1    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5179     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5290, episode_reward=-18.38 +/- 0.02\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -18.4    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5290     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.3    |\n",
      "|    critic_loss     | 0.00765  |\n",
      "|    ent_coef        | 0.212    |\n",
      "|    ent_coef_loss   | -10.1    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5189     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5300, episode_reward=-29.26 +/- 0.50\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -29.3    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5300     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.8    |\n",
      "|    critic_loss     | 0.00717  |\n",
      "|    ent_coef        | 0.212    |\n",
      "|    ent_coef_loss   | -9.75    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5199     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5310, episode_reward=-47.60 +/- 0.13\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -47.6    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5310     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.2    |\n",
      "|    critic_loss     | 0.00721  |\n",
      "|    ent_coef        | 0.211    |\n",
      "|    ent_coef_loss   | -10.1    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5209     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5320, episode_reward=-74.00 +/- 0.89\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -74      |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5320     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.1    |\n",
      "|    critic_loss     | 0.00629  |\n",
      "|    ent_coef        | 0.21     |\n",
      "|    ent_coef_loss   | -10.1    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5219     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5330, episode_reward=-30.47 +/- 0.50\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -30.5    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5330     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.2    |\n",
      "|    critic_loss     | 0.00684  |\n",
      "|    ent_coef        | 0.21     |\n",
      "|    ent_coef_loss   | -10.1    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5229     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5340, episode_reward=-31.92 +/- 0.15\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -31.9    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5340     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.9    |\n",
      "|    critic_loss     | 0.00628  |\n",
      "|    ent_coef        | 0.209    |\n",
      "|    ent_coef_loss   | -10.1    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5239     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5350, episode_reward=-49.89 +/- 0.54\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -49.9    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5350     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24      |\n",
      "|    critic_loss     | 0.00711  |\n",
      "|    ent_coef        | 0.209    |\n",
      "|    ent_coef_loss   | -10.2    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5249     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5360, episode_reward=-30.18 +/- 0.17\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -30.2    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5360     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.3    |\n",
      "|    critic_loss     | 0.00643  |\n",
      "|    ent_coef        | 0.208    |\n",
      "|    ent_coef_loss   | -10.1    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5259     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5370, episode_reward=-66.41 +/- 0.73\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -66.4    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5370     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.1    |\n",
      "|    critic_loss     | 0.00646  |\n",
      "|    ent_coef        | 0.207    |\n",
      "|    ent_coef_loss   | -10.1    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5269     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5380, episode_reward=-55.85 +/- 0.77\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -55.9    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5380     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.1    |\n",
      "|    critic_loss     | 0.00667  |\n",
      "|    ent_coef        | 0.207    |\n",
      "|    ent_coef_loss   | -10.2    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5279     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5390, episode_reward=-79.37 +/- 1.00\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -79.4    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5390     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.2    |\n",
      "|    critic_loss     | 0.00708  |\n",
      "|    ent_coef        | 0.206    |\n",
      "|    ent_coef_loss   | -10.2    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5289     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5400, episode_reward=-42.31 +/- 0.80\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -42.3    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5400     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24      |\n",
      "|    critic_loss     | 0.00592  |\n",
      "|    ent_coef        | 0.206    |\n",
      "|    ent_coef_loss   | -10.2    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5299     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5410, episode_reward=-50.83 +/- 0.46\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -50.8    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5410     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.2    |\n",
      "|    critic_loss     | 0.00527  |\n",
      "|    ent_coef        | 0.205    |\n",
      "|    ent_coef_loss   | -10.3    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5309     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5420, episode_reward=-43.52 +/- 0.63\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -43.5    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5420     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.2    |\n",
      "|    critic_loss     | 0.00707  |\n",
      "|    ent_coef        | 0.204    |\n",
      "|    ent_coef_loss   | -10.2    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5319     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5430, episode_reward=-29.91 +/- 0.12\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -29.9    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5430     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.2    |\n",
      "|    critic_loss     | 0.00798  |\n",
      "|    ent_coef        | 0.204    |\n",
      "|    ent_coef_loss   | -10.3    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5329     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5440, episode_reward=-51.23 +/- 0.82\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -51.2    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5440     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.6    |\n",
      "|    critic_loss     | 0.00558  |\n",
      "|    ent_coef        | 0.203    |\n",
      "|    ent_coef_loss   | -10.2    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5339     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5450, episode_reward=-21.05 +/- 0.32\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -21.1    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5450     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.1    |\n",
      "|    critic_loss     | 0.00615  |\n",
      "|    ent_coef        | 0.203    |\n",
      "|    ent_coef_loss   | -10.2    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5349     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5460, episode_reward=-37.46 +/- 0.03\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -37.5    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5460     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.4    |\n",
      "|    critic_loss     | 0.00759  |\n",
      "|    ent_coef        | 0.202    |\n",
      "|    ent_coef_loss   | -10.4    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5359     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5470, episode_reward=-42.65 +/- 0.33\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -42.6    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5470     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24      |\n",
      "|    critic_loss     | 0.00691  |\n",
      "|    ent_coef        | 0.201    |\n",
      "|    ent_coef_loss   | -10.2    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5369     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5480, episode_reward=-55.30 +/- 0.67\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -55.3    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5480     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.9    |\n",
      "|    critic_loss     | 0.00623  |\n",
      "|    ent_coef        | 0.201    |\n",
      "|    ent_coef_loss   | -10.3    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5379     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5490, episode_reward=-43.58 +/- 0.36\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -43.6    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5490     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.2    |\n",
      "|    critic_loss     | 0.00632  |\n",
      "|    ent_coef        | 0.2      |\n",
      "|    ent_coef_loss   | -10.3    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5389     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=-12.63 +/- 0.16\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -12.6    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5500     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.4    |\n",
      "|    critic_loss     | 0.00537  |\n",
      "|    ent_coef        | 0.2      |\n",
      "|    ent_coef_loss   | -10.4    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5399     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5510, episode_reward=-41.67 +/- 0.06\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -41.7    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5510     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.2    |\n",
      "|    critic_loss     | 0.00548  |\n",
      "|    ent_coef        | 0.199    |\n",
      "|    ent_coef_loss   | -10.5    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5409     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5520, episode_reward=-60.00 +/- 0.40\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -60      |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5520     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.2    |\n",
      "|    critic_loss     | 0.00659  |\n",
      "|    ent_coef        | 0.198    |\n",
      "|    ent_coef_loss   | -10.4    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5419     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5530, episode_reward=-47.69 +/- 0.79\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -47.7    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5530     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.4    |\n",
      "|    critic_loss     | 0.00615  |\n",
      "|    ent_coef        | 0.198    |\n",
      "|    ent_coef_loss   | -10.4    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5429     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5540, episode_reward=-52.95 +/- 0.52\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -52.9    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5540     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.2    |\n",
      "|    critic_loss     | 0.00646  |\n",
      "|    ent_coef        | 0.197    |\n",
      "|    ent_coef_loss   | -10.5    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5439     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5550, episode_reward=-51.68 +/- 0.18\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -51.7    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5550     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.1    |\n",
      "|    critic_loss     | 0.00523  |\n",
      "|    ent_coef        | 0.197    |\n",
      "|    ent_coef_loss   | -10.3    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5449     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5560, episode_reward=-35.52 +/- 0.04\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -35.5    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5560     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.2    |\n",
      "|    critic_loss     | 0.0087   |\n",
      "|    ent_coef        | 0.196    |\n",
      "|    ent_coef_loss   | -10.3    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5459     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5570, episode_reward=-29.51 +/- 0.55\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -29.5    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5570     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.3    |\n",
      "|    critic_loss     | 0.00637  |\n",
      "|    ent_coef        | 0.195    |\n",
      "|    ent_coef_loss   | -10.4    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5469     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5580, episode_reward=-85.60 +/- 0.98\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -85.6    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5580     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.4    |\n",
      "|    critic_loss     | 0.00731  |\n",
      "|    ent_coef        | 0.195    |\n",
      "|    ent_coef_loss   | -10.5    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5479     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5590, episode_reward=-69.70 +/- 0.52\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -69.7    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5590     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.2    |\n",
      "|    critic_loss     | 0.00588  |\n",
      "|    ent_coef        | 0.194    |\n",
      "|    ent_coef_loss   | -10.5    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5489     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5600, episode_reward=-69.87 +/- 0.88\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -69.9    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5600     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.4    |\n",
      "|    critic_loss     | 0.00766  |\n",
      "|    ent_coef        | 0.194    |\n",
      "|    ent_coef_loss   | -10.6    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5499     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5610, episode_reward=-59.75 +/- 0.60\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -59.7    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5610     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.2    |\n",
      "|    critic_loss     | 0.0063   |\n",
      "|    ent_coef        | 0.193    |\n",
      "|    ent_coef_loss   | -10.7    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5509     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5620, episode_reward=-28.87 +/- 0.43\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -28.9    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5620     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.3    |\n",
      "|    critic_loss     | 0.00585  |\n",
      "|    ent_coef        | 0.193    |\n",
      "|    ent_coef_loss   | -10.5    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5519     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5630, episode_reward=-68.90 +/- 0.61\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -68.9    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5630     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.4    |\n",
      "|    critic_loss     | 0.00581  |\n",
      "|    ent_coef        | 0.192    |\n",
      "|    ent_coef_loss   | -10.7    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5529     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5640, episode_reward=-65.26 +/- 0.56\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -65.3    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5640     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.2    |\n",
      "|    critic_loss     | 0.00611  |\n",
      "|    ent_coef        | 0.191    |\n",
      "|    ent_coef_loss   | -10.3    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5539     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5650, episode_reward=-73.27 +/- 0.57\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -73.3    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5650     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.4    |\n",
      "|    critic_loss     | 0.00764  |\n",
      "|    ent_coef        | 0.191    |\n",
      "|    ent_coef_loss   | -10.7    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5549     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5660, episode_reward=-46.15 +/- 0.15\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -46.2    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5660     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.4    |\n",
      "|    critic_loss     | 0.00589  |\n",
      "|    ent_coef        | 0.19     |\n",
      "|    ent_coef_loss   | -10.7    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5559     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5670, episode_reward=-44.21 +/- 0.15\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -44.2    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5670     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.2    |\n",
      "|    critic_loss     | 0.00654  |\n",
      "|    ent_coef        | 0.19     |\n",
      "|    ent_coef_loss   | -10.7    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5569     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5680, episode_reward=-58.11 +/- 0.54\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -58.1    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5680     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.3    |\n",
      "|    critic_loss     | 0.00579  |\n",
      "|    ent_coef        | 0.189    |\n",
      "|    ent_coef_loss   | -10.7    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5579     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5690, episode_reward=-60.27 +/- 0.92\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -60.3    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5690     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.4    |\n",
      "|    critic_loss     | 0.00559  |\n",
      "|    ent_coef        | 0.189    |\n",
      "|    ent_coef_loss   | -10.8    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5589     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5700, episode_reward=-77.75 +/- 0.85\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -77.7    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5700     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.4    |\n",
      "|    critic_loss     | 0.00614  |\n",
      "|    ent_coef        | 0.188    |\n",
      "|    ent_coef_loss   | -10.8    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5599     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5710, episode_reward=-28.49 +/- 0.12\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -28.5    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5710     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.1    |\n",
      "|    critic_loss     | 0.00703  |\n",
      "|    ent_coef        | 0.188    |\n",
      "|    ent_coef_loss   | -10.6    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5609     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5720, episode_reward=-12.47 +/- 0.12\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -12.5    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5720     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.4    |\n",
      "|    critic_loss     | 0.00535  |\n",
      "|    ent_coef        | 0.187    |\n",
      "|    ent_coef_loss   | -10.8    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5619     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5730, episode_reward=-62.12 +/- 0.83\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -62.1    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5730     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.5    |\n",
      "|    critic_loss     | 0.00649  |\n",
      "|    ent_coef        | 0.186    |\n",
      "|    ent_coef_loss   | -10.9    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5629     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5740, episode_reward=-61.99 +/- 0.80\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -62      |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5740     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.1    |\n",
      "|    critic_loss     | 0.00565  |\n",
      "|    ent_coef        | 0.186    |\n",
      "|    ent_coef_loss   | -10.6    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5639     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5750, episode_reward=-73.43 +/- 0.88\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -73.4    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5750     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.3    |\n",
      "|    critic_loss     | 0.00588  |\n",
      "|    ent_coef        | 0.185    |\n",
      "|    ent_coef_loss   | -10.8    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5649     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5760, episode_reward=-50.27 +/- 0.78\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -50.3    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5760     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.3    |\n",
      "|    critic_loss     | 0.00521  |\n",
      "|    ent_coef        | 0.185    |\n",
      "|    ent_coef_loss   | -11      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5659     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5770, episode_reward=-31.53 +/- 0.03\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -31.5    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5770     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.1    |\n",
      "|    critic_loss     | 0.00646  |\n",
      "|    ent_coef        | 0.184    |\n",
      "|    ent_coef_loss   | -10.7    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5669     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5780, episode_reward=-48.22 +/- 0.47\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -48.2    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5780     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.6    |\n",
      "|    critic_loss     | 0.00706  |\n",
      "|    ent_coef        | 0.184    |\n",
      "|    ent_coef_loss   | -10.9    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5679     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5790, episode_reward=-63.38 +/- 0.61\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -63.4    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5790     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.1    |\n",
      "|    critic_loss     | 0.00597  |\n",
      "|    ent_coef        | 0.183    |\n",
      "|    ent_coef_loss   | -10.8    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5689     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5800, episode_reward=-58.38 +/- 0.69\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -58.4    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5800     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.5    |\n",
      "|    critic_loss     | 0.00754  |\n",
      "|    ent_coef        | 0.183    |\n",
      "|    ent_coef_loss   | -10.9    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5699     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5810, episode_reward=-47.18 +/- 0.77\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -47.2    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5810     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.3    |\n",
      "|    critic_loss     | 0.00585  |\n",
      "|    ent_coef        | 0.182    |\n",
      "|    ent_coef_loss   | -10.7    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5709     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5820, episode_reward=-66.55 +/- 0.90\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -66.6    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5820     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.1    |\n",
      "|    critic_loss     | 0.0068   |\n",
      "|    ent_coef        | 0.182    |\n",
      "|    ent_coef_loss   | -10.8    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5719     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5830, episode_reward=-39.05 +/- 0.59\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -39.1    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5830     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.1    |\n",
      "|    critic_loss     | 0.00632  |\n",
      "|    ent_coef        | 0.181    |\n",
      "|    ent_coef_loss   | -10.9    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5729     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5840, episode_reward=-61.59 +/- 0.82\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -61.6    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5840     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.4    |\n",
      "|    critic_loss     | 0.00679  |\n",
      "|    ent_coef        | 0.181    |\n",
      "|    ent_coef_loss   | -10.9    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5739     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5850, episode_reward=-22.43 +/- 0.43\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -22.4    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5850     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.2    |\n",
      "|    critic_loss     | 0.00545  |\n",
      "|    ent_coef        | 0.18     |\n",
      "|    ent_coef_loss   | -10.8    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5749     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5860, episode_reward=-62.30 +/- 0.78\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -62.3    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5860     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.4    |\n",
      "|    critic_loss     | 0.00508  |\n",
      "|    ent_coef        | 0.179    |\n",
      "|    ent_coef_loss   | -11      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5759     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5870, episode_reward=-22.48 +/- 0.52\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -22.5    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5870     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.4    |\n",
      "|    critic_loss     | 0.00627  |\n",
      "|    ent_coef        | 0.179    |\n",
      "|    ent_coef_loss   | -10.9    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5769     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5880, episode_reward=-48.82 +/- 0.77\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -48.8    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5880     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.6    |\n",
      "|    critic_loss     | 0.00599  |\n",
      "|    ent_coef        | 0.178    |\n",
      "|    ent_coef_loss   | -11.1    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5779     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5890, episode_reward=-44.82 +/- 0.36\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -44.8    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5890     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.6    |\n",
      "|    critic_loss     | 0.00629  |\n",
      "|    ent_coef        | 0.178    |\n",
      "|    ent_coef_loss   | -11      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5789     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5900, episode_reward=-34.50 +/- 0.51\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -34.5    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5900     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.4    |\n",
      "|    critic_loss     | 0.00663  |\n",
      "|    ent_coef        | 0.177    |\n",
      "|    ent_coef_loss   | -11      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5799     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5910, episode_reward=-33.43 +/- 0.65\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -33.4    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5910     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.6    |\n",
      "|    critic_loss     | 0.00558  |\n",
      "|    ent_coef        | 0.177    |\n",
      "|    ent_coef_loss   | -11      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5809     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5920, episode_reward=-31.56 +/- 0.57\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -31.6    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5920     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.4    |\n",
      "|    critic_loss     | 0.00653  |\n",
      "|    ent_coef        | 0.176    |\n",
      "|    ent_coef_loss   | -11.1    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5819     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5930, episode_reward=-44.74 +/- 0.77\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -44.7    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5930     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.4    |\n",
      "|    critic_loss     | 0.00629  |\n",
      "|    ent_coef        | 0.176    |\n",
      "|    ent_coef_loss   | -11      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5829     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5940, episode_reward=-15.14 +/- 0.42\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -15.1    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5940     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.3    |\n",
      "|    critic_loss     | 0.00683  |\n",
      "|    ent_coef        | 0.175    |\n",
      "|    ent_coef_loss   | -10.9    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5839     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5950, episode_reward=-36.06 +/- 0.48\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -36.1    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5950     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.4    |\n",
      "|    critic_loss     | 0.00607  |\n",
      "|    ent_coef        | 0.175    |\n",
      "|    ent_coef_loss   | -11.1    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5849     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5960, episode_reward=-55.26 +/- 0.66\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -55.3    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5960     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.4    |\n",
      "|    critic_loss     | 0.00566  |\n",
      "|    ent_coef        | 0.174    |\n",
      "|    ent_coef_loss   | -10.8    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5859     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5970, episode_reward=-45.12 +/- 0.31\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -45.1    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5970     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.5    |\n",
      "|    critic_loss     | 0.00654  |\n",
      "|    ent_coef        | 0.174    |\n",
      "|    ent_coef_loss   | -11.2    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5869     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5980, episode_reward=-79.94 +/- 0.90\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -79.9    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5980     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.2    |\n",
      "|    critic_loss     | 0.00814  |\n",
      "|    ent_coef        | 0.173    |\n",
      "|    ent_coef_loss   | -11.1    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5879     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5990, episode_reward=-45.83 +/- 0.49\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -45.8    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 5990     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.3    |\n",
      "|    critic_loss     | 0.00669  |\n",
      "|    ent_coef        | 0.173    |\n",
      "|    ent_coef_loss   | -11.1    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5889     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=-52.91 +/- 0.72\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -52.9    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.5    |\n",
      "|    critic_loss     | 0.00495  |\n",
      "|    ent_coef        | 0.172    |\n",
      "|    ent_coef_loss   | -11.3    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5899     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6010, episode_reward=-46.04 +/- 0.61\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -46      |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6010     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.6    |\n",
      "|    critic_loss     | 0.00438  |\n",
      "|    ent_coef        | 0.172    |\n",
      "|    ent_coef_loss   | -11.3    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5909     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6020, episode_reward=-16.82 +/- 0.40\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -16.8    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6020     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.7    |\n",
      "|    critic_loss     | 0.00518  |\n",
      "|    ent_coef        | 0.171    |\n",
      "|    ent_coef_loss   | -11.1    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5919     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6030, episode_reward=-59.67 +/- 0.62\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -59.7    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6030     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.4    |\n",
      "|    critic_loss     | 0.00874  |\n",
      "|    ent_coef        | 0.171    |\n",
      "|    ent_coef_loss   | -11.2    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5929     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6040, episode_reward=-32.38 +/- 0.51\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -32.4    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6040     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.3    |\n",
      "|    critic_loss     | 0.00547  |\n",
      "|    ent_coef        | 0.17     |\n",
      "|    ent_coef_loss   | -11.2    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5939     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6050, episode_reward=-55.57 +/- 0.61\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -55.6    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6050     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.3    |\n",
      "|    critic_loss     | 0.00489  |\n",
      "|    ent_coef        | 0.17     |\n",
      "|    ent_coef_loss   | -11.2    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5949     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6060, episode_reward=-23.65 +/- 0.38\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -23.6    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6060     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.4    |\n",
      "|    critic_loss     | 0.00464  |\n",
      "|    ent_coef        | 0.169    |\n",
      "|    ent_coef_loss   | -11.2    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5959     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6070, episode_reward=-31.36 +/- 0.16\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -31.4    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6070     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.2    |\n",
      "|    critic_loss     | 0.00534  |\n",
      "|    ent_coef        | 0.169    |\n",
      "|    ent_coef_loss   | -11.1    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5969     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6080, episode_reward=-62.40 +/- 0.89\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -62.4    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6080     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.3    |\n",
      "|    critic_loss     | 0.0055   |\n",
      "|    ent_coef        | 0.168    |\n",
      "|    ent_coef_loss   | -11.1    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5979     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6090, episode_reward=-46.74 +/- 0.66\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -46.7    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6090     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.7    |\n",
      "|    critic_loss     | 0.0074   |\n",
      "|    ent_coef        | 0.168    |\n",
      "|    ent_coef_loss   | -11.4    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5989     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6100, episode_reward=-66.49 +/- 0.38\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -66.5    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6100     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.6    |\n",
      "|    critic_loss     | 0.00462  |\n",
      "|    ent_coef        | 0.167    |\n",
      "|    ent_coef_loss   | -11.4    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5999     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6110, episode_reward=-30.18 +/- 0.53\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -30.2    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6110     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.6    |\n",
      "|    critic_loss     | 0.00449  |\n",
      "|    ent_coef        | 0.167    |\n",
      "|    ent_coef_loss   | -11.3    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6009     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6120, episode_reward=-65.05 +/- 0.33\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -65.1    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6120     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.7    |\n",
      "|    critic_loss     | 0.00628  |\n",
      "|    ent_coef        | 0.166    |\n",
      "|    ent_coef_loss   | -11.2    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6019     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6130, episode_reward=-61.19 +/- 0.46\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -61.2    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6130     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.4    |\n",
      "|    critic_loss     | 0.00658  |\n",
      "|    ent_coef        | 0.166    |\n",
      "|    ent_coef_loss   | -11.4    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6029     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6140, episode_reward=-53.42 +/- 0.52\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -53.4    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6140     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.6    |\n",
      "|    critic_loss     | 0.00519  |\n",
      "|    ent_coef        | 0.165    |\n",
      "|    ent_coef_loss   | -11.4    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6039     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6150, episode_reward=-68.53 +/- 0.56\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -68.5    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6150     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.6    |\n",
      "|    critic_loss     | 0.00515  |\n",
      "|    ent_coef        | 0.165    |\n",
      "|    ent_coef_loss   | -11.4    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6049     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6160, episode_reward=-28.71 +/- 0.14\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -28.7    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6160     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.4    |\n",
      "|    critic_loss     | 0.00742  |\n",
      "|    ent_coef        | 0.164    |\n",
      "|    ent_coef_loss   | -11.5    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6059     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6170, episode_reward=-29.93 +/- 0.11\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -29.9    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6170     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.4    |\n",
      "|    critic_loss     | 0.00464  |\n",
      "|    ent_coef        | 0.164    |\n",
      "|    ent_coef_loss   | -11.5    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6069     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6180, episode_reward=-35.67 +/- 0.20\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -35.7    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6180     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.5    |\n",
      "|    critic_loss     | 0.00586  |\n",
      "|    ent_coef        | 0.163    |\n",
      "|    ent_coef_loss   | -11.3    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6079     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6190, episode_reward=-26.64 +/- 0.09\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -26.6    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6190     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.3    |\n",
      "|    critic_loss     | 0.00588  |\n",
      "|    ent_coef        | 0.163    |\n",
      "|    ent_coef_loss   | -11.4    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6089     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6200, episode_reward=-41.11 +/- 0.54\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -41.1    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6200     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.4    |\n",
      "|    critic_loss     | 0.0051   |\n",
      "|    ent_coef        | 0.162    |\n",
      "|    ent_coef_loss   | -11.3    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6099     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6210, episode_reward=-58.91 +/- 0.60\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -58.9    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6210     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.6    |\n",
      "|    critic_loss     | 0.00567  |\n",
      "|    ent_coef        | 0.162    |\n",
      "|    ent_coef_loss   | -11.6    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6109     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6220, episode_reward=-28.30 +/- 0.61\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -28.3    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6220     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.8    |\n",
      "|    critic_loss     | 0.00557  |\n",
      "|    ent_coef        | 0.161    |\n",
      "|    ent_coef_loss   | -11.5    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6119     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6230, episode_reward=-37.56 +/- 0.67\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -37.6    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6230     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.5    |\n",
      "|    critic_loss     | 0.00434  |\n",
      "|    ent_coef        | 0.161    |\n",
      "|    ent_coef_loss   | -11.6    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6129     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6240, episode_reward=-45.99 +/- 0.65\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -46      |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6240     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.6    |\n",
      "|    critic_loss     | 0.00635  |\n",
      "|    ent_coef        | 0.161    |\n",
      "|    ent_coef_loss   | -11.5    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6139     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6250, episode_reward=-35.64 +/- 0.12\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -35.6    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6250     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.7    |\n",
      "|    critic_loss     | 0.00534  |\n",
      "|    ent_coef        | 0.16     |\n",
      "|    ent_coef_loss   | -11.5    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6149     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6260, episode_reward=-19.03 +/- 0.20\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -19      |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6260     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.4    |\n",
      "|    critic_loss     | 0.00591  |\n",
      "|    ent_coef        | 0.16     |\n",
      "|    ent_coef_loss   | -11.3    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6159     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6270, episode_reward=-84.15 +/- 0.80\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -84.2    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6270     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.4    |\n",
      "|    critic_loss     | 0.00609  |\n",
      "|    ent_coef        | 0.159    |\n",
      "|    ent_coef_loss   | -11.6    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6169     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6280, episode_reward=-56.59 +/- 0.59\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -56.6    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6280     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.6    |\n",
      "|    critic_loss     | 0.00538  |\n",
      "|    ent_coef        | 0.159    |\n",
      "|    ent_coef_loss   | -11.5    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6179     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6290, episode_reward=-48.57 +/- 0.16\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -48.6    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6290     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.5    |\n",
      "|    critic_loss     | 0.00495  |\n",
      "|    ent_coef        | 0.158    |\n",
      "|    ent_coef_loss   | -11.7    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6189     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6300, episode_reward=-38.40 +/- 0.33\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -38.4    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6300     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.4    |\n",
      "|    critic_loss     | 0.0057   |\n",
      "|    ent_coef        | 0.158    |\n",
      "|    ent_coef_loss   | -11.5    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6199     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6310, episode_reward=-61.17 +/- 0.35\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -61.2    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6310     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.3    |\n",
      "|    critic_loss     | 0.00629  |\n",
      "|    ent_coef        | 0.157    |\n",
      "|    ent_coef_loss   | -11.6    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6209     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6320, episode_reward=-24.36 +/- 0.12\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -24.4    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6320     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.4    |\n",
      "|    critic_loss     | 0.00477  |\n",
      "|    ent_coef        | 0.157    |\n",
      "|    ent_coef_loss   | -11.6    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6219     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6330, episode_reward=-40.79 +/- 0.34\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -40.8    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6330     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.8    |\n",
      "|    critic_loss     | 0.00543  |\n",
      "|    ent_coef        | 0.156    |\n",
      "|    ent_coef_loss   | -11.9    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6229     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6340, episode_reward=-33.17 +/- 0.59\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -33.2    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6340     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.6    |\n",
      "|    critic_loss     | 0.00721  |\n",
      "|    ent_coef        | 0.156    |\n",
      "|    ent_coef_loss   | -11.7    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6239     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6350, episode_reward=-70.35 +/- 0.86\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -70.3    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6350     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.5    |\n",
      "|    critic_loss     | 0.00389  |\n",
      "|    ent_coef        | 0.155    |\n",
      "|    ent_coef_loss   | -11.7    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6249     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6360, episode_reward=-14.41 +/- 0.07\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -14.4    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6360     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.8    |\n",
      "|    critic_loss     | 0.00546  |\n",
      "|    ent_coef        | 0.155    |\n",
      "|    ent_coef_loss   | -12      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6259     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6370, episode_reward=-49.27 +/- 0.45\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -49.3    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6370     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.3    |\n",
      "|    critic_loss     | 0.0046   |\n",
      "|    ent_coef        | 0.155    |\n",
      "|    ent_coef_loss   | -11.6    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6269     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6380, episode_reward=-55.75 +/- 0.45\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -55.7    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6380     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.4    |\n",
      "|    critic_loss     | 0.00565  |\n",
      "|    ent_coef        | 0.154    |\n",
      "|    ent_coef_loss   | -11.7    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6279     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6390, episode_reward=-46.65 +/- 0.36\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -46.6    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6390     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.5    |\n",
      "|    critic_loss     | 0.00524  |\n",
      "|    ent_coef        | 0.154    |\n",
      "|    ent_coef_loss   | -11.6    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6289     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6400, episode_reward=-56.63 +/- 0.62\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -56.6    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6400     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.5    |\n",
      "|    critic_loss     | 0.00372  |\n",
      "|    ent_coef        | 0.153    |\n",
      "|    ent_coef_loss   | -11.8    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6299     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6410, episode_reward=-77.29 +/- 0.78\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -77.3    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6410     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.3    |\n",
      "|    critic_loss     | 0.0045   |\n",
      "|    ent_coef        | 0.153    |\n",
      "|    ent_coef_loss   | -11.6    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6309     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6420, episode_reward=-27.69 +/- 0.55\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -27.7    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6420     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.5    |\n",
      "|    critic_loss     | 0.00379  |\n",
      "|    ent_coef        | 0.152    |\n",
      "|    ent_coef_loss   | -11.8    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6319     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6430, episode_reward=-61.34 +/- 0.53\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -61.3    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6430     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.5    |\n",
      "|    critic_loss     | 0.00597  |\n",
      "|    ent_coef        | 0.152    |\n",
      "|    ent_coef_loss   | -11.9    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6329     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6440, episode_reward=-59.93 +/- 0.40\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -59.9    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6440     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.4    |\n",
      "|    critic_loss     | 0.0048   |\n",
      "|    ent_coef        | 0.151    |\n",
      "|    ent_coef_loss   | -11.7    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6339     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6450, episode_reward=-46.22 +/- 0.55\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -46.2    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6450     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.5    |\n",
      "|    critic_loss     | 0.00463  |\n",
      "|    ent_coef        | 0.151    |\n",
      "|    ent_coef_loss   | -11.6    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6349     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6460, episode_reward=-58.10 +/- 0.79\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -58.1    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6460     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.7    |\n",
      "|    critic_loss     | 0.00454  |\n",
      "|    ent_coef        | 0.151    |\n",
      "|    ent_coef_loss   | -11.9    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6359     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6470, episode_reward=-51.25 +/- 0.71\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -51.2    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6470     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.6    |\n",
      "|    critic_loss     | 0.00478  |\n",
      "|    ent_coef        | 0.15     |\n",
      "|    ent_coef_loss   | -12.2    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6369     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6480, episode_reward=-47.20 +/- 0.23\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -47.2    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6480     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.5    |\n",
      "|    critic_loss     | 0.00474  |\n",
      "|    ent_coef        | 0.15     |\n",
      "|    ent_coef_loss   | -12.1    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6379     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6490, episode_reward=-63.31 +/- 0.58\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -63.3    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6490     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.4    |\n",
      "|    critic_loss     | 0.00565  |\n",
      "|    ent_coef        | 0.149    |\n",
      "|    ent_coef_loss   | -12.1    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6389     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=-38.93 +/- 0.49\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -38.9    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6500     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.5    |\n",
      "|    critic_loss     | 0.00569  |\n",
      "|    ent_coef        | 0.149    |\n",
      "|    ent_coef_loss   | -11.8    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6399     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6510, episode_reward=-63.06 +/- 0.77\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -63.1    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6510     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.6    |\n",
      "|    critic_loss     | 0.00673  |\n",
      "|    ent_coef        | 0.148    |\n",
      "|    ent_coef_loss   | -11.9    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6409     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6520, episode_reward=-52.92 +/- 0.12\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -52.9    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6520     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.7    |\n",
      "|    critic_loss     | 0.00655  |\n",
      "|    ent_coef        | 0.148    |\n",
      "|    ent_coef_loss   | -12      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6419     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6530, episode_reward=-60.90 +/- 0.63\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -60.9    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6530     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.5    |\n",
      "|    critic_loss     | 0.00487  |\n",
      "|    ent_coef        | 0.147    |\n",
      "|    ent_coef_loss   | -12      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6429     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6540, episode_reward=-33.80 +/- 0.32\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -33.8    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6540     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.7    |\n",
      "|    critic_loss     | 0.00455  |\n",
      "|    ent_coef        | 0.147    |\n",
      "|    ent_coef_loss   | -12      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6439     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6550, episode_reward=-35.17 +/- 0.19\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -35.2    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6550     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.8    |\n",
      "|    critic_loss     | 0.00549  |\n",
      "|    ent_coef        | 0.147    |\n",
      "|    ent_coef_loss   | -12.3    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6449     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6560, episode_reward=-75.83 +/- 0.72\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -75.8    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6560     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.5    |\n",
      "|    critic_loss     | 0.00446  |\n",
      "|    ent_coef        | 0.146    |\n",
      "|    ent_coef_loss   | -11.9    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6459     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6570, episode_reward=-42.40 +/- 0.64\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -42.4    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6570     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.5    |\n",
      "|    critic_loss     | 0.00568  |\n",
      "|    ent_coef        | 0.146    |\n",
      "|    ent_coef_loss   | -12      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6469     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6580, episode_reward=-78.07 +/- 0.83\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -78.1    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6580     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.3    |\n",
      "|    critic_loss     | 0.00621  |\n",
      "|    ent_coef        | 0.145    |\n",
      "|    ent_coef_loss   | -12.2    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6479     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6590, episode_reward=-26.52 +/- 0.26\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -26.5    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6590     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.5    |\n",
      "|    critic_loss     | 0.00528  |\n",
      "|    ent_coef        | 0.145    |\n",
      "|    ent_coef_loss   | -12      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6489     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6600, episode_reward=-61.80 +/- 0.76\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -61.8    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6600     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.4    |\n",
      "|    critic_loss     | 0.00722  |\n",
      "|    ent_coef        | 0.144    |\n",
      "|    ent_coef_loss   | -12.2    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6499     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6610, episode_reward=-62.46 +/- 0.77\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -62.5    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6610     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.7    |\n",
      "|    critic_loss     | 0.00652  |\n",
      "|    ent_coef        | 0.144    |\n",
      "|    ent_coef_loss   | -12.3    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6509     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6620, episode_reward=-61.63 +/- 0.59\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -61.6    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6620     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.4    |\n",
      "|    critic_loss     | 0.00457  |\n",
      "|    ent_coef        | 0.144    |\n",
      "|    ent_coef_loss   | -11.8    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6519     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6630, episode_reward=-74.03 +/- 0.57\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -74      |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6630     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.6    |\n",
      "|    critic_loss     | 0.00409  |\n",
      "|    ent_coef        | 0.143    |\n",
      "|    ent_coef_loss   | -12.3    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6529     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6640, episode_reward=-62.48 +/- 0.47\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -62.5    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6640     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.5    |\n",
      "|    critic_loss     | 0.00463  |\n",
      "|    ent_coef        | 0.143    |\n",
      "|    ent_coef_loss   | -12.1    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6539     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6650, episode_reward=-73.30 +/- 0.84\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -73.3    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6650     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.5    |\n",
      "|    critic_loss     | 0.00492  |\n",
      "|    ent_coef        | 0.142    |\n",
      "|    ent_coef_loss   | -12.2    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6549     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6660, episode_reward=-53.76 +/- 0.45\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -53.8    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6660     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.5    |\n",
      "|    critic_loss     | 0.00492  |\n",
      "|    ent_coef        | 0.142    |\n",
      "|    ent_coef_loss   | -12.2    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6559     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6670, episode_reward=-59.24 +/- 0.45\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -59.2    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6670     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.4    |\n",
      "|    critic_loss     | 0.00444  |\n",
      "|    ent_coef        | 0.142    |\n",
      "|    ent_coef_loss   | -12.4    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6569     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6680, episode_reward=-59.80 +/- 0.52\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -59.8    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6680     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.7    |\n",
      "|    critic_loss     | 0.00489  |\n",
      "|    ent_coef        | 0.141    |\n",
      "|    ent_coef_loss   | -12.1    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6579     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6690, episode_reward=-79.02 +/- 0.79\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -79      |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6690     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.6    |\n",
      "|    critic_loss     | 0.00446  |\n",
      "|    ent_coef        | 0.141    |\n",
      "|    ent_coef_loss   | -12.4    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6589     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6700, episode_reward=-52.11 +/- 0.58\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -52.1    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6700     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.4    |\n",
      "|    critic_loss     | 0.00554  |\n",
      "|    ent_coef        | 0.14     |\n",
      "|    ent_coef_loss   | -12.3    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6599     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6710, episode_reward=-47.63 +/- 0.48\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -47.6    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6710     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.7    |\n",
      "|    critic_loss     | 0.00432  |\n",
      "|    ent_coef        | 0.14     |\n",
      "|    ent_coef_loss   | -12.4    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6609     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6720, episode_reward=-23.57 +/- 0.32\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -23.6    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6720     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.8    |\n",
      "|    critic_loss     | 0.0053   |\n",
      "|    ent_coef        | 0.139    |\n",
      "|    ent_coef_loss   | -12.6    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6619     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6730, episode_reward=-81.25 +/- 0.72\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -81.3    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6730     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.5    |\n",
      "|    critic_loss     | 0.00575  |\n",
      "|    ent_coef        | 0.139    |\n",
      "|    ent_coef_loss   | -12.4    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6629     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6740, episode_reward=-33.67 +/- 0.56\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -33.7    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6740     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.7    |\n",
      "|    critic_loss     | 0.00405  |\n",
      "|    ent_coef        | 0.139    |\n",
      "|    ent_coef_loss   | -12.3    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6639     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6750, episode_reward=-49.97 +/- 0.40\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -50      |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6750     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.5    |\n",
      "|    critic_loss     | 0.00513  |\n",
      "|    ent_coef        | 0.138    |\n",
      "|    ent_coef_loss   | -12.4    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6649     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6760, episode_reward=-39.87 +/- 0.16\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -39.9    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6760     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.6    |\n",
      "|    critic_loss     | 0.00466  |\n",
      "|    ent_coef        | 0.138    |\n",
      "|    ent_coef_loss   | -12.4    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6659     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6770, episode_reward=-50.47 +/- 0.48\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -50.5    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6770     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.5    |\n",
      "|    critic_loss     | 0.00397  |\n",
      "|    ent_coef        | 0.137    |\n",
      "|    ent_coef_loss   | -12.3    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6669     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6780, episode_reward=-58.24 +/- 0.40\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -58.2    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6780     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.4    |\n",
      "|    critic_loss     | 0.00502  |\n",
      "|    ent_coef        | 0.137    |\n",
      "|    ent_coef_loss   | -12.5    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6679     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6790, episode_reward=-15.64 +/- 0.43\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -15.6    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6790     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.6    |\n",
      "|    critic_loss     | 0.00387  |\n",
      "|    ent_coef        | 0.137    |\n",
      "|    ent_coef_loss   | -12.5    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6689     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6800, episode_reward=-58.49 +/- 0.42\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -58.5    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6800     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.3    |\n",
      "|    critic_loss     | 0.00563  |\n",
      "|    ent_coef        | 0.136    |\n",
      "|    ent_coef_loss   | -12.4    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6699     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6810, episode_reward=-41.17 +/- 0.36\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -41.2    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6810     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.8    |\n",
      "|    critic_loss     | 0.0048   |\n",
      "|    ent_coef        | 0.136    |\n",
      "|    ent_coef_loss   | -12.8    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6709     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6820, episode_reward=-52.74 +/- 0.21\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -52.7    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6820     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.7    |\n",
      "|    critic_loss     | 0.00416  |\n",
      "|    ent_coef        | 0.135    |\n",
      "|    ent_coef_loss   | -12.7    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6719     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6830, episode_reward=-61.68 +/- 0.32\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -61.7    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6830     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.3    |\n",
      "|    critic_loss     | 0.00455  |\n",
      "|    ent_coef        | 0.135    |\n",
      "|    ent_coef_loss   | -12.3    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6729     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6840, episode_reward=-56.97 +/- 0.72\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -57      |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6840     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.6    |\n",
      "|    critic_loss     | 0.00393  |\n",
      "|    ent_coef        | 0.135    |\n",
      "|    ent_coef_loss   | -12.5    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6739     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6850, episode_reward=-31.69 +/- 0.11\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -31.7    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6850     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.6    |\n",
      "|    critic_loss     | 0.00402  |\n",
      "|    ent_coef        | 0.134    |\n",
      "|    ent_coef_loss   | -12.3    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6749     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6860, episode_reward=-24.61 +/- 0.09\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -24.6    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6860     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.7    |\n",
      "|    critic_loss     | 0.00416  |\n",
      "|    ent_coef        | 0.134    |\n",
      "|    ent_coef_loss   | -12.4    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6759     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6870, episode_reward=-74.84 +/- 0.50\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -74.8    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6870     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.4    |\n",
      "|    critic_loss     | 0.00463  |\n",
      "|    ent_coef        | 0.133    |\n",
      "|    ent_coef_loss   | -12.5    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6769     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6880, episode_reward=-54.06 +/- 0.32\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -54.1    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6880     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.4    |\n",
      "|    critic_loss     | 0.00367  |\n",
      "|    ent_coef        | 0.133    |\n",
      "|    ent_coef_loss   | -12.6    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6779     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6890, episode_reward=-47.72 +/- 0.64\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -47.7    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6890     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.6    |\n",
      "|    critic_loss     | 0.00481  |\n",
      "|    ent_coef        | 0.133    |\n",
      "|    ent_coef_loss   | -12.6    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6789     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6900, episode_reward=-21.19 +/- 0.33\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -21.2    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6900     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.4    |\n",
      "|    critic_loss     | 0.00349  |\n",
      "|    ent_coef        | 0.132    |\n",
      "|    ent_coef_loss   | -12.5    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6799     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6910, episode_reward=-27.20 +/- 0.09\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -27.2    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6910     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.7    |\n",
      "|    critic_loss     | 0.00381  |\n",
      "|    ent_coef        | 0.132    |\n",
      "|    ent_coef_loss   | -12.5    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6809     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6920, episode_reward=-85.90 +/- 0.74\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -85.9    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6920     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.4    |\n",
      "|    critic_loss     | 0.00588  |\n",
      "|    ent_coef        | 0.132    |\n",
      "|    ent_coef_loss   | -12.3    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6819     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6930, episode_reward=-69.41 +/- 0.58\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -69.4    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6930     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.7    |\n",
      "|    critic_loss     | 0.00344  |\n",
      "|    ent_coef        | 0.131    |\n",
      "|    ent_coef_loss   | -12.7    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6829     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6940, episode_reward=-79.56 +/- 0.59\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -79.6    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6940     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.4    |\n",
      "|    critic_loss     | 0.00536  |\n",
      "|    ent_coef        | 0.131    |\n",
      "|    ent_coef_loss   | -12.4    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6839     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6950, episode_reward=-45.45 +/- 0.41\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -45.4    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6950     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.4    |\n",
      "|    critic_loss     | 0.00464  |\n",
      "|    ent_coef        | 0.13     |\n",
      "|    ent_coef_loss   | -12.7    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6849     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6960, episode_reward=-51.77 +/- 0.43\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -51.8    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6960     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.6    |\n",
      "|    critic_loss     | 0.00344  |\n",
      "|    ent_coef        | 0.13     |\n",
      "|    ent_coef_loss   | -12.8    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6859     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6970, episode_reward=-67.20 +/- 0.75\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -67.2    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6970     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.6    |\n",
      "|    critic_loss     | 0.00426  |\n",
      "|    ent_coef        | 0.13     |\n",
      "|    ent_coef_loss   | -12.6    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6869     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6980, episode_reward=-69.79 +/- 0.77\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -69.8    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6980     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.4    |\n",
      "|    critic_loss     | 0.00411  |\n",
      "|    ent_coef        | 0.129    |\n",
      "|    ent_coef_loss   | -12.6    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6879     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6990, episode_reward=-67.80 +/- 0.68\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -67.8    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 6990     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.5    |\n",
      "|    critic_loss     | 0.00532  |\n",
      "|    ent_coef        | 0.129    |\n",
      "|    ent_coef_loss   | -12.8    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6889     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=-11.54 +/- 0.25\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -11.5    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.6    |\n",
      "|    critic_loss     | 0.00599  |\n",
      "|    ent_coef        | 0.128    |\n",
      "|    ent_coef_loss   | -12.9    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6899     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7010, episode_reward=-33.77 +/- 0.10\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -33.8    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7010     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.6    |\n",
      "|    critic_loss     | 0.00342  |\n",
      "|    ent_coef        | 0.128    |\n",
      "|    ent_coef_loss   | -12.5    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6909     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7020, episode_reward=-53.38 +/- 0.38\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -53.4    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7020     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.3    |\n",
      "|    critic_loss     | 0.00386  |\n",
      "|    ent_coef        | 0.128    |\n",
      "|    ent_coef_loss   | -12.9    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6919     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7030, episode_reward=-70.49 +/- 0.68\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -70.5    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7030     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.6    |\n",
      "|    critic_loss     | 0.0045   |\n",
      "|    ent_coef        | 0.127    |\n",
      "|    ent_coef_loss   | -12.8    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6929     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7040, episode_reward=-46.70 +/- 0.63\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -46.7    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7040     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.8    |\n",
      "|    critic_loss     | 0.00442  |\n",
      "|    ent_coef        | 0.127    |\n",
      "|    ent_coef_loss   | -13      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6939     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7050, episode_reward=-65.20 +/- 0.52\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -65.2    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7050     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.5    |\n",
      "|    critic_loss     | 0.00386  |\n",
      "|    ent_coef        | 0.127    |\n",
      "|    ent_coef_loss   | -12.7    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6949     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7060, episode_reward=-59.80 +/- 0.73\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -59.8    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7060     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.7    |\n",
      "|    critic_loss     | 0.00343  |\n",
      "|    ent_coef        | 0.126    |\n",
      "|    ent_coef_loss   | -13      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6959     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7070, episode_reward=-37.40 +/- 0.15\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -37.4    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7070     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.4    |\n",
      "|    critic_loss     | 0.00427  |\n",
      "|    ent_coef        | 0.126    |\n",
      "|    ent_coef_loss   | -12.9    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6969     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7080, episode_reward=-67.92 +/- 0.53\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -67.9    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7080     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.6    |\n",
      "|    critic_loss     | 0.00728  |\n",
      "|    ent_coef        | 0.126    |\n",
      "|    ent_coef_loss   | -12.7    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6979     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7090, episode_reward=-38.93 +/- 0.26\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -38.9    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7090     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.5    |\n",
      "|    critic_loss     | 0.00661  |\n",
      "|    ent_coef        | 0.125    |\n",
      "|    ent_coef_loss   | -12.7    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6989     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7100, episode_reward=-25.53 +/- 0.31\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -25.5    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7100     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.3    |\n",
      "|    critic_loss     | 0.00467  |\n",
      "|    ent_coef        | 0.125    |\n",
      "|    ent_coef_loss   | -12.8    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6999     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7110, episode_reward=-78.50 +/- 0.68\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -78.5    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7110     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.4    |\n",
      "|    critic_loss     | 0.00531  |\n",
      "|    ent_coef        | 0.124    |\n",
      "|    ent_coef_loss   | -12.9    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7009     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7120, episode_reward=-40.56 +/- 0.49\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -40.6    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7120     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.5    |\n",
      "|    critic_loss     | 0.0037   |\n",
      "|    ent_coef        | 0.124    |\n",
      "|    ent_coef_loss   | -12.9    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7019     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7130, episode_reward=-48.41 +/- 0.67\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -48.4    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7130     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.4    |\n",
      "|    critic_loss     | 0.00453  |\n",
      "|    ent_coef        | 0.124    |\n",
      "|    ent_coef_loss   | -12.8    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7029     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7140, episode_reward=-41.90 +/- 0.20\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -41.9    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7140     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.3    |\n",
      "|    critic_loss     | 0.00347  |\n",
      "|    ent_coef        | 0.123    |\n",
      "|    ent_coef_loss   | -12.9    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7039     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7150, episode_reward=-58.75 +/- 0.45\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -58.7    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7150     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.4    |\n",
      "|    critic_loss     | 0.00381  |\n",
      "|    ent_coef        | 0.123    |\n",
      "|    ent_coef_loss   | -13      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7049     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7160, episode_reward=-55.02 +/- 0.63\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -55      |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7160     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.4    |\n",
      "|    critic_loss     | 0.0048   |\n",
      "|    ent_coef        | 0.123    |\n",
      "|    ent_coef_loss   | -13      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7059     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7170, episode_reward=-58.68 +/- 0.48\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -58.7    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7170     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.3    |\n",
      "|    critic_loss     | 0.00346  |\n",
      "|    ent_coef        | 0.122    |\n",
      "|    ent_coef_loss   | -12.8    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7069     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7180, episode_reward=-44.03 +/- 0.41\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -44      |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7180     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.4    |\n",
      "|    critic_loss     | 0.00589  |\n",
      "|    ent_coef        | 0.122    |\n",
      "|    ent_coef_loss   | -13.1    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7079     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7190, episode_reward=-61.65 +/- 0.49\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -61.7    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7190     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.6    |\n",
      "|    critic_loss     | 0.00394  |\n",
      "|    ent_coef        | 0.122    |\n",
      "|    ent_coef_loss   | -13      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7089     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7200, episode_reward=-52.16 +/- 0.25\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -52.2    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7200     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.3    |\n",
      "|    critic_loss     | 0.00366  |\n",
      "|    ent_coef        | 0.121    |\n",
      "|    ent_coef_loss   | -12.9    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7099     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7210, episode_reward=-53.39 +/- 0.33\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -53.4    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7210     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.4    |\n",
      "|    critic_loss     | 0.00281  |\n",
      "|    ent_coef        | 0.121    |\n",
      "|    ent_coef_loss   | -13      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7109     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7220, episode_reward=-48.15 +/- 0.25\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -48.2    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7220     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.3    |\n",
      "|    critic_loss     | 0.005    |\n",
      "|    ent_coef        | 0.12     |\n",
      "|    ent_coef_loss   | -13.2    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7119     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7230, episode_reward=-55.87 +/- 0.62\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -55.9    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7230     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.7    |\n",
      "|    critic_loss     | 0.00449  |\n",
      "|    ent_coef        | 0.12     |\n",
      "|    ent_coef_loss   | -13      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7129     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7240, episode_reward=-73.52 +/- 0.26\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -73.5    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7240     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.5    |\n",
      "|    critic_loss     | 0.00381  |\n",
      "|    ent_coef        | 0.12     |\n",
      "|    ent_coef_loss   | -12.8    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7139     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7250, episode_reward=-59.16 +/- 0.66\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -59.2    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7250     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.4    |\n",
      "|    critic_loss     | 0.00453  |\n",
      "|    ent_coef        | 0.119    |\n",
      "|    ent_coef_loss   | -13.1    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7149     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7260, episode_reward=-50.21 +/- 0.17\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -50.2    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7260     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.5    |\n",
      "|    critic_loss     | 0.00394  |\n",
      "|    ent_coef        | 0.119    |\n",
      "|    ent_coef_loss   | -13.1    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7159     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7270, episode_reward=-61.30 +/- 0.21\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -61.3    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7270     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.3    |\n",
      "|    critic_loss     | 0.00493  |\n",
      "|    ent_coef        | 0.119    |\n",
      "|    ent_coef_loss   | -13      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7169     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7280, episode_reward=-50.76 +/- 0.23\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -50.8    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7280     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.3    |\n",
      "|    critic_loss     | 0.00539  |\n",
      "|    ent_coef        | 0.118    |\n",
      "|    ent_coef_loss   | -13.1    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7179     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7290, episode_reward=-23.47 +/- 0.03\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -23.5    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7290     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.3    |\n",
      "|    critic_loss     | 0.00385  |\n",
      "|    ent_coef        | 0.118    |\n",
      "|    ent_coef_loss   | -13.2    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7189     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7300, episode_reward=-62.94 +/- 0.64\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -62.9    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7300     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.6    |\n",
      "|    critic_loss     | 0.00328  |\n",
      "|    ent_coef        | 0.118    |\n",
      "|    ent_coef_loss   | -13.1    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7199     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7310, episode_reward=-63.81 +/- 0.40\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -63.8    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7310     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.4    |\n",
      "|    critic_loss     | 0.00347  |\n",
      "|    ent_coef        | 0.117    |\n",
      "|    ent_coef_loss   | -12.9    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7209     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7320, episode_reward=-54.19 +/- 0.34\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -54.2    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7320     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.3    |\n",
      "|    critic_loss     | 0.00434  |\n",
      "|    ent_coef        | 0.117    |\n",
      "|    ent_coef_loss   | -13.1    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7219     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7330, episode_reward=-62.03 +/- 0.69\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -62      |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7330     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.4    |\n",
      "|    critic_loss     | 0.00418  |\n",
      "|    ent_coef        | 0.117    |\n",
      "|    ent_coef_loss   | -13.2    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7229     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7340, episode_reward=-73.49 +/- 0.64\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -73.5    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7340     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.3    |\n",
      "|    critic_loss     | 0.00614  |\n",
      "|    ent_coef        | 0.116    |\n",
      "|    ent_coef_loss   | -13      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7239     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7350, episode_reward=-47.46 +/- 0.49\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -47.5    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7350     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.3    |\n",
      "|    critic_loss     | 0.00588  |\n",
      "|    ent_coef        | 0.116    |\n",
      "|    ent_coef_loss   | -12.9    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7249     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7360, episode_reward=-41.87 +/- 0.48\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -41.9    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7360     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.3    |\n",
      "|    critic_loss     | 0.00559  |\n",
      "|    ent_coef        | 0.116    |\n",
      "|    ent_coef_loss   | -13.3    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7259     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7370, episode_reward=-42.15 +/- 0.46\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7370     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.3    |\n",
      "|    critic_loss     | 0.00403  |\n",
      "|    ent_coef        | 0.115    |\n",
      "|    ent_coef_loss   | -13.1    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7269     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7380, episode_reward=-29.22 +/- 0.48\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -29.2    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7380     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.4    |\n",
      "|    critic_loss     | 0.00581  |\n",
      "|    ent_coef        | 0.115    |\n",
      "|    ent_coef_loss   | -13.1    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7279     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7390, episode_reward=-50.19 +/- 0.49\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -50.2    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7390     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.6    |\n",
      "|    critic_loss     | 0.00387  |\n",
      "|    ent_coef        | 0.115    |\n",
      "|    ent_coef_loss   | -13.2    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7289     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7400, episode_reward=-54.84 +/- 0.56\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -54.8    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7400     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.5    |\n",
      "|    critic_loss     | 0.00355  |\n",
      "|    ent_coef        | 0.114    |\n",
      "|    ent_coef_loss   | -13.1    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7299     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7410, episode_reward=-61.73 +/- 0.46\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -61.7    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7410     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.5    |\n",
      "|    critic_loss     | 0.00355  |\n",
      "|    ent_coef        | 0.114    |\n",
      "|    ent_coef_loss   | -13.6    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7309     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7420, episode_reward=-78.97 +/- 0.59\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -79      |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7420     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.6    |\n",
      "|    critic_loss     | 0.00402  |\n",
      "|    ent_coef        | 0.114    |\n",
      "|    ent_coef_loss   | -13.5    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7319     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7430, episode_reward=-39.87 +/- 0.15\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -39.9    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7430     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.3    |\n",
      "|    critic_loss     | 0.00513  |\n",
      "|    ent_coef        | 0.113    |\n",
      "|    ent_coef_loss   | -13.2    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7329     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7440, episode_reward=-39.43 +/- 0.24\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -39.4    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7440     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.5    |\n",
      "|    critic_loss     | 0.00382  |\n",
      "|    ent_coef        | 0.113    |\n",
      "|    ent_coef_loss   | -13.5    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7339     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7450, episode_reward=-47.24 +/- 0.20\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -47.2    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7450     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.7    |\n",
      "|    critic_loss     | 0.00357  |\n",
      "|    ent_coef        | 0.113    |\n",
      "|    ent_coef_loss   | -13.6    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7349     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7460, episode_reward=-38.54 +/- 0.37\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -38.5    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7460     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.6    |\n",
      "|    critic_loss     | 0.00331  |\n",
      "|    ent_coef        | 0.112    |\n",
      "|    ent_coef_loss   | -13.6    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7359     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7470, episode_reward=-66.56 +/- 0.42\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -66.6    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7470     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.6    |\n",
      "|    critic_loss     | 0.0043   |\n",
      "|    ent_coef        | 0.112    |\n",
      "|    ent_coef_loss   | -13.6    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7369     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7480, episode_reward=-77.28 +/- 0.62\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -77.3    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7480     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.4    |\n",
      "|    critic_loss     | 0.00522  |\n",
      "|    ent_coef        | 0.112    |\n",
      "|    ent_coef_loss   | -13.4    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7379     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7490, episode_reward=-66.34 +/- 0.39\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -66.3    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7490     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.3    |\n",
      "|    critic_loss     | 0.00455  |\n",
      "|    ent_coef        | 0.111    |\n",
      "|    ent_coef_loss   | -13.6    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7389     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=-49.70 +/- 0.59\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -49.7    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7500     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.4    |\n",
      "|    critic_loss     | 0.00326  |\n",
      "|    ent_coef        | 0.111    |\n",
      "|    ent_coef_loss   | -13.5    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7399     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7510, episode_reward=-87.95 +/- 0.69\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -88      |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7510     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.2    |\n",
      "|    critic_loss     | 0.00353  |\n",
      "|    ent_coef        | 0.111    |\n",
      "|    ent_coef_loss   | -13.3    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7409     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7520, episode_reward=-68.02 +/- 0.69\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -68      |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7520     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.3    |\n",
      "|    critic_loss     | 0.00453  |\n",
      "|    ent_coef        | 0.11     |\n",
      "|    ent_coef_loss   | -13.5    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7419     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7530, episode_reward=-17.00 +/- 0.13\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -17      |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7530     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.4    |\n",
      "|    critic_loss     | 0.0037   |\n",
      "|    ent_coef        | 0.11     |\n",
      "|    ent_coef_loss   | -13.8    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7429     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7540, episode_reward=-42.87 +/- 0.49\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -42.9    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7540     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.2    |\n",
      "|    critic_loss     | 0.00437  |\n",
      "|    ent_coef        | 0.11     |\n",
      "|    ent_coef_loss   | -13.5    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7439     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7550, episode_reward=-61.67 +/- 0.62\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -61.7    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7550     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.3    |\n",
      "|    critic_loss     | 0.00366  |\n",
      "|    ent_coef        | 0.109    |\n",
      "|    ent_coef_loss   | -13.4    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7449     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7560, episode_reward=-60.56 +/- 0.72\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -60.6    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7560     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.3    |\n",
      "|    critic_loss     | 0.00413  |\n",
      "|    ent_coef        | 0.109    |\n",
      "|    ent_coef_loss   | -13.4    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7459     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7570, episode_reward=-62.17 +/- 0.67\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -62.2    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7570     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.5    |\n",
      "|    critic_loss     | 0.00312  |\n",
      "|    ent_coef        | 0.109    |\n",
      "|    ent_coef_loss   | -13.5    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7469     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7580, episode_reward=-65.14 +/- 0.70\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -65.1    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7580     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.4    |\n",
      "|    critic_loss     | 0.00308  |\n",
      "|    ent_coef        | 0.108    |\n",
      "|    ent_coef_loss   | -13.5    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7479     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7590, episode_reward=-63.37 +/- 0.63\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -63.4    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7590     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.1    |\n",
      "|    critic_loss     | 0.00378  |\n",
      "|    ent_coef        | 0.108    |\n",
      "|    ent_coef_loss   | -13.4    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7489     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7600, episode_reward=-53.18 +/- 0.62\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -53.2    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7600     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.4    |\n",
      "|    critic_loss     | 0.00364  |\n",
      "|    ent_coef        | 0.108    |\n",
      "|    ent_coef_loss   | -13.6    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7499     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7610, episode_reward=-18.84 +/- 0.27\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -18.8    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7610     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.2    |\n",
      "|    critic_loss     | 0.00503  |\n",
      "|    ent_coef        | 0.107    |\n",
      "|    ent_coef_loss   | -13.7    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7509     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7620, episode_reward=-58.40 +/- 0.68\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -58.4    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7620     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.4    |\n",
      "|    critic_loss     | 0.00389  |\n",
      "|    ent_coef        | 0.107    |\n",
      "|    ent_coef_loss   | -13.7    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7519     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7630, episode_reward=-53.69 +/- 0.20\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -53.7    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7630     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.4    |\n",
      "|    critic_loss     | 0.00422  |\n",
      "|    ent_coef        | 0.107    |\n",
      "|    ent_coef_loss   | -13.7    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7529     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7640, episode_reward=-29.13 +/- 0.28\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -29.1    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7640     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.4    |\n",
      "|    critic_loss     | 0.00612  |\n",
      "|    ent_coef        | 0.107    |\n",
      "|    ent_coef_loss   | -13.8    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7539     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7650, episode_reward=-44.47 +/- 0.40\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -44.5    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7650     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.2    |\n",
      "|    critic_loss     | 0.0037   |\n",
      "|    ent_coef        | 0.106    |\n",
      "|    ent_coef_loss   | -13.7    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7549     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7660, episode_reward=-24.01 +/- 0.34\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -24      |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7660     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24      |\n",
      "|    critic_loss     | 0.00396  |\n",
      "|    ent_coef        | 0.106    |\n",
      "|    ent_coef_loss   | -13.4    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7559     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7670, episode_reward=-46.43 +/- 0.36\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -46.4    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7670     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.4    |\n",
      "|    critic_loss     | 0.00353  |\n",
      "|    ent_coef        | 0.106    |\n",
      "|    ent_coef_loss   | -13.8    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7569     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7680, episode_reward=-39.49 +/- 0.44\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -39.5    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7680     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.2    |\n",
      "|    critic_loss     | 0.00363  |\n",
      "|    ent_coef        | 0.105    |\n",
      "|    ent_coef_loss   | -13.6    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7579     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7690, episode_reward=-56.00 +/- 0.54\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -56      |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7690     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.3    |\n",
      "|    critic_loss     | 0.00412  |\n",
      "|    ent_coef        | 0.105    |\n",
      "|    ent_coef_loss   | -13.6    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7589     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7700, episode_reward=-49.22 +/- 0.55\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -49.2    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7700     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.3    |\n",
      "|    critic_loss     | 0.00332  |\n",
      "|    ent_coef        | 0.105    |\n",
      "|    ent_coef_loss   | -13.7    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7599     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7710, episode_reward=-58.71 +/- 0.60\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -58.7    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7710     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.4    |\n",
      "|    critic_loss     | 0.00299  |\n",
      "|    ent_coef        | 0.104    |\n",
      "|    ent_coef_loss   | -14.1    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7609     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7720, episode_reward=-44.78 +/- 0.35\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -44.8    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7720     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.3    |\n",
      "|    critic_loss     | 0.0052   |\n",
      "|    ent_coef        | 0.104    |\n",
      "|    ent_coef_loss   | -13.4    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7619     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7730, episode_reward=-52.96 +/- 0.42\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -53      |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7730     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.3    |\n",
      "|    critic_loss     | 0.00367  |\n",
      "|    ent_coef        | 0.104    |\n",
      "|    ent_coef_loss   | -13.9    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7629     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7740, episode_reward=-31.25 +/- 0.28\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -31.3    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7740     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.4    |\n",
      "|    critic_loss     | 0.00323  |\n",
      "|    ent_coef        | 0.103    |\n",
      "|    ent_coef_loss   | -13.9    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7639     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7750, episode_reward=-47.54 +/- 0.33\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -47.5    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7750     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.7    |\n",
      "|    critic_loss     | 0.00379  |\n",
      "|    ent_coef        | 0.103    |\n",
      "|    ent_coef_loss   | -14.3    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7649     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7760, episode_reward=-58.33 +/- 0.68\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -58.3    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7760     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.4    |\n",
      "|    critic_loss     | 0.00392  |\n",
      "|    ent_coef        | 0.103    |\n",
      "|    ent_coef_loss   | -14      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7659     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7770, episode_reward=-73.03 +/- 0.66\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -73      |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7770     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.1    |\n",
      "|    critic_loss     | 0.0047   |\n",
      "|    ent_coef        | 0.103    |\n",
      "|    ent_coef_loss   | -13.8    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7669     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7780, episode_reward=-41.36 +/- 0.34\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -41.4    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7780     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.1    |\n",
      "|    critic_loss     | 0.00359  |\n",
      "|    ent_coef        | 0.102    |\n",
      "|    ent_coef_loss   | -13.8    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7679     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7790, episode_reward=-61.04 +/- 0.47\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -61      |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7790     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.4    |\n",
      "|    critic_loss     | 0.00471  |\n",
      "|    ent_coef        | 0.102    |\n",
      "|    ent_coef_loss   | -13.9    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7689     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7800, episode_reward=-37.41 +/- 0.20\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -37.4    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7800     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.1    |\n",
      "|    critic_loss     | 0.00336  |\n",
      "|    ent_coef        | 0.102    |\n",
      "|    ent_coef_loss   | -13.4    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7699     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7810, episode_reward=-59.00 +/- 0.40\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -59      |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7810     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.6    |\n",
      "|    critic_loss     | 0.00344  |\n",
      "|    ent_coef        | 0.101    |\n",
      "|    ent_coef_loss   | -14.3    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7709     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7820, episode_reward=-55.02 +/- 0.39\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -55      |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7820     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.3    |\n",
      "|    critic_loss     | 0.00574  |\n",
      "|    ent_coef        | 0.101    |\n",
      "|    ent_coef_loss   | -14.2    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7719     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7830, episode_reward=-63.57 +/- 0.66\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -63.6    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7830     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.3    |\n",
      "|    critic_loss     | 0.00493  |\n",
      "|    ent_coef        | 0.101    |\n",
      "|    ent_coef_loss   | -14.1    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7729     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7840, episode_reward=-22.52 +/- 0.45\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -22.5    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7840     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24      |\n",
      "|    critic_loss     | 0.00459  |\n",
      "|    ent_coef        | 0.1      |\n",
      "|    ent_coef_loss   | -13.9    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7739     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7850, episode_reward=-48.50 +/- 0.60\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -48.5    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7850     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.3    |\n",
      "|    critic_loss     | 0.00478  |\n",
      "|    ent_coef        | 0.1      |\n",
      "|    ent_coef_loss   | -14.1    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7749     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7860, episode_reward=-60.83 +/- 0.47\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -60.8    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7860     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.2    |\n",
      "|    critic_loss     | 0.00343  |\n",
      "|    ent_coef        | 0.0999   |\n",
      "|    ent_coef_loss   | -13.9    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7759     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7870, episode_reward=-11.12 +/- 0.14\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -11.1    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7870     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.2    |\n",
      "|    critic_loss     | 0.0028   |\n",
      "|    ent_coef        | 0.0996   |\n",
      "|    ent_coef_loss   | -14.1    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7769     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7880, episode_reward=-74.33 +/- 0.50\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -74.3    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7880     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.2    |\n",
      "|    critic_loss     | 0.00322  |\n",
      "|    ent_coef        | 0.0993   |\n",
      "|    ent_coef_loss   | -14.2    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7779     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7890, episode_reward=-44.14 +/- 0.48\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -44.1    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7890     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.3    |\n",
      "|    critic_loss     | 0.00339  |\n",
      "|    ent_coef        | 0.099    |\n",
      "|    ent_coef_loss   | -13.8    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7789     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7900, episode_reward=-61.31 +/- 0.52\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -61.3    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7900     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.2    |\n",
      "|    critic_loss     | 0.0039   |\n",
      "|    ent_coef        | 0.0987   |\n",
      "|    ent_coef_loss   | -14.4    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7799     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7910, episode_reward=-49.09 +/- 0.52\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -49.1    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7910     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.4    |\n",
      "|    critic_loss     | 0.00535  |\n",
      "|    ent_coef        | 0.0984   |\n",
      "|    ent_coef_loss   | -14.3    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7809     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7920, episode_reward=-48.16 +/- 0.37\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -48.2    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7920     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.1    |\n",
      "|    critic_loss     | 0.00287  |\n",
      "|    ent_coef        | 0.0981   |\n",
      "|    ent_coef_loss   | -14      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7819     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7930, episode_reward=-30.85 +/- 0.12\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -30.9    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7930     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.1    |\n",
      "|    critic_loss     | 0.00275  |\n",
      "|    ent_coef        | 0.0979   |\n",
      "|    ent_coef_loss   | -13.9    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7829     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7940, episode_reward=-68.58 +/- 0.47\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -68.6    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7940     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24      |\n",
      "|    critic_loss     | 0.00351  |\n",
      "|    ent_coef        | 0.0976   |\n",
      "|    ent_coef_loss   | -13.9    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7839     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7950, episode_reward=-49.15 +/- 0.12\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -49.1    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7950     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.1    |\n",
      "|    critic_loss     | 0.00522  |\n",
      "|    ent_coef        | 0.0973   |\n",
      "|    ent_coef_loss   | -14.3    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7849     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7960, episode_reward=-47.96 +/- 0.20\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -48      |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7960     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.2    |\n",
      "|    critic_loss     | 0.00323  |\n",
      "|    ent_coef        | 0.097    |\n",
      "|    ent_coef_loss   | -14.3    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7859     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7970, episode_reward=-59.32 +/- 0.59\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -59.3    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7970     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.2    |\n",
      "|    critic_loss     | 0.00289  |\n",
      "|    ent_coef        | 0.0967   |\n",
      "|    ent_coef_loss   | -14.3    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7869     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7980, episode_reward=-59.11 +/- 0.62\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -59.1    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7980     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.2    |\n",
      "|    critic_loss     | 0.004    |\n",
      "|    ent_coef        | 0.0964   |\n",
      "|    ent_coef_loss   | -14.3    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7879     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7990, episode_reward=-51.70 +/- 0.61\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -51.7    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 7990     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.1    |\n",
      "|    critic_loss     | 0.00404  |\n",
      "|    ent_coef        | 0.0961   |\n",
      "|    ent_coef_loss   | -14.3    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7889     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=-44.75 +/- 0.42\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -44.7    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 8000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.1    |\n",
      "|    critic_loss     | 0.00304  |\n",
      "|    ent_coef        | 0.0959   |\n",
      "|    ent_coef_loss   | -14.1    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7899     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8010, episode_reward=-61.47 +/- 0.44\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -61.5    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 8010     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.9    |\n",
      "|    critic_loss     | 0.00303  |\n",
      "|    ent_coef        | 0.0956   |\n",
      "|    ent_coef_loss   | -14.1    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7909     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8020, episode_reward=-35.46 +/- 0.49\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -35.5    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 8020     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.1    |\n",
      "|    critic_loss     | 0.00366  |\n",
      "|    ent_coef        | 0.0953   |\n",
      "|    ent_coef_loss   | -14.1    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7919     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8030, episode_reward=-18.46 +/- 0.09\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -18.5    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 8030     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.3    |\n",
      "|    critic_loss     | 0.00346  |\n",
      "|    ent_coef        | 0.095    |\n",
      "|    ent_coef_loss   | -14.4    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7929     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8040, episode_reward=-51.57 +/- 0.57\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -51.6    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 8040     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.1    |\n",
      "|    critic_loss     | 0.00299  |\n",
      "|    ent_coef        | 0.0948   |\n",
      "|    ent_coef_loss   | -14.1    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7939     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8050, episode_reward=-40.68 +/- 0.42\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -40.7    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 8050     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24      |\n",
      "|    critic_loss     | 0.00397  |\n",
      "|    ent_coef        | 0.0945   |\n",
      "|    ent_coef_loss   | -14.3    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7949     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8060, episode_reward=-52.98 +/- 0.43\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -53      |\n",
      "| time/              |          |\n",
      "|    total timesteps | 8060     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.1    |\n",
      "|    critic_loss     | 0.00374  |\n",
      "|    ent_coef        | 0.0942   |\n",
      "|    ent_coef_loss   | -14.1    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7959     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8070, episode_reward=-53.18 +/- 0.50\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -53.2    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 8070     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24      |\n",
      "|    critic_loss     | 0.00326  |\n",
      "|    ent_coef        | 0.0939   |\n",
      "|    ent_coef_loss   | -14.2    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7969     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8080, episode_reward=-48.83 +/- 0.45\n",
      "Episode length: 201.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | -48.8    |\n",
      "| time/              |          |\n",
      "|    total timesteps | 8080     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.2    |\n",
      "|    critic_loss     | 0.00312  |\n",
      "|    ent_coef        | 0.0936   |\n",
      "|    ent_coef_loss   | -14.4    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7979     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 58\u001b[0m\n\u001b[1;32m     52\u001b[0m model \u001b[39m=\u001b[39m SAC(MlpPolicy, env, verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, \n\u001b[1;32m     53\u001b[0m             device\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m,wandb_log\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     54\u001b[0m \u001b[39m#model = PPO(MlpPolicy, env, verbose=1,\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[39m#            device='cuda')\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \n\u001b[1;32m     57\u001b[0m \u001b[39m# train model\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m model\u001b[39m.\u001b[39;49mlearn(total_timesteps\u001b[39m=\u001b[39;49mmax_steps, \n\u001b[1;32m     59\u001b[0m             log_interval\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, \n\u001b[1;32m     60\u001b[0m             tb_log_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39msac_fetch_reach\u001b[39;49m\u001b[39m\"\u001b[39;49m, \n\u001b[1;32m     61\u001b[0m             reset_num_timesteps\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, \n\u001b[1;32m     62\u001b[0m             eval_freq\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, \n\u001b[1;32m     63\u001b[0m             n_eval_episodes\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m,\n\u001b[1;32m     64\u001b[0m             eval_log_path\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39msac_fetch_reach_eval\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     65\u001b[0m             eval_env\u001b[39m=\u001b[39;49menv_eval,\n\u001b[1;32m     66\u001b[0m             )\n\u001b[1;32m     68\u001b[0m model\u001b[39m.\u001b[39msave(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msac_fetch_reach_\u001b[39m\u001b[39m{\u001b[39;00mmax_steps\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/env/lib/python3.8/site-packages/stable_baselines3/sac/sac.py:292\u001b[0m, in \u001b[0;36mSAC.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[1;32m    280\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    281\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    289\u001b[0m     reset_num_timesteps: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    290\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m OffPolicyAlgorithm:\n\u001b[0;32m--> 292\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m(SAC, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mlearn(\n\u001b[1;32m    293\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[1;32m    294\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[1;32m    295\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[1;32m    296\u001b[0m         eval_env\u001b[39m=\u001b[39;49meval_env,\n\u001b[1;32m    297\u001b[0m         eval_freq\u001b[39m=\u001b[39;49meval_freq,\n\u001b[1;32m    298\u001b[0m         n_eval_episodes\u001b[39m=\u001b[39;49mn_eval_episodes,\n\u001b[1;32m    299\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49mtb_log_name,\n\u001b[1;32m    300\u001b[0m         eval_log_path\u001b[39m=\u001b[39;49meval_log_path,\n\u001b[1;32m    301\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49mreset_num_timesteps,\n\u001b[1;32m    302\u001b[0m     )\n",
      "File \u001b[0;32m/env/lib/python3.8/site-packages/stable_baselines3/common/off_policy_algorithm.py:352\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    349\u001b[0m callback\u001b[39m.\u001b[39mon_training_start(\u001b[39mlocals\u001b[39m(), \u001b[39mglobals\u001b[39m())\n\u001b[1;32m    351\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 352\u001b[0m     rollout \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollect_rollouts(\n\u001b[1;32m    353\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv,\n\u001b[1;32m    354\u001b[0m         train_freq\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_freq,\n\u001b[1;32m    355\u001b[0m         action_noise\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maction_noise,\n\u001b[1;32m    356\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[1;32m    357\u001b[0m         learning_starts\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlearning_starts,\n\u001b[1;32m    358\u001b[0m         replay_buffer\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreplay_buffer,\n\u001b[1;32m    359\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[1;32m    360\u001b[0m     )\n\u001b[1;32m    362\u001b[0m     \u001b[39mif\u001b[39;00m rollout\u001b[39m.\u001b[39mcontinue_training \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m    363\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/env/lib/python3.8/site-packages/stable_baselines3/common/off_policy_algorithm.py:584\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, train_freq, replay_buffer, action_noise, learning_starts, log_interval)\u001b[0m\n\u001b[1;32m    582\u001b[0m callback\u001b[39m.\u001b[39mupdate_locals(\u001b[39mlocals\u001b[39m())\n\u001b[1;32m    583\u001b[0m \u001b[39m# Only stop training if return value is False, not when it is None.\u001b[39;00m\n\u001b[0;32m--> 584\u001b[0m \u001b[39mif\u001b[39;00m callback\u001b[39m.\u001b[39;49mon_step() \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m    585\u001b[0m     \u001b[39mreturn\u001b[39;00m RolloutReturn(\u001b[39m0.0\u001b[39m, num_collected_steps, num_collected_episodes, continue_training\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    587\u001b[0m episode_reward \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m reward\n",
      "File \u001b[0;32m/env/lib/python3.8/site-packages/stable_baselines3/common/callbacks.py:88\u001b[0m, in \u001b[0;36mBaseCallback.on_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[39m# timesteps start at zero\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mnum_timesteps\n\u001b[0;32m---> 88\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_on_step()\n",
      "File \u001b[0;32m/env/lib/python3.8/site-packages/stable_baselines3/common/callbacks.py:192\u001b[0m, in \u001b[0;36mCallbackList._on_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    189\u001b[0m continue_training \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[39mfor\u001b[39;00m callback \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks:\n\u001b[1;32m    191\u001b[0m     \u001b[39m# Return False (stop training) if at least one callback returns False\u001b[39;00m\n\u001b[0;32m--> 192\u001b[0m     continue_training \u001b[39m=\u001b[39m callback\u001b[39m.\u001b[39;49mon_step() \u001b[39mand\u001b[39;00m continue_training\n\u001b[1;32m    193\u001b[0m \u001b[39mreturn\u001b[39;00m continue_training\n",
      "File \u001b[0;32m/env/lib/python3.8/site-packages/stable_baselines3/common/callbacks.py:88\u001b[0m, in \u001b[0;36mBaseCallback.on_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[39m# timesteps start at zero\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mnum_timesteps\n\u001b[0;32m---> 88\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_on_step()\n",
      "File \u001b[0;32m/env/lib/python3.8/site-packages/stable_baselines3/common/callbacks.py:369\u001b[0m, in \u001b[0;36mEvalCallback._on_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[39m# Reset success rate buffer\u001b[39;00m\n\u001b[1;32m    367\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_success_buffer \u001b[39m=\u001b[39m []\n\u001b[0;32m--> 369\u001b[0m episode_rewards, episode_lengths \u001b[39m=\u001b[39m evaluate_policy(\n\u001b[1;32m    370\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel,\n\u001b[1;32m    371\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meval_env,\n\u001b[1;32m    372\u001b[0m     n_eval_episodes\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_eval_episodes,\n\u001b[1;32m    373\u001b[0m     render\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrender,\n\u001b[1;32m    374\u001b[0m     deterministic\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdeterministic,\n\u001b[1;32m    375\u001b[0m     return_episode_rewards\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    376\u001b[0m     warn\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwarn,\n\u001b[1;32m    377\u001b[0m     callback\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_log_success_callback,\n\u001b[1;32m    378\u001b[0m )\n\u001b[1;32m    380\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog_path \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    381\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluations_timesteps\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps)\n",
      "File \u001b[0;32m/env/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:87\u001b[0m, in \u001b[0;36mevaluate_policy\u001b[0;34m(model, env, n_eval_episodes, deterministic, render, callback, reward_threshold, return_episode_rewards, warn)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[39mwhile\u001b[39;00m (episode_counts \u001b[39m<\u001b[39m episode_count_targets)\u001b[39m.\u001b[39many():\n\u001b[1;32m     86\u001b[0m     step_counts \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m    \n\u001b[0;32m---> 87\u001b[0m     actions, states \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict(observations, state\u001b[39m=\u001b[39;49mstates, deterministic\u001b[39m=\u001b[39;49mdeterministic)\n\u001b[1;32m     88\u001b[0m     observations, rewards, dones, infos \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(actions)\n\u001b[1;32m     89\u001b[0m     current_rewards \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m rewards\n",
      "File \u001b[0;32m/env/lib/python3.8/site-packages/stable_baselines3/common/base_class.py:540\u001b[0m, in \u001b[0;36mBaseAlgorithm.predict\u001b[0;34m(self, observation, state, mask, deterministic)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\n\u001b[1;32m    524\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    525\u001b[0m     observation: np\u001b[39m.\u001b[39mndarray,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    528\u001b[0m     deterministic: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    529\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[np\u001b[39m.\u001b[39mndarray, Optional[np\u001b[39m.\u001b[39mndarray]]:\n\u001b[1;32m    530\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    531\u001b[0m \u001b[39m    Get the model's action(s) from an observation\u001b[39;00m\n\u001b[1;32m    532\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[39m        (used in recurrent policies)\u001b[39;00m\n\u001b[1;32m    539\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 540\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolicy\u001b[39m.\u001b[39;49mpredict(observation, state, mask, deterministic)\n",
      "File \u001b[0;32m/env/lib/python3.8/site-packages/stable_baselines3/common/policies.py:299\u001b[0m, in \u001b[0;36mBasePolicy.predict\u001b[0;34m(self, observation, state, mask, deterministic)\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[39m# Add batch dimension if needed\u001b[39;00m\n\u001b[1;32m    297\u001b[0m     observation \u001b[39m=\u001b[39m observation\u001b[39m.\u001b[39mreshape((\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation_space\u001b[39m.\u001b[39mshape)\n\u001b[0;32m--> 299\u001b[0m observation \u001b[39m=\u001b[39m obs_as_tensor(observation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice)\n\u001b[1;32m    301\u001b[0m \u001b[39mwith\u001b[39;00m th\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m    302\u001b[0m     actions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_predict(observation, deterministic\u001b[39m=\u001b[39mdeterministic)\n",
      "File \u001b[0;32m/env/lib/python3.8/site-packages/stable_baselines3/common/utils.py:433\u001b[0m, in \u001b[0;36mobs_as_tensor\u001b[0;34m(obs, device)\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[39mreturn\u001b[39;00m th\u001b[39m.\u001b[39mas_tensor(obs)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m    432\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(obs, \u001b[39mdict\u001b[39m):\n\u001b[0;32m--> 433\u001b[0m     \u001b[39mreturn\u001b[39;00m {key: th\u001b[39m.\u001b[39mas_tensor(_obs)\u001b[39m.\u001b[39mto(device) \u001b[39mfor\u001b[39;00m (key, _obs) \u001b[39min\u001b[39;00m obs\u001b[39m.\u001b[39mitems()}\n\u001b[1;32m    434\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    435\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized type of observation \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(obs)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/env/lib/python3.8/site-packages/stable_baselines3/common/utils.py:433\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[39mreturn\u001b[39;00m th\u001b[39m.\u001b[39mas_tensor(obs)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m    432\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(obs, \u001b[39mdict\u001b[39m):\n\u001b[0;32m--> 433\u001b[0m     \u001b[39mreturn\u001b[39;00m {key: th\u001b[39m.\u001b[39;49mas_tensor(_obs)\u001b[39m.\u001b[39;49mto(device) \u001b[39mfor\u001b[39;00m (key, _obs) \u001b[39min\u001b[39;00m obs\u001b[39m.\u001b[39mitems()}\n\u001b[1;32m    434\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    435\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized type of observation \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(obs)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train with SAC, stable baseline3\n",
    "import stable_baselines3\n",
    "from stable_baselines3 import SAC, PPO\n",
    "from stable_baselines3.sac import MlpPolicy\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from wandb.integration.sb3 import WandbCallback\n",
    "import wandb\n",
    "# pip install gym-robotics\n",
    "import matplotlib.pyplot as plt\n",
    "from gym_robotics.envs.fetch.reach import MujocoPyFetchReachEnv\n",
    "from gym.wrappers import TimeLimit\n",
    "\n",
    "\n",
    "# init mujoco fetch environment\n",
    "env = MujocoPyFetchReachEnv()\n",
    "\n",
    "log_dir = \"./sac_fetch_reach_tensorboard/\"\n",
    "\n",
    "\n",
    "max_steps = 30_000\n",
    "\n",
    "config = {\n",
    "    \"policy_type\": \"PPO\",\n",
    "    \"total_timesteps\": max_steps,\n",
    "    \"env_name\": \"FetchReach\",\n",
    "}\n",
    "run = wandb.init(\n",
    "    project=\"sb3\",\n",
    "    config=config,\n",
    "    sync_tensorboard=True,  # auto-upload sb3's tensorboard metrics\n",
    "    monitor_gym=True,  # auto-upload the videos of agents playing the game\n",
    "    save_code=True,  # optional\n",
    ")\n",
    "\n",
    "\n",
    "# init mujoco fetch environment\n",
    "# init mujoco fetch environment\n",
    "env = MujocoPyFetchReachEnv(reward_type='dense')\n",
    "env = Monitor(env, log_dir)\n",
    "#env = TimeLimit(env, max_episode_steps=100)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "\n",
    "env_eval = MujocoPyFetchReachEnv(reward_type='dense')\n",
    "env_eval = Monitor(env_eval, log_dir)\n",
    "#env_eval = TimeLimit(env_eval, max_episode_steps=100)\n",
    "env_eval = DummyVecEnv([lambda: env_eval])\n",
    "\n",
    "env.render_mode = 'rgb_array'\n",
    "# wrap environment\n",
    "# init model\n",
    "model = SAC(MlpPolicy, env, verbose=1, \n",
    "            device='cuda',wandb_log=True)\n",
    "#model = PPO(MlpPolicy, env, verbose=1,\n",
    "#            device='cuda')\n",
    "\n",
    "# train model\n",
    "model.learn(total_timesteps=max_steps, \n",
    "            log_interval=10, \n",
    "            tb_log_name=\"sac_fetch_reach\", \n",
    "            reset_num_timesteps=False, \n",
    "            eval_freq=100, \n",
    "            n_eval_episodes=10,\n",
    "            eval_log_path=\"sac_fetch_reach_eval\",\n",
    "            eval_env=env_eval,\n",
    "            )\n",
    "\n",
    "model.save(f\"sac_fetch_reach_{max_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# load results\n",
    "results = load_results(\"./sac_fetch_reach_tensorboard\")\n",
    "\n",
    "# plot results\n",
    "plt.plot(results)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "#model.save(\"sac_fetch_reach\")\n",
    "# load model\n",
    "model = SAC.load(\"sac_fetch_reach_100000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1000 done\n"
     ]
    }
   ],
   "source": [
    "# test model\n",
    "obs = env.reset()\n",
    "frames = []\n",
    "for i in range(1,1001):\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    #print(dones)\n",
    "    frame = env.render()\n",
    "    frames.append(frame)\n",
    "    if dones[0]:\n",
    "        obs = env.reset()\n",
    "        print(dones)\n",
    "    if i % 100 == 0:\n",
    "        obs = env.reset()\n",
    "        \n",
    "rgb_to_video.set_frames(frames)\n",
    "rgb_to_video.save(path=f'{i}.gif',mode='gif')\n",
    "print(f'episode {i} done')\n",
    "frames = []\n",
    "rgb_to_video.container.clear()\n",
    "\n",
    "# save model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
