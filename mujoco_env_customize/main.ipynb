{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apt install xvfb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xvfb-run: error: Xvfb failed to start\n"
     ]
    }
   ],
   "source": [
    "!export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libGLEW.so:/usr/lib/x86_64-linux-gnu/libGL.so\n",
    "!xvfb-run -s \"-screen 0 1400x900x24\" /bin/bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['LD_PRELOAD'] = \"/usr/lib/x86_64-linux-gnu/libGLEW.so:/usr/lib/x86_64-linux-gnu/libGL.so\"\n",
    "os.environ['DISPLAY'] = ':0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":0\n"
     ]
    }
   ],
   "source": [
    "!echo $DISPLAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/research/agent-playground/mujoco_env_customize/gym_robotics/envs/fetch/reach.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/research/agent-playground/mujoco_env_customize/gym_robotics/envs/robot_env.py:309: UserWarning: \u001b[33mWARN: This version of the mujoco environments depends on the mujoco-py bindings, which are no longer maintained and may stop working. Please upgrade to the v4 versions of the environments (which depend on the mujoco python bindings instead), unless you are trying to precisely replicate previous works).\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reach environment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reach environment\n",
    "Dict(\n",
    "    \n",
    "    'achieved_goal': Box(-inf, inf, (3,), float64), \n",
    "    \n",
    "    'desired_goal': Box(-inf, inf, (3,), float64), \n",
    "    \n",
    "    'observation': Box(-inf, inf, (10,), float64)) : \n",
    "    [ grip_pos, object_pos.ravel(), object_rel_pos.ravel(), gripper_state, object_rot.ravel(), object_velp.ravel(), object_velr.ravel(), grip_velp, gripper_vel,]\n",
    "    [3, 0, 0, 2, 0, 0, 0, 3, 2]\n",
    "\n",
    "    so, [grip_pos 3, gripper_state 2, grip_velp 3, gripper_vel 2]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jesnk_utils.rgb_to_video import RGB2VIDEO\n",
    "# reset environment\n",
    "env.reset()\n",
    "env.render_mode = 'rgb_array'\n",
    "img = env.render()\n",
    "\n",
    "rgb_to_video = RGB2VIDEO()\n",
    "\n",
    "# get frames for video, random actions, 30 frames\n",
    "frames = []\n",
    "for i in range(30):\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, terminated, info = env.step(action)\n",
    "    # obs, reward, terminated, truncated, info\n",
    "    # obs : ['observation', 'achieved_goal', 'desired_goal']\n",
    "    img = env.render()\n",
    "    frames.append(img)\n",
    "    \n",
    "# save video\n",
    "rgb_to_video.set_frames(frames)\n",
    "rgb_to_video.save(mode='gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/research/agent-playground/mujoco_env_customize/gym_robotics/envs/robot_env.py:313: UserWarning: \u001b[33mWARN: This version of the mujoco environments depends on the mujoco-py bindings, which are no longer maintained and may stop working. Please upgrade to the v4 versions of the environments (which depend on the mujoco python bindings instead), unless you are trying to precisely replicate previous works).\u001b[0m\n",
      "  logger.warn(\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/research/agent-playground/mujoco_env_customize/gym_robotics/envs/fetch/reach.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjesnk\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/research/agent-playground/mujoco_env_customize/wandb/run-20230620_004522-lm4zyw1k</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jesnk/sb3/runs/lm4zyw1k' target=\"_blank\">swift-bush-88</a></strong> to <a href='https://wandb.ai/jesnk/sb3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jesnk/sb3' target=\"_blank\">https://wandb.ai/jesnk/sb3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jesnk/sb3/runs/lm4zyw1k' target=\"_blank\">https://wandb.ai/jesnk/sb3/runs/lm4zyw1k</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/research/agent-playground/mujoco_env_customize/gym_robotics/envs/robot_env.py:313: UserWarning: \u001b[33mWARN: This version of the mujoco environments depends on the mujoco-py bindings, which are no longer maintained and may stop working. Please upgrade to the v4 versions of the environments (which depend on the mujoco python bindings instead), unless you are trying to precisely replicate previous works).\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/env/lib/python3.8/site-packages/numpy/linalg/linalg.py:2517: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  (ord in ('f', 'fro') and ndim == 2) or\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 58\u001b[0m\n\u001b[1;32m     52\u001b[0m model \u001b[39m=\u001b[39m SAC(MlpPolicy, env, verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, \n\u001b[1;32m     53\u001b[0m             device\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m,wandb_log\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     54\u001b[0m \u001b[39m#model = PPO(MlpPolicy, env, verbose=1,\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[39m#            device='cuda')\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \n\u001b[1;32m     57\u001b[0m \u001b[39m# train model\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m model\u001b[39m.\u001b[39;49mlearn(total_timesteps\u001b[39m=\u001b[39;49mmax_steps, \n\u001b[1;32m     59\u001b[0m             log_interval\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, \n\u001b[1;32m     60\u001b[0m             tb_log_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39msac_fetch_reach\u001b[39;49m\u001b[39m\"\u001b[39;49m, \n\u001b[1;32m     61\u001b[0m             reset_num_timesteps\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, \n\u001b[1;32m     62\u001b[0m             eval_freq\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, \n\u001b[1;32m     63\u001b[0m             n_eval_episodes\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m,\n\u001b[1;32m     64\u001b[0m             eval_log_path\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39msac_fetch_reach_eval\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     65\u001b[0m             eval_env\u001b[39m=\u001b[39;49menv_eval,\n\u001b[1;32m     66\u001b[0m             )\n\u001b[1;32m     68\u001b[0m model\u001b[39m.\u001b[39msave(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msac_fetch_reach_\u001b[39m\u001b[39m{\u001b[39;00mmax_steps\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/env/lib/python3.8/site-packages/stable_baselines3/sac/sac.py:292\u001b[0m, in \u001b[0;36mSAC.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[1;32m    280\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    281\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    289\u001b[0m     reset_num_timesteps: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    290\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m OffPolicyAlgorithm:\n\u001b[0;32m--> 292\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m(SAC, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mlearn(\n\u001b[1;32m    293\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[1;32m    294\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[1;32m    295\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[1;32m    296\u001b[0m         eval_env\u001b[39m=\u001b[39;49meval_env,\n\u001b[1;32m    297\u001b[0m         eval_freq\u001b[39m=\u001b[39;49meval_freq,\n\u001b[1;32m    298\u001b[0m         n_eval_episodes\u001b[39m=\u001b[39;49mn_eval_episodes,\n\u001b[1;32m    299\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49mtb_log_name,\n\u001b[1;32m    300\u001b[0m         eval_log_path\u001b[39m=\u001b[39;49meval_log_path,\n\u001b[1;32m    301\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49mreset_num_timesteps,\n\u001b[1;32m    302\u001b[0m     )\n",
      "File \u001b[0;32m/env/lib/python3.8/site-packages/stable_baselines3/common/off_policy_algorithm.py:352\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    349\u001b[0m callback\u001b[39m.\u001b[39mon_training_start(\u001b[39mlocals\u001b[39m(), \u001b[39mglobals\u001b[39m())\n\u001b[1;32m    351\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 352\u001b[0m     rollout \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollect_rollouts(\n\u001b[1;32m    353\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv,\n\u001b[1;32m    354\u001b[0m         train_freq\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_freq,\n\u001b[1;32m    355\u001b[0m         action_noise\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maction_noise,\n\u001b[1;32m    356\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[1;32m    357\u001b[0m         learning_starts\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlearning_starts,\n\u001b[1;32m    358\u001b[0m         replay_buffer\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreplay_buffer,\n\u001b[1;32m    359\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[1;32m    360\u001b[0m     )\n\u001b[1;32m    362\u001b[0m     \u001b[39mif\u001b[39;00m rollout\u001b[39m.\u001b[39mcontinue_training \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m    363\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/env/lib/python3.8/site-packages/stable_baselines3/common/off_policy_algorithm.py:566\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, train_freq, replay_buffer, action_noise, learning_starts, log_interval)\u001b[0m\n\u001b[1;32m    563\u001b[0m action, buffer_action \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sample_action(learning_starts, action_noise)\n\u001b[1;32m    565\u001b[0m \u001b[39m# Rescale and perform action\u001b[39;00m\n\u001b[0;32m--> 566\u001b[0m new_obs, reward, done, infos \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m    568\u001b[0m \u001b[39m#jesnk: if observation is a dict, convert to array\u001b[39;00m\n\u001b[1;32m    569\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(new_obs, Dict) \u001b[39mand\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m    570\u001b[0m     \u001b[39m#convert to array\u001b[39;00m\n",
      "File \u001b[0;32m/env/lib/python3.8/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:162\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \u001b[39mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \n\u001b[1;32m    158\u001b[0m \u001b[39m:param actions: the action\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[39m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 162\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep_wait()\n",
      "File \u001b[0;32m/env/lib/python3.8/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:44\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep_wait\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m     43\u001b[0m     \u001b[39mfor\u001b[39;00m env_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_envs):\n\u001b[0;32m---> 44\u001b[0m         obs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_rews[env_idx], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_dones[env_idx], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_truncated[env_idx], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_infos[env_idx] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menvs[env_idx]\u001b[39m.\u001b[39;49mstep(\n\u001b[1;32m     45\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactions[env_idx]\n\u001b[1;32m     46\u001b[0m         )\n\u001b[1;32m     47\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_dones[env_idx]:\n\u001b[1;32m     48\u001b[0m             \u001b[39m# save final observation where user can get it, then reset\u001b[39;00m\n\u001b[1;32m     49\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_infos[env_idx][\u001b[39m\"\u001b[39m\u001b[39mterminal_observation\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m obs\n",
      "File \u001b[0;32m/env/lib/python3.8/site-packages/stable_baselines3/common/monitor.py:90\u001b[0m, in \u001b[0;36mMonitor.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mneeds_reset:\n\u001b[1;32m     89\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTried to step environment that needs reset\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 90\u001b[0m observation, reward, done, truncated, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     91\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrewards\u001b[39m.\u001b[39mappend(reward)\n\u001b[1;32m     92\u001b[0m \u001b[39mif\u001b[39;00m done:\n",
      "File \u001b[0;32m/research/agent-playground/mujoco_env_customize/gym_robotics/envs/robot_env.py:129\u001b[0m, in \u001b[0;36mBaseRobotEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    122\u001b[0m obs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_obs()\n\u001b[1;32m    124\u001b[0m info \u001b[39m=\u001b[39m {\n\u001b[1;32m    125\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mis_success\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_success(obs[\u001b[39m\"\u001b[39m\u001b[39machieved_goal\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgoal),\n\u001b[1;32m    126\u001b[0m \n\u001b[1;32m    127\u001b[0m }\n\u001b[0;32m--> 129\u001b[0m terminated \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_terminated(obs[\u001b[39m\"\u001b[39;49m\u001b[39machieved_goal\u001b[39;49m\u001b[39m\"\u001b[39;49m], \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgoal, info)\n\u001b[1;32m    130\u001b[0m truncated \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_truncated(obs[\u001b[39m\"\u001b[39m\u001b[39machieved_goal\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgoal, info)\n\u001b[1;32m    131\u001b[0m \u001b[39mif\u001b[39;00m terminated :\n",
      "File \u001b[0;32m/research/agent-playground/mujoco_env_customize/gym_robotics/envs/fetch/reach.py:42\u001b[0m, in \u001b[0;36mMujocoPyFetchReachEnv.compute_terminated\u001b[0;34m(self, achieved_goal, desired_goal, info)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_terminated\u001b[39m(\u001b[39mself\u001b[39m, achieved_goal, desired_goal, info):\n\u001b[1;32m     41\u001b[0m     \u001b[39m# Get distance\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39;49mlinalg\u001b[39m.\u001b[39;49mnorm(achieved_goal,desired_goal) \u001b[39m<\u001b[39m \u001b[39m0.05\u001b[39m\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mnorm\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/env/lib/python3.8/site-packages/numpy/linalg/linalg.py:2516\u001b[0m, in \u001b[0;36mnorm\u001b[0;34m(x, ord, axis, keepdims)\u001b[0m\n\u001b[1;32m   2514\u001b[0m \u001b[39mif\u001b[39;00m axis \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2515\u001b[0m     ndim \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mndim\n\u001b[0;32m-> 2516\u001b[0m     \u001b[39mif\u001b[39;00m ((\u001b[39mord\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mor\u001b[39;00m\n\u001b[1;32m   2517\u001b[0m         (\u001b[39mord\u001b[39m \u001b[39min\u001b[39;00m (\u001b[39m'\u001b[39m\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mfro\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mand\u001b[39;00m ndim \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m) \u001b[39mor\u001b[39;00m\n\u001b[1;32m   2518\u001b[0m         (\u001b[39mord\u001b[39m \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m \u001b[39mand\u001b[39;00m ndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m)):\n\u001b[1;32m   2520\u001b[0m         x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mravel(order\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mK\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m   2521\u001b[0m         \u001b[39mif\u001b[39;00m isComplexType(x\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mtype):\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "# train with SAC, stable baseline3\n",
    "import stable_baselines3\n",
    "from stable_baselines3 import SAC, PPO\n",
    "from stable_baselines3.sac import MlpPolicy\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from wandb.integration.sb3 import WandbCallback\n",
    "import wandb\n",
    "# pip install gym-robotics\n",
    "import matplotlib.pyplot as plt\n",
    "from gym_robotics.envs.fetch.reach import MujocoPyFetchReachEnv\n",
    "from gym.wrappers import TimeLimit\n",
    "\n",
    "\n",
    "# init mujoco fetch environment\n",
    "env = MujocoPyFetchReachEnv()\n",
    "\n",
    "log_dir = \"./sac_fetch_reach_tensorboard/\"\n",
    "\n",
    "\n",
    "max_steps = 30_000\n",
    "\n",
    "config = {\n",
    "    \"policy_type\": \"PPO\",\n",
    "    \"total_timesteps\": max_steps,\n",
    "    \"env_name\": \"FetchReach\",\n",
    "}\n",
    "run = wandb.init(\n",
    "    project=\"sb3\",\n",
    "    config=config,\n",
    "    sync_tensorboard=True,  # auto-upload sb3's tensorboard metrics\n",
    "    monitor_gym=True,  # auto-upload the videos of agents playing the game\n",
    "    save_code=True,  # optional\n",
    ")\n",
    "\n",
    "\n",
    "# init mujoco fetch environment\n",
    "# init mujoco fetch environment\n",
    "env = MujocoPyFetchReachEnv(reward_type='dense')\n",
    "env = Monitor(env, log_dir)\n",
    "#env = TimeLimit(env, max_episode_steps=100)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "\n",
    "env_eval = MujocoPyFetchReachEnv(reward_type='dense')\n",
    "env_eval = Monitor(env_eval, log_dir)\n",
    "#env_eval = TimeLimit(env_eval, max_episode_steps=100)\n",
    "env_eval = DummyVecEnv([lambda: env_eval])\n",
    "\n",
    "env.render_mode = 'rgb_array'\n",
    "# wrap environment\n",
    "# init model\n",
    "model = SAC(MlpPolicy, env, verbose=1, \n",
    "            device='cuda',wandb_log=True)\n",
    "#model = PPO(MlpPolicy, env, verbose=1,\n",
    "#            device='cuda')\n",
    "\n",
    "# train model\n",
    "model.learn(total_timesteps=max_steps, \n",
    "            log_interval=10, \n",
    "            tb_log_name=\"sac_fetch_reach\", \n",
    "            reset_num_timesteps=False, \n",
    "            eval_freq=100, \n",
    "            n_eval_episodes=10,\n",
    "            eval_log_path=\"sac_fetch_reach_eval\",\n",
    "            eval_env=env_eval,\n",
    "            )\n",
    "\n",
    "model.save(f\"sac_fetch_reach_{max_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# load results\n",
    "results = load_results(\"./sac_fetch_reach_tensorboard\")\n",
    "\n",
    "# plot results\n",
    "plt.plot(results)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "#model.save(\"sac_fetch_reach\")\n",
    "# load model\n",
    "# train with SAC, stable baseline3\n",
    "import stable_baselines3\n",
    "from stable_baselines3 import SAC, PPO\n",
    "from stable_baselines3.sac import MlpPolicy\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from wandb.integration.sb3 import WandbCallback\n",
    "import wandb\n",
    "# pip install gym-robotics\n",
    "import matplotlib.pyplot as plt\n",
    "from gym_robotics.envs.fetch.reach import MujocoPyFetchReachEnv\n",
    "from gym.wrappers import TimeLimit\n",
    "from jesnk_utils.rgb_to_video import RGB2VIDEO\n",
    "\n",
    "model = SAC.load(\"sac_fetch_reach_30000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RGB2VIDEO' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# test model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m rgb_to_video \u001b[39m=\u001b[39m RGB2VIDEO()\n\u001b[1;32m      4\u001b[0m env \u001b[39m=\u001b[39m MujocoPyFetchReachEnv(reward_type\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdense\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m env\u001b[39m.\u001b[39mrender_mode \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mrgb_array\u001b[39m\u001b[39m'\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'RGB2VIDEO' is not defined"
     ]
    }
   ],
   "source": [
    "# test model\n",
    "rgb_to_video = RGB2VIDEO()\n",
    "\n",
    "env = MujocoPyFetchReachEnv(reward_type='dense')\n",
    "env.render_mode = 'rgb_array'\n",
    "#env = Monitor(env_eval, log_dir)\n",
    "env = TimeLimit(env, max_episode_steps=100)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "env.render_mode = 'rgb_array'\n",
    "\n",
    "obs = env.reset()\n",
    "frames = []\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "episode_step = 0\n",
    "episode_num = 0\n",
    "for i in range(1,1001):\n",
    "    \n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    #print(dones)\n",
    "    frame = env.render()\n",
    "    \n",
    "    # insert infos\n",
    "    \n",
    "    \n",
    "    frames.append(frame)\n",
    "    \n",
    "    # insert infos\n",
    "    \n",
    "    \n",
    "    if dones[0]:\n",
    "        obs = env.reset()\n",
    "        episode_step = 0\n",
    "        episode_num += 1    \n",
    "\n",
    "        \n",
    "rgb_to_video.set_frames(frames)\n",
    "rgb_to_video.save(path=f'{i}_test.gif',mode='gif')\n",
    "print(f'episode {i} done')\n",
    "frames = []\n",
    "rgb_to_video.container.clear()\n",
    "\n",
    "# save model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
