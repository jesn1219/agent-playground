{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/research/jesnk_packages/gym_robotics/envs/fetch/reach.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjesnk\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/research/agent-playground/mujoco_env_customize/wandb/run-20230701_153735-a20jzmr3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jesnk/sb3/runs/a20jzmr3' target=\"_blank\">FetchReach-SAC-dense</a></strong> to <a href='https://wandb.ai/jesnk/sb3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jesnk/sb3' target=\"_blank\">https://wandb.ai/jesnk/sb3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jesnk/sb3/runs/a20jzmr3' target=\"_blank\">https://wandb.ai/jesnk/sb3/runs/a20jzmr3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/research/jesnk_packages/gym_robotics/envs/robot_env.py:330: UserWarning: \u001b[33mWARN: This version of the mujoco environments depends on the mujoco-py bindings, which are no longer maintained and may stop working. Please upgrade to the v4 versions of the environments (which depend on the mujoco python bindings instead), unless you are trying to precisely replicate previous works).\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# train with SAC, stable baseline3\n",
    "import stable_baselines3\n",
    "from stable_baselines3 import SAC, PPO\n",
    "from stable_baselines3.sac import MlpPolicy\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from wandb.integration.sb3 import WandbCallback\n",
    "import wandb\n",
    "# pip install gym-robotics\n",
    "import matplotlib.pyplot as plt\n",
    "from gym_robotics.envs.fetch.reach import MujocoPyFetchReachEnv\n",
    "from gym_robotics.envs.fetch.push import MujocoPyFetchPushEnv\n",
    "from gym.wrappers import TimeLimit\n",
    "from stable_baselines3 import HerReplayBuffer, DDPG, DQN, SAC, TD3\n",
    "from stable_baselines3.her.goal_selection_strategy import GoalSelectionStrategy\n",
    "import datetime\n",
    "log_dir = \"./tb_log/\"\n",
    "\n",
    "goal_selection_strategy = \"future\" # equivalent to GoalSelectionStrategy.FUTURE\n",
    "\n",
    "\n",
    "# init mujoco fetch enviroenment\n",
    "env_name = 'FetchReach'\n",
    "\n",
    "if env_name == 'FetchReach':\n",
    "    env_class = MujocoPyFetchReachEnv\n",
    "elif env_name == 'FetchPush':\n",
    "    env_class = MujocoPyFetchPushEnv\n",
    "else :\n",
    "    raise ValueError(f\"env_name: {env_name} is not supported\")\n",
    "\n",
    "model_name = 'SAC'\n",
    "\n",
    "\n",
    "max_steps = 100_000\n",
    "reward_type = 'dense'\n",
    "time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "#distance_threshold = 0.05\n",
    "config = {\n",
    "    \"policy_type\": model_name,\n",
    "    \"total_timesteps\": max_steps,\n",
    "    \"env_name\": env_name,\n",
    "    \"reward_type\": reward_type,\n",
    "    \"max_steps\": max_steps,\n",
    "}\n",
    "\n",
    "name = f\"{config['env_name']}-{config['policy_type']}-{config['reward_type']}\"\n",
    "run = wandb.init(\n",
    "    project=\"sb3\",\n",
    "    name= name,\n",
    "    config=config,\n",
    "    sync_tensorboard=True,  # auto-upload sb3's tensorboard metrics\n",
    "    monitor_gym=True,  # auto-upload the videos of agents playing the game\n",
    "    save_code=True,  # optional \n",
    ")\n",
    "\n",
    "\n",
    "# init mujoco fetch environment\n",
    "env = env_class(reward_type=config['reward_type'], max_episode_steps=50, action_scale=0.1)\n",
    "env = Monitor(env, log_dir)\n",
    "env = TimeLimit(env, max_episode_steps=100)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "\n",
    "env_eval = env_class(reward_type=config['reward_type'],max_episode_steps=50,action_scale=0.1)\n",
    "env_eval = Monitor(env_eval, log_dir)\n",
    "env_eval = TimeLimit(env_eval, max_episode_steps=100)\n",
    "env_eval = DummyVecEnv([lambda: env_eval])\n",
    "\n",
    "env.render_mode = 'rgb_array'\n",
    "# wrap environment\n",
    "# init model\n",
    "\n",
    "if model_name == 'SAC-HER':\n",
    "    model = SAC(MlpPolicy, env, verbose=1, \n",
    "            replay_buffer_class=HerReplayBuffer,\n",
    "            # Parameters for HER\n",
    "            replay_buffer_kwargs=dict(\n",
    "                n_sampled_goal=4,\n",
    "                goal_selection_strategy=goal_selection_strategy,),\n",
    "            device='cuda',wandb_log=True)\n",
    "\n",
    "elif model_name == 'SAC':\n",
    "    model = SAC(MlpPolicy, env, verbose=1,\n",
    "                device='cuda',wandb_log=True)\n",
    "else :\n",
    "    raise ValueError(f\"model_name: {model_name} is not supported\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(-0.1, 0.1, (4,), float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_to_video = RGB2VIDEO()\n",
    "#env = MujocoPyFetchReachEnv(reward_type='dense')\n",
    "env.render_mode = 'rgb_array'\n",
    "#env = Monitor(env_eval, log_dir)\n",
    "#env = TimeLimit(env, max_episode_steps=100)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "#env.render_mode = 'rgb_array'\n",
    "\n",
    "episode_step = 0\n",
    "episode_num = 0\n",
    "replay_step = 300\n",
    "cumulative_reward = 0\n",
    "frames = []\n",
    "obs = env.reset()\n",
    "\n",
    "success = []\n",
    "\n",
    "for i in range(1,replay_step+1):\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    cumulative_reward += rewards[0]\n",
    "    \n",
    "    frame = env.render()\n",
    "    #frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # append infos into image (rewards, episode_step, episode_num)\n",
    "    frame = cv2.putText(frame, f'rewards: {rewards[0]:.2f}', (10, 35), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1, cv2.LINE_AA)\n",
    "    frame = cv2.putText(frame, f'cumulative_reward: {cumulative_reward:.2f}', (10, 15), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1, cv2.LINE_AA)\n",
    "    frame = cv2.putText(frame, f'episode_step: {episode_step}', (10, 55), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1, cv2.LINE_AA)\n",
    "    frame = cv2.putText(frame, f'episode_num: {episode_num}', (10, 75), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1, cv2.LINE_AA)\n",
    "    frames.append(frame)\n",
    "    \n",
    "    episode_step += 1\n",
    "    if dones[0]:\n",
    "        obs = env.reset()\n",
    "        success.append(info[0]['is_success'])\n",
    "        episode_step = 0\n",
    "        cumulative_reward = 0\n",
    "        episode_num += 1\n",
    "\n",
    "print(f'episode {i} done')\n",
    "success_rate = sum(success)/len(success)\n",
    "print(f'success rate: {success_rate}')\n",
    "\n",
    "rgb_to_video.set_frames(frames)\n",
    "rgb_to_video.set_fps(5)\n",
    "rgb_to_video.save(path=f'{rollout_path}epi{len(success)}_sucrat{success_rate:.3f}.gif',mode='gif')\n",
    "\n",
    "frames = []\n",
    "rgb_to_video.container.clear()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train model\n",
    "model.learn(total_timesteps=max_steps, \n",
    "            log_interval=10, \n",
    "            tb_log_name=\"sac_fetch_reach\", \n",
    "            reset_num_timesteps=False, \n",
    "            eval_freq=100, \n",
    "            n_eval_episodes=20,\n",
    "            eval_log_path=\"sac_fetch_reach_eval\",\n",
    "            eval_env=env_eval,\n",
    "            )\n",
    "\n",
    "model.save(f\"./checkpoint/{name}-{time}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
